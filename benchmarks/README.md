# Semantica Benchmark Suite

## Overview

The Semantica Benchmark Suite is a comprehensive performance testing framework designed to measure speed, throughput, latency, and scalability across all Semantica modules. This suite provides standardized benchmarks for tracking improvements, comparing configurations, and ensuring production-ready performance.

## ðŸ“Š Documentation Structure

| Document | Purpose | Key Information |
|----------|---------|-----------------|
| **[BENCHMARK_RESULTS.md](./BENCHMARK_RESULTS.md)** | Complete results overview | All benchmark results with performance analysis |
| **[PERFORMANCE_SUMMARY.md](./PERFORMANCE_SUMMARY.md)** | Executive summary | High-level performance metrics and insights |
| **[DETAILED_RESULTS.md](./DETAILED_RESULTS.md)** | Raw test data | Individual test results in tabular format |
| **[benchmarks.md](./benchmarks.md)** | Architecture guide | Suite design and implementation details |

## ðŸš€ Quick Start

### Run All Benchmarks
```bash
python benchmarks/benchmarks_runner.py
```

### Run Specific Module
```bash
# Input Layer tests
pytest benchmarks/input_layer/

# Storage tests  
pytest benchmarks/storage/

# Visualization tests
pytest benchmarks/visualization/
```

### Compare Performance
```bash
# Compare with baseline
python benchmarks/infrastructure/compare.py baseline.json current.json

# Set new baseline
cp benchmarks/results/run_TIMESTAMP.json benchmarks/results/baseline.json
```

## ðŸ“ˆ Performance Overview

### Suite Statistics
- **Total Benchmarks**: 137 tests
- **Modules Covered**: 10 core modules
- **Test Duration**: ~27 minutes
- **Success Rate**: 100% (138 passed, 1 skipped)

### Module Performance Summary

| Module | Tests | Avg Performance | Grade |
|--------|-------|----------------|-------|
| Input Layer | 12 | 12,877 ops/s | ðŸŸ¢ Excellent |
| Normalization | 3 | 8,300 ops/s | ðŸŸ¢ Excellent |
| Storage | 4 | 1,571 ops/s | ðŸŸ¢ Excellent |
| Context & Memory | 2 | 1,393 ops/s | ðŸŸ¢ Excellent |
| Output Orchestration | 8 | 847 ops/s | ðŸŸ¢ Excellent |
| Ontology | 5 | 593 ops/s | ðŸŸ¢ Excellent |
| Core Processing | 2 | 291 ops/s | ðŸŸ¢ Excellent |
| Quality Assurance | 2 | 173 ops/s | ðŸŸ¢ Excellent |
| Visualization | 1 | 51 ops/s | ðŸŸ¢ Excellent |
| Export | 6 | 20 ops/s | ðŸŸ¢ Excellent |

## ðŸ—ï¸ Architecture

### Environment-Agnostic Design
The benchmark suite is designed to run in any environment:
- **CI/CD Compatibility**: Mocked heavy dependencies for lightweight runners
- **Real Library Testing**: Optional real library testing with `BENCHMARK_REAL_LIBS=1`
- **Cross-Platform**: Works on Windows, Linux, and macOS

### Regression Detection
- **Statistical Analysis**: Z-score based regression detection
- **Thresholds**: 10% performance degradation trigger
- **Baseline Tracking**: Automatic baseline management
- **Continuous Monitoring**: CI/CD integration for automated testing

## ðŸ“ Directory Structure

```
benchmarks/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ BENCHMARK_RESULTS.md         # Complete results overview
â”œâ”€â”€ PERFORMANCE_SUMMARY.md       # Executive summary
â”œâ”€â”€ DETAILED_RESULTS.md          # Raw test data
â”œâ”€â”€ benchmarks.md               # Architecture guide
â”œâ”€â”€ benchmarks_runner.py        # Master runner script
â”œâ”€â”€ conftest.py                  # Global configuration and mocking
â”œâ”€â”€ requirements.txt             # Benchmark dependencies
â”œâ”€â”€ infrastructure/              # Core infrastructure
â”‚   â””â”€â”€ compare.py              # Performance comparison tool
â”œâ”€â”€ input_layer/                 # Input processing benchmarks
â”œâ”€â”€ core_processing/            # Core logic benchmarks
â”œâ”€â”€ storage/                     # Data storage benchmarks
â”œâ”€â”€ context_memory/              # Context and memory benchmarks
â”œâ”€â”€ quality_assurance/           # QA benchmarks
â”œâ”€â”€ ontology/                    # Ontology benchmarks
â”œâ”€â”€ export/                      # Export benchmarks
â”œâ”€â”€ visualization/               # Visualization benchmarks
â”œâ”€â”€ normalization/              # Data normalization benchmarks
â”œâ”€â”€ output_orchestration/       # Pipeline orchestration benchmarks
â””â”€â”€ results/                     # Benchmark results storage
    â”œâ”€â”€ run_TIMESTAMP.json       # Individual run results
    â””â”€â”€ baseline.json            # Performance baseline
```

## ðŸ”§ Configuration

### Environment Variables
- `BENCHMARK_REAL_LIBS=1`: Enable real library testing
- `BENCHMARK_STRICT=1`: Fail on performance regression

### Benchmark Categories

#### Input Layer
- CSV/JSON/XML/YAML parsing performance
- Code parsing and analysis
- Text chunking and splitting
- Document ingestion

#### Core Processing
- Entity extraction speed
- Graph building performance
- Semantic analysis throughput

#### Storage
- Vector storage operations
- Graph store performance
- Triplet storage efficiency
- Bulk loading optimization

#### Context & Memory
- Context retrieval speed
- Memory storage overhead
- Short-term pruning performance
- Graph linking efficiency

#### Quality Assurance
- Deduplication performance
- Conflict detection speed
- Data validation throughput

#### Ontology
- Ontology inference speed
- Reasoning performance
- Serialization efficiency
- Reuse optimization

#### Export
- JSON/CSV/YAML export
- Graph export formats
- Semantic export performance
- Structured data export

#### Visualization
- Knowledge graph rendering
- Embedding visualization
- Analytics dashboard
- Temporal animations
- Matrix view rendering

#### Normalization
- Text normalization speed
- Data cleaning performance
- Parser normalization
- Heavy library mocking

#### Output Orchestration
- Pipeline execution performance
- Parallelism efficiency
- Task scheduling
- Workflow serialization

## ðŸ“Š Results Analysis

### Performance Tiers

#### ðŸŸ¢ Ultra-Fast (>10,000 ops/s)
- Text normalization: 28,986 ops/s
- JSON parsing: 22,222 ops/s
- CSV parsing: 16,949 ops/s

#### ðŸŸ¡ Fast (1,000-10,000 ops/s)
- Vector storage: 2,894 ops/s
- Graph operations: 1,761 ops/s
- Context retrieval: 4,255 ops/s

#### ðŸ”µ Standard (100-1,000 ops/s)
- Entity extraction: 407 ops/s
- Graph building: 176 ops/s
- Quality checks: 173 ops/s

#### ðŸŸ£ Intensive (<100 ops/s)
- Export operations: 20 ops/s
- Visualization: 51 ops/s
- Temporal processing: 1 ops/s

### Key Insights

1. **Input Processing Excellence**: Text and data parsing operations perform exceptionally well
2. **Storage Efficiency**: Vector and graph storage operations are highly optimized
3. **Visualization Bottlenecks**: Complex rendering operations require optimization
4. **Export Performance**: Large dataset exports need parallel processing
5. **Memory Management**: Context operations show good memory efficiency

## ðŸ”„ Continuous Integration

### GitHub Actions Integration
```yaml
# .github/workflows/benchmark.yml
name: Semantica Performance Suite
on: [push, pull_request]
jobs:
  performance-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
      - name: Install Dependencies
        run: |
          pip install -e .
          pip install -r benchmarks/requirements.txt
      - name: Execute Benchmarks
        run: python benchmarks/benchmarks_runner.py
      - name: Upload Results
        uses: actions/upload-artifact@v4
```

### Regression Detection
- Automatic baseline comparison
- Z-score statistical analysis
- Performance threshold enforcement
- Automated failure reporting

## ðŸ› ï¸ Development

### Adding New Benchmarks
1. Create test file in appropriate module directory
2. Use `@pytest.mark.benchmark` decorator
3. Follow naming convention: `test_[module]_[operation]`
4. Add performance assertions if needed

### Example Benchmark
```python
import pytest

@pytest.mark.benchmark
def test_new_operation_performance(benchmark):
    result = benchmark.pedantic(operation_to_test, data=test_data)
    assert result is not None
```

### Mocking Heavy Dependencies
The suite automatically mocks heavy libraries in CI environments:
- `torch`, `transformers`, `spacy`
- `neo4j`, `weaviate`, `qdrant_client`
- `matplotlib`, `umap`, `networkx`
- `docling`, `instructor`, `fireworks`

## ðŸ“ˆ Monitoring

### Performance Tracking
- Historical performance data
- Trend analysis over time
- Environment comparison
- Regression alerts

### Metrics Collected
- Execution time (min, mean, max, stddev)
- Operations per second (OPS)
- Memory usage (when available)
- Error rates and exceptions

## ðŸš¨ Troubleshooting

### Common Issues

#### Import Errors
```bash
# Solution: Use mocked environment
BENCHMARK_REAL_LIBS="" python benchmarks/benchmarks_runner.py
```

#### Performance Regression
```bash
# Solution: Compare with baseline
python benchmarks/infrastructure/compare.py baseline.json current.json
```

#### Memory Issues
```bash
# Solution: Run specific module
pytest benchmarks/storage/ -v
```

### Debug Mode
```bash
# Enable verbose output
pytest benchmarks/ -v -s --benchmark-only

# Run specific test
pytest benchmarks/input_layer/test_parsing.py::test_json_parsing_performance -v
```

## ðŸ“ Contributing

### Guidelines
1. Follow existing naming conventions
2. Add appropriate performance assertions
3. Document benchmark purpose
4. Test in both mocked and real environments
5. Update documentation

### Performance Standards
- New operations should maintain <100ms mean time
- Batch operations should scale linearly
- Memory usage should be bounded
- Error handling should be robust

## ðŸ”— Related Resources

- **[Evaluation Framework](../docs/evaluation.md)**: Accuracy and quality measurement
- **[Quality Assurance Module](../semantica/quality_assurance/)**: Data quality detection
- **[Performance Optimization Guide](../docs/performance.md)**: Optimization strategies
- **[CI/CD Configuration](../.github/workflows/)**: Automated testing setup

---

**Last Updated**: February 7, 2026  
**Version**: 1.0.0  
**Maintainers**: [@ZohaibHassan16](https://github.com/ZohaibHassan16), [@KaifAhmad1](https://github.com/KaifAhmad1)
