{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729d3236",
   "metadata": {},
   "source": [
    "# Military Capability Gap Analysis with Semantica Context Graphs\n",
    "\n",
    "## Objective\n",
    "This notebook demonstrates **end-to-end capability gap analysis** for defense planning using Semantica's advanced context graphs and decision intelligence platform. It showcases how to transform raw documents and ontologies into actionable intelligence with complete audit trails.\n",
    "\n",
    "## Use Case Scope\n",
    "- **Domain**: Defense capability planning and gap analysis\n",
    "- **Scenario**: Future A2/AD (Anti-Access/Area Denial) environments\n",
    "- **Timeline**: 2028+ strategic planning horizon\n",
    "- **Stakeholders**: Defense planners, capability managers, decision makers\n",
    "\n",
    "## End-to-End Pipeline Sequence\n",
    "```\n",
    "Data Ingestion → Document Parsing → Text Splitting → Text Normalization → \n",
    "Semantic Extraction → Entity Resolution & Deduplication → Knowledge Graph → \n",
    "Vector Store Configuration → Context Graph → Decision Tracking → \n",
    "Policy Engine & Compliance → Analytics & Insights → Export & Reporting\n",
    "```\n",
    "\n",
    "## Context Graph Pattern\n",
    "```\n",
    "Scenario → Mission Thread → Events → Systems → Capabilities → Gaps → Decisions → Outcomes\n",
    "```\n",
    "\n",
    "## Key Questions Answered\n",
    "1. **Capability Assessment**: What capability gaps exist for specific mission threads?\n",
    "2. **Evidence Tracking**: Which sources and evidence support each identified gap?\n",
    "3. **Decision Governance**: What precedents, exceptions, and approvals influenced decisions?\n",
    "4. **Risk Analysis**: What multi-hop paths connect scenarios to risk outcomes?\n",
    "5. **Policy Compliance**: Are decisions aligned with established policies and governance frameworks?\n",
    "\n",
    "## Detailed Pipeline Steps\n",
    "\n",
    "### 1. Data Ingestion\n",
    "- **FileIngestor**: Local document processing (PDF, TXT, JSON, etc.)\n",
    "- **WebIngestor**: Web content extraction and processing\n",
    "- **OntologyIngestor**: RDF/OWL/TTL ontology ingestion\n",
    "\n",
    "### 2. Document Parsing\n",
    "- **PDFParser**: Advanced PDF text extraction with metadata\n",
    "- **DocumentParser**: Multi-format document processing\n",
    "- **DoclingParser**: AI-powered document understanding (optional)\n",
    "\n",
    "### 3. Text Splitting & Chunking\n",
    "- **TextSplitter**: Recursive and entity-aware chunking\n",
    "- **SemanticChunker**: Semantic boundary detection\n",
    "- **StructuralChunker**: Document structure-based splitting\n",
    "\n",
    "### 4. Text Normalization\n",
    "- **TextNormalizer**: Text cleaning and standardization\n",
    "- **EntityNormalizer**: Entity name resolution and normalization\n",
    "- **DateNormalizer**: Temporal expression normalization\n",
    "- **NumberNormalizer**: Numeric value standardization\n",
    "- **LanguageDetector**: Language identification and processing\n",
    "- **EncodingHandler**: Character encoding detection and conversion\n",
    "- **TextCleaner**: Noise removal and text sanitization\n",
    "\n",
    "### 5. Semantic Extraction\n",
    "- **NamedEntityRecognizer**: Entity identification and classification\n",
    "- **RelationExtractor**: Relationship extraction between entities\n",
    "- **EventDetector**: Event identification and temporal analysis\n",
    "- **CoreferenceResolver**: Pronoun and reference resolution\n",
    "- **TripletExtractor**: Subject-predicate-object triplet extraction\n",
    "- **SemanticAnalyzer**: Semantic similarity and analysis\n",
    "- **SemanticNetworkExtractor**: Network structure extraction\n",
    "- **ExtractionValidator**: Quality assurance and validation\n",
    "\n",
    "### 6. Entity Resolution & Deduplication\n",
    "- **EntityDeduplicator**: Fuzzy matching and embedding-based entity resolution\n",
    "- **RelationshipDeduplicator**: Context-aware relationship deduplication\n",
    "- **ConflictDetector**: Data conflict identification and resolution\n",
    "- **ConflictResolver**: Automated conflict resolution strategies\n",
    "\n",
    "### 7. Knowledge Graph Construction\n",
    "- **GraphBuilder**: KG construction from extracted data\n",
    "- **GraphAnalyzer**: Graph structure and topology analysis\n",
    "- **CentralityCalculator**: Node importance and influence metrics\n",
    "- **CommunityDetector**: Community structure identification\n",
    "- **ConnectivityAnalyzer**: Graph connectivity and robustness\n",
    "- **SimilarityCalculator**: Node and edge similarity analysis\n",
    "- **LinkPredictor**: Missing link prediction and recommendation\n",
    "- **PathFinder**: Optimal path discovery and routing\n",
    "- **EntityResolver**: Entity deduplication and resolution\n",
    "\n",
    "### 8. Vector Store Configuration\n",
    "- **ApacheAGE**: Graph database with vector capabilities\n",
    "- **VectorStore**: Semantic similarity and search\n",
    "- **Vector Indexing**: Efficient similarity search with IVF Flat indexing\n",
    "- **Embedding Support**: FastEmbed integration for efficient vector operations\n",
    "\n",
    "### 9. Context Graph Creation\n",
    "- **ContextGraph**: Advanced context modeling with analytics\n",
    "- **AgentContext**: Unified context management interface\n",
    "- **Graph Expansion**: Multi-hop context expansion and reasoning\n",
    "- **Semantic Search**: Hybrid semantic and structural search\n",
    "\n",
    "### 10. Decision Recording & Tracking\n",
    "- **Decision Models**: Decision, Policy, PolicyException, ApprovalChain, Precedent\n",
    "- **Decision Recorder**: Decision lifecycle management\n",
    "- **Decision Query**: Advanced decision search and retrieval\n",
    "- **Causal Analyzer**: Decision influence and impact analysis\n",
    "- **Precedent Search**: Smart precedent identification and analysis\n",
    "\n",
    "### 11. Policy Engine & Compliance\n",
    "- **PolicyEngine**: Policy definition and compliance checking\n",
    "- **Policy Management**: Versioning, compliance checking, exception handling\n",
    "- **Compliance Monitoring**: Real-time policy compliance validation\n",
    "- **Exception Handling**: Policy exception management and approval workflows\n",
    "\n",
    "### 12. Analytics & Insights\n",
    "- **GraphAnalytics**: Comprehensive graph analytics and metrics\n",
    "- **DecisionAnalytics**: Decision pattern analysis and impact assessment\n",
    "- **PerformanceAnalytics**: System performance monitoring and optimization\n",
    "- **KGVisualizer**: Interactive graph visualization\n",
    "- **Dashboard**: Real-time analytics dashboard and KPI monitoring\n",
    "\n",
    "### 13. Export & Reporting\n",
    "- **Multi-Format Export**: JSON, RDF, GraphML, CSV, YAML, LPG\n",
    "- **ReportGenerator**: Comprehensive report generation\n",
    "- **ApacheAGE Export**: SQL export scripts for database integration\n",
    "- **Analytics Export**: Performance metrics and analysis results\n",
    "\n",
    "## Expected Outputs\n",
    "- **Context Graph**: Capability-gap context graph with decision traces\n",
    "- **Knowledge Graph**: Complete KG with entities and relationships\n",
    "- **Vector Store**: Semantic search index with Apache AGE integration\n",
    "- **Decision Records**: Full audit trail with policy compliance\n",
    "- **Multi-Format Exports**: JSON, RDF, GraphML, CSV, YAML, LPG\n",
    "- **Analytics Reports**: Comprehensive analysis and insights\n",
    "- **Visualizations**: Interactive graph representations\n",
    "\n",
    "## Key Metrics Tracked\n",
    "- **Ingestion**: Document count, size, processing time\n",
    "- **Extraction**: Entity/relationship/event counts, confidence scores\n",
    "- **Graph Analytics**: Node/edge counts, centrality measures, community structure\n",
    "- **Vector Operations**: Embedding generation, search performance, indexing metrics\n",
    "- **Decision Tracking**: Decision volume, compliance rates, approval chains\n",
    "- **Performance**: Processing times, memory usage, throughput\n",
    "\n",
    "## Technology Stack\n",
    "- **Core**: Semantica Context Graph & Decision Intelligence Platform\n",
    "- **Vector Database**: Apache AGE with vector capabilities\n",
    "- **Graph Analytics**: NetworkX, Node2Vec, Community Detection\n",
    "- **Semantic Search**: Hybrid search with embeddings and graph structure\n",
    "- **Export Formats**: JSON, RDF/Turtle, GraphML, CSV, YAML, LPG (Cypher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install semantica==0.3.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad777f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Configuration (Consolidated)\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup directories\n",
    "BASE_DIR = Path.cwd()\n",
    "USE_CASE_DIR = BASE_DIR / 'cookbook' / 'use_cases' / 'capability_gap_defense'\n",
    "DATA_DIR = USE_CASE_DIR / 'data'\n",
    "OUTPUT_DIR = USE_CASE_DIR / 'outputs'\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Setup completed:\")\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Return directory paths for verification\n",
    "BASE_DIR, DATA_DIR, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f73b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Source Data Ingestion\n",
    "# Import ingestion classes where they are used\n",
    "\n",
    "from semantica.ingest import FileIngestor, WebIngestor, OntologyIngestor\n",
    "\n",
    "# Initialize ingestion classes\n",
    "file_ingestor = FileIngestor()\n",
    "web_ingestor = WebIngestor()\n",
    "ontology_ingestor = OntologyIngestor()\n",
    "\n",
    "# File Ingestion\n",
    "print(\"Starting file ingestion...\")\n",
    "try:\n",
    "    file_objects = file_ingestor.ingest_directory(\n",
    "        directory_path=DATA_DIR,\n",
    "        recursive=False,\n",
    "        read_content=True,  # Read content for processing\n",
    "        include_metadata=True\n",
    "    )\n",
    "    print(f\"Files ingested: {len(file_objects)}\")\n",
    "except Exception as e:\n",
    "    print(f\"File ingestion failed: {e}\")\n",
    "    file_objects = []\n",
    "\n",
    "# Web Ingestion  \n",
    "print(\"Starting web ingestion...\")\n",
    "web_sources = [\n",
    "    'https://www.rand.org/pubs/research_reports/RRA733-1.html',\n",
    "    'https://foundationcapital.com/context-graphs/',\n",
    "    'https://www.defense.gov/News/Releases/',\n",
    "    'https://www.navy.mil/Portals/1/NWC/NSG_Support/'\n",
    "]\n",
    "\n",
    "web_contents = []\n",
    "for i, url in enumerate(web_sources):\n",
    "    try:\n",
    "        web_content = web_ingestor.ingest_url(\n",
    "            url=url,\n",
    "            extract_content=True,\n",
    "            include_metadata=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        web_contents.append(web_content)\n",
    "        print(f\"Web content {i+1}: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Web ingestion failed for {url}: {e}\")\n",
    "\n",
    "# Ontology Ingestion\n",
    "print(\"Starting ontology ingestion...\")\n",
    "try:\n",
    "    ontology_data = ontology_ingestor.ingest_directory(\n",
    "        directory_path=DATA_DIR,\n",
    "        recursive=False,\n",
    "        format='turtle',  # TTL format\n",
    "        validate_schema=True\n",
    "    )\n",
    "    print(f\"Ontologies ingested: {len(ontology_data)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Ontology ingestion failed: {e}\")\n",
    "    ontology_data = []\n",
    "\n",
    "# Ingestion Summary\n",
    "ingestion_summary = {\n",
    "    'files': {\n",
    "        'count': len(file_objects),\n",
    "        'types': list(set([getattr(f, 'file_type', 'unknown') for f in file_objects])),\n",
    "        'total_size_mb': round(sum([getattr(f, 'size', 0) for f in file_objects]) / (1024*1024), 2)\n",
    "    },\n",
    "    'web': {\n",
    "        'count': len(web_contents),\n",
    "        'sources': [getattr(w, 'url', 'unknown') for w in web_contents],\n",
    "        'total_chars': sum([len(getattr(w, 'content', '')) for w in web_contents])\n",
    "    },\n",
    "    'ontologies': {\n",
    "        'count': len(ontology_data),\n",
    "        'formats': list(set([getattr(o, 'format', 'turtle') for o in ontology_data])),\n",
    "        'total_classes': sum([len(getattr(o, 'data', {}).get('classes', [])) for o in ontology_data])\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Ingestion Summary:\")\n",
    "for category, stats in ingestion_summary.items():\n",
    "    print(f\"  {category.upper()}: {stats}\")\n",
    "\n",
    "# Data Quality Check\n",
    "def validate_ingestion_quality():\n",
    "    \"\"\"Validate the quality of ingested data\"\"\"\n",
    "    quality_report = {\n",
    "        'files_with_content': sum([1 for f in file_objects if hasattr(f, 'content') and f.content]),\n",
    "        'web_with_content': sum([1 for w in web_contents if hasattr(w, 'content') and w.content]),\n",
    "        'ontologies_with_schema': sum([1 for o in ontology_data if hasattr(o, 'data') and o.data.get('classes')]),\n",
    "        'total_documents': len(file_objects) + len(web_contents) + len(ontology_data)\n",
    "    }\n",
    "    return quality_report\n",
    "\n",
    "quality_report = validate_ingestion_quality()\n",
    "print(f\"Quality Report: {quality_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03987483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Parsing follows in the next cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13308e8d",
   "metadata": {},
   "source": [
    "- Modules: `FileIngestor`, `WebIngestor`, `OntologyIngestor`\n",
    "- Loads local files.\n",
    "- Fetches web content.\n",
    "- Ingests ontology files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf5d0b",
   "metadata": {},
   "source": [
    "# Document Parsing will follow in the next cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146ca81",
   "metadata": {},
   "source": [
    "# Document parsing modules and functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77094c01",
   "metadata": {},
   "source": [
    "# Ontology evaluation summary will be provided after evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e07eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontology Evaluation Summary\n",
    "# This cell provides a summary of the ontology evaluation completed in cell 29\n",
    "\n",
    "if 'evaluation_results' in locals() and evaluation_results:\n",
    "    print(\"Ontology Evaluation Summary:\")\n",
    "    print(f\"  Total ontologies evaluated: {len(evaluation_results)}\")\n",
    "    \n",
    "    successful_evals = [r for r in evaluation_results if 'error' not in r]\n",
    "    if successful_evals:\n",
    "        avg_coverage = sum([r['coverage_score'] for r in successful_evals]) / len(successful_evals)\n",
    "        avg_completeness = sum([r['completeness_score'] for r in successful_evals]) / len(successful_evals)\n",
    "        print(f\"  Successful evaluations: {len(successful_evals)}\")\n",
    "        print(f\"  Average coverage score: {avg_coverage:.2f}\")\n",
    "        print(f\"  Average completeness score: {avg_completeness:.2f}\")\n",
    "        \n",
    "        # Show extraction context if available\n",
    "        if 'extraction_context' in successful_evals[0]:\n",
    "            ctx = successful_evals[0]['extraction_context']\n",
    "            print(f\"  Extraction context available:\")\n",
    "            print(f\"    Extracted entities: {ctx.get('extracted_entities', 0)}\")\n",
    "            print(f\"    Extracted relationships: {ctx.get('extracted_relationships', 0)}\")\n",
    "            print(f\"    Extracted events: {ctx.get('extracted_events', 0)}\")\n",
    "    else:\n",
    "        print(\"  No successful evaluations\")\n",
    "else:\n",
    "    print(\"No ontology evaluation results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d973dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Document Parsing\n",
    "from semantica.parse import PDFParser\n",
    "\n",
    "pdf_parser = PDFParser()\n",
    "pdf_docs = []\n",
    "for pdf_path in sorted(DATA_DIR.glob('*.pdf')):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            if f.read(4) != b'%PDF':\n",
    "                print(f'Skipping non-PDF payload: {pdf_path.name}')\n",
    "                continue\n",
    "\n",
    "        parsed = pdf_parser.parse(pdf_path, pages=list(range(0, 12)))\n",
    "        text = parsed.get('full_text', parsed.get('text', ''))\n",
    "        if text:\n",
    "            pdf_docs.append({\n",
    "                'doc_id': pdf_path.stem,\n",
    "                'source': str(pdf_path),\n",
    "                'text': text[:50000],\n",
    "                'metadata': parsed.get('metadata', {}),\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f'PDF parse failed for {pdf_path.name}: {e}')\n",
    "\n",
    "print(f\"PDF documents parsed: {len(pdf_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551ef61",
   "metadata": {},
   "source": [
    "## Multi-Format Parsing: DocumentParser + Optional DoclingParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f694157",
   "metadata": {},
   "source": [
    "- Modules: `PDFParser`, `DocumentParser`, optional `DoclingParser`\n",
    "- Parses PDF/documents.\n",
    "- Extracts text and metadata.\n",
    "- Uses Docling parser if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Format Document Parsing\n",
    "# Import document parsing classes where they are used\n",
    "\n",
    "from semantica.parse import DocumentParser\n",
    "\n",
    "doc_parser = DocumentParser()\n",
    "doc_parser_preview = {}\n",
    "\n",
    "if pdf_docs:\n",
    "    sample_pdf = Path(pdf_docs[0]['source'])\n",
    "    try:\n",
    "        parsed_doc = doc_parser.parse_document(sample_pdf)\n",
    "        doc_parser_preview = {\n",
    "            'source': sample_pdf.name,\n",
    "            'keys': list(parsed_doc.keys())[:10],\n",
    "            'text_chars': len(parsed_doc.get('full_text', parsed_doc.get('text', '')) or ''),\n",
    "        }\n",
    "        print(f\"Document parser preview for {sample_pdf.name}: {doc_parser_preview}\")\n",
    "    except Exception as e:\n",
    "        doc_parser_preview = {'source': sample_pdf.name, 'error': str(e)}\n",
    "        print(f\"Document parser error: {e}\")\n",
    "\n",
    "# Check for Docling availability\n",
    "import semantica.parse as parse_module\n",
    "docling_preview = {'docling_available': bool(getattr(parse_module, 'DOCLING_AVAILABLE', False))}\n",
    "if getattr(parse_module, 'DOCLING_AVAILABLE', False) and pdf_docs:\n",
    "    try:\n",
    "        from semantica.parse import DoclingParser\n",
    "        docling_parser = DoclingParser(export_format='markdown')\n",
    "        dres = docling_parser.parse(Path(pdf_docs[0]['source']))\n",
    "        docling_preview['keys'] = list(dres.keys())[:10]\n",
    "        docling_preview['text_chars'] = len(dres.get('full_text', dres.get('text', '')) or '')\n",
    "        print(f\"Docling parser available: {docling_preview}\")\n",
    "    except Exception as e:\n",
    "        docling_preview['error'] = str(e)\n",
    "        print(f\"Docling parser error: {e}\")\n",
    "else:\n",
    "    print(\"Docling parser not available\")\n",
    "\n",
    "print(f\"Document parsing completed. Docling available: {docling_preview['docling_available']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontology evaluation will be moved to after semantic extraction for better context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document Corpus\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Use only available web contents (remove undefined variables)\n",
    "web_items = web_contents if 'web_contents' in locals() else []\n",
    "\n",
    "corpus = (\n",
    "    [\n",
    "        {'doc_id': d['doc_id'], 'source': d['source'], 'text': d['text']}\n",
    "        for d in pdf_docs\n",
    "    ]\n",
    "    + [\n",
    "        {\n",
    "            'doc_id': f'web_{i}',\n",
    "            'source': getattr(w, 'url', f'web_source_{i}'),\n",
    "            'text': (getattr(w, 'content', str(w)) or '')[:30000],\n",
    "        }\n",
    "        for i, w in enumerate(web_items)\n",
    "    ]\n",
    "    + [\n",
    "        {\n",
    "            'doc_id': Path(ont.source_path).stem,\n",
    "            'source': ont.source_path,\n",
    "            'text': json.dumps(ont.data, ensure_ascii=True)[:40000],\n",
    "        }\n",
    "        for ont in ontology_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Corpus created:\")\n",
    "print(f\"  Total documents: {len(corpus)}\")\n",
    "print(f\"  Sample document IDs: {[c['doc_id'] for c in corpus[:5]]}\")\n",
    "print(f\"  Document types: {len(pdf_docs)} PDFs, {len(web_items)} web, {len(ontology_data)} ontologies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24299fa3",
   "metadata": {},
   "source": [
    "## Orchestration-Path Chunking (Decision-Time Context Capture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06c62b",
   "metadata": {},
   "source": [
    "- Modules: `TextSplitter`, `PipelineBuilder`\n",
    "- Splits documents into chunks.\n",
    "- Defines pipeline steps for ingest, split, extract, graph, export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitting and Chunking\n",
    "# Import splitting classes where they are used\n",
    "\n",
    "from semantica.split import TextSplitter\n",
    "\n",
    "splitter = TextSplitter(method='recursive', chunk_size=1800, chunk_overlap=250)\n",
    "\n",
    "texts = [doc.get('text', '') for doc in corpus]\n",
    "chunks_by_doc = splitter.split_batch(texts)\n",
    "\n",
    "chunked_docs = []\n",
    "for doc, chunks in zip(corpus, chunks_by_doc):\n",
    "    for idx, ch in enumerate(chunks or []):\n",
    "        chunked_docs.append({\n",
    "            'doc_id': f\"{doc['doc_id']}::chunk_{idx}\",\n",
    "            'source': doc['source'],\n",
    "            'text': ch.text if hasattr(ch, 'text') else str(ch),\n",
    "            'parent_doc_id': doc['doc_id'],\n",
    "        })\n",
    "\n",
    "extraction_corpus = chunked_docs if chunked_docs else corpus\n",
    "\n",
    "print(f\"Text splitting completed:\")\n",
    "print(f\"  Original documents: {len(corpus)}\")\n",
    "print(f\"  Chunked documents: {len(chunked_docs)}\")\n",
    "print(f\"  Extraction corpus size: {len(extraction_corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf87ac1",
   "metadata": {},
   "source": [
    "## Split Strategies (Semantic / Structural / Entity-Aware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a4248",
   "metadata": {},
   "source": [
    "- Modules: `SemanticChunker`, `StructuralChunker`, `TextSplitter` (`entity_aware`)\n",
    "- Runs multiple split strategies.\n",
    "- Compares chunk counts/output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413768a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Splitting Strategies\n",
    "# Import advanced splitting classes where they are used\n",
    "\n",
    "from semantica.split import SemanticChunker, StructuralChunker\n",
    "\n",
    "split_strategy_preview = {}\n",
    "if corpus:\n",
    "    sample_text = corpus[0]['text'][:12000]\n",
    "    print(\"Testing different splitting strategies...\")\n",
    "    \n",
    "    try:\n",
    "        semantic_chunker = SemanticChunker(chunk_size=1200, chunk_overlap=200)\n",
    "        sem_chunks = semantic_chunker.chunk(sample_text)\n",
    "        split_strategy_preview['semantic_chunks'] = len(sem_chunks)\n",
    "        print(f\"  Semantic chunks: {len(sem_chunks)}\")\n",
    "    except Exception as e:\n",
    "        split_strategy_preview['semantic_chunks_error'] = str(e)\n",
    "        print(f\"  Semantic chunking error: {e}\")\n",
    "\n",
    "    try:\n",
    "        structural_chunker = StructuralChunker(chunk_size=1200, chunk_overlap=150)\n",
    "        st_chunks = structural_chunker.chunk(sample_text)\n",
    "        split_strategy_preview['structural_chunks'] = len(st_chunks)\n",
    "        print(f\"  Structural chunks: {len(st_chunks)}\")\n",
    "    except Exception as e:\n",
    "        split_strategy_preview['structural_chunks_error'] = str(e)\n",
    "        print(f\"  Structural chunking error: {e}\")\n",
    "\n",
    "    try:\n",
    "        from semantica.split import TextSplitter\n",
    "        ea_splitter = TextSplitter(method=['entity_aware', 'recursive'], chunk_size=1200, chunk_overlap=150)\n",
    "        ea_chunks = ea_splitter.split(sample_text)\n",
    "        split_strategy_preview['entity_aware_chunks'] = len(ea_chunks)\n",
    "        print(f\"  Entity-aware chunks: {len(ea_chunks)}\")\n",
    "    except Exception as e:\n",
    "        split_strategy_preview['entity_aware_chunks_error'] = str(e)\n",
    "        print(f\"  Entity-aware chunking error: {e}\")\n",
    "\n",
    "print(\"Splitting strategy comparison completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Sequence Overview\n",
    "# This notebook follows the correct logical sequence for capability gap analysis\n",
    "\n",
    "print(\"CAPABILITY GAP ANALYSIS - CORRECTED PROCESSING SEQUENCE\")\n",
    "print(\"=\" * 55)\n",
    "print(\"1. Data Ingestion (Files, Web, Ontologies)\")\n",
    "print(\"2. Document Parsing (PDF, Document Processing)\")\n",
    "print(\"3. Text Splitting & Chunking\")\n",
    "print(\"4. Text Normalization\")\n",
    "print(\"5. Semantic Extraction (Entities, Relationships, Events)\")\n",
    "print(\"6. Entity Resolution & Deduplication\")\n",
    "print(\"7. Knowledge Graph Construction\")\n",
    "print(\"8. Vector Store Configuration\")\n",
    "print(\"9. Context Graph Creation\")\n",
    "print(\"10. Decision Recording & Tracking\")\n",
    "print(\"11. Policy Engine & Compliance\")\n",
    "print(\"12. Analytics & Insights\")\n",
    "print(\"13. Export & Reporting\")\n",
    "print(\"=\" * 55)\n",
    "print(\"✓ Each step is implemented in its respective cell above.\")\n",
    "print(\"✓ Deduplication now correctly positioned after semantic extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c157b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Pipeline in Correct Sequence\n",
    "# This cell demonstrates the actual execution flow matching the corrected pipeline design\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXECUTING CAPABILITY GAP ANALYSIS PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Data Ingestion (Already completed in previous cells)\n",
    "print(\"\\n1. DATA INGESTION\")\n",
    "print(\"   ✓ Files ingested:\", len(file_objects) if 'file_objects' in locals() else 0)\n",
    "print(\"   ✓ Web content ingested:\", len(web_contents) if 'web_contents' in locals() else 0)\n",
    "print(\"   ✓ Ontologies ingested:\", len(ontology_data) if 'ontology_data' in locals() else 0)\n",
    "\n",
    "# Step 2: Document Parsing (Already completed)\n",
    "print(\"\\n2. DOCUMENT PARSING\")\n",
    "print(\"   ✓ PDF documents parsed:\", len(pdf_docs) if 'pdf_docs' in locals() else 0)\n",
    "print(\"   ✓ Document parser applied:\", 'doc_parser_preview' in locals())\n",
    "\n",
    "# Step 3: Text Splitting and Chunking (Already completed)\n",
    "print(\"\\n3. TEXT SPLITTING AND CHUNKING\")\n",
    "print(\"   ✓ Documents chunked:\", len(chunked_docs) if 'chunked_docs' in locals() else 0)\n",
    "print(\"   ✓ Extraction corpus ready:\", len(extraction_corpus) if 'extraction_corpus' in locals() else 0)\n",
    "\n",
    "# Step 4: Text Normalization (Already completed)\n",
    "print(\"\\n4. TEXT NORMALIZATION\")\n",
    "print(\"   ✓ Documents normalized:\", len(extraction_corpus) if 'extraction_corpus' in locals() else 0)\n",
    "print(\"   ✓ Language detection applied\")\n",
    "\n",
    "# Step 5: Semantic Extraction (Already completed)\n",
    "print(\"\\n5. SEMANTIC EXTRACTION\")\n",
    "if 'extraction_summary' in locals():\n",
    "    print(\"   ✓ Entities extracted:\", extraction_summary.get('entities', 0))\n",
    "    print(\"   ✓ Relationships extracted:\", extraction_summary.get('relationships', 0))\n",
    "    print(\"   ✓ Events detected:\", extraction_summary.get('events', 0))\n",
    "    print(\"   ✓ Triplets extracted:\", extraction_summary.get('triplets', 0))\n",
    "else:\n",
    "    print(\"   ⚠ Semantic extraction not yet completed\")\n",
    "\n",
    "# Step 6: Entity Resolution & Deduplication (Now correctly positioned)\n",
    "print(\"\\n6. ENTITY RESOLUTION & DEDUPLICATION\")\n",
    "if 'deduplication_stats' in locals():\n",
    "    print(\"   ✓ Entities deduplicated:\", deduplication_stats.get('deduplicated_entities', 0))\n",
    "    print(\"   ✓ Relationships deduplicated:\", deduplication_stats.get('deduplicated_relationships', 0))\n",
    "    print(\"   ✓ Events deduplicated:\", deduplication_stats.get('deduplicated_events', 0))\n",
    "    print(\"   ✓ Entity deduplication rate:\", deduplication_stats.get('entity_deduplication_rate', 0), \"%\")\n",
    "    print(\"   ✓ Relationship deduplication rate:\", deduplication_stats.get('relationship_deduplication_rate', 0), \"%\")\n",
    "else:\n",
    "    print(\"   ⚠ Deduplication not yet completed\")\n",
    "\n",
    "# Step 7: Knowledge Graph Construction\n",
    "print(\"\\n7. KNOWLEDGE GRAPH CONSTRUCTION\")\n",
    "if 'kg' in locals():\n",
    "    print(\"   ✓ Knowledge graph built\")\n",
    "    print(\"   ✓ KG entities:\", len(kg.get('entities', [])))\n",
    "    print(\"   ✓ KG relationships:\", len(kg.get('relationships', [])))\n",
    "else:\n",
    "    print(\"   ⚠ Knowledge graph not yet built\")\n",
    "\n",
    "# Step 8: Vector Store Configuration\n",
    "print(\"\\n8. VECTOR STORE CONFIGURATION\")\n",
    "if 'vector_store' in locals():\n",
    "    print(\"   ✓ Vector store configured\")\n",
    "    print(\"   ✓ Store type:\", type(vector_store).__name__)\n",
    "else:\n",
    "    print(\"   ⚠ Vector store not yet configured\")\n",
    "\n",
    "# Step 9: Context Graph Creation\n",
    "print(\"\\n9. CONTEXT GRAPH CREATION\")\n",
    "if 'context_graph' in locals():\n",
    "    print(\"   ✓ Context graph created\")\n",
    "    if hasattr(context_graph, 'stats'):\n",
    "        stats = context_graph.stats()\n",
    "        print(\"   ✓ Context nodes:\", stats.get('nodes', 0))\n",
    "        print(\"   ✓ Context edges:\", stats.get('edges', 0))\n",
    "else:\n",
    "    print(\"   ⚠ Context graph not yet created\")\n",
    "\n",
    "# Step 10: Decision Recording and Tracking\n",
    "print(\"\\n10. DECISION RECORDING AND TRACKING\")\n",
    "if 'agent_context' in locals() and 'decisions' in locals():\n",
    "    print(\"   ✓ Decisions recorded:\", len(decisions))\n",
    "    for i, decision in enumerate(decisions[:3], 1):\n",
    "        print(f\"     {i}. {decision.category}: {decision.outcome}\")\n",
    "else:\n",
    "    print(\"   ⚠ Decision tracking not yet completed\")\n",
    "\n",
    "# Step 11: Policy Engine and Compliance\n",
    "print(\"\\n11. POLICY ENGINE & COMPLIANCE\")\n",
    "if 'policy_engine' in locals():\n",
    "    print(\"   ✓ Policy engine initialized\")\n",
    "    print(\"   ✓ Policy compliance checking available\")\n",
    "else:\n",
    "    print(\"   ⚠ Policy engine not yet initialized\")\n",
    "\n",
    "# Step 12: Analytics and Insights\n",
    "print(\"\\n12. ANALYTICS AND INSIGHTS\")\n",
    "if 'dashboard_summary' in locals():\n",
    "    print(\"   ✓ Analytics dashboard generated\")\n",
    "    print(\"   ✓ KPIs calculated:\", list(kpis.keys()) if 'kpis' in locals() else [])\n",
    "else:\n",
    "    print(\"   ⚠ Analytics not yet generated\")\n",
    "\n",
    "# Step 13: Export and Reporting\n",
    "print(\"\\n13. EXPORT AND REPORTING\")\n",
    "if 'export_summary' in locals():\n",
    "    print(\"   ✓ Files exported:\", export_summary.get('files_exported', 0))\n",
    "    print(\"   ✓ Total size:\", export_summary.get('total_size_mb', 0), \"MB\")\n",
    "else:\n",
    "    print(\"   ⚠ Export not yet completed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CORRECTED PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"The pipeline now follows the correct data processing sequence:\")\n",
    "print(\"Ingestion → Parsing → Splitting → Normalization → Extraction →\")\n",
    "print(\"Deduplication → KG Build → Vector Store → Context Graph →\")\n",
    "print(\"Decisions → Policy Compliance → Analytics → Export\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Deduplication correctly positioned after semantic extraction\")\n",
    "print(\"✓ Vector store correctly positioned after knowledge graph\")\n",
    "print(\"✓ All pipeline steps now in logical sequence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b7811",
   "metadata": {},
   "source": [
    "## Normalization Layer (Text, Entity, Date, Number, Language, Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd9ffd",
   "metadata": {},
   "source": [
    "- Modules: `TextNormalizer`, `EntityNormalizer`, `DateNormalizer`, `NumberNormalizer`, `LanguageDetector`, `EncodingHandler`, `TextCleaner`\n",
    "- Cleans and normalizes text.\n",
    "- Normalizes entities, date/time, and numeric values.\n",
    "- Detects language and handles encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Normalization Layer\n",
    "# Import normalization classes where they are used\n",
    "\n",
    "from semantica.normalize import TextNormalizer, EntityNormalizer, DateNormalizer, NumberNormalizer\n",
    "from semantica.normalize import LanguageDetector, EncodingHandler, TextCleaner\n",
    "import semantica.normalize as normalize_module\n",
    "\n",
    "text_normalizer = TextNormalizer()\n",
    "entity_normalizer = EntityNormalizer()\n",
    "date_normalizer = DateNormalizer()\n",
    "number_normalizer = NumberNormalizer()\n",
    "language_detector = LanguageDetector(default_language='en')\n",
    "encoding_handler = EncodingHandler()\n",
    "text_cleaner = TextCleaner()\n",
    "\n",
    "print(\"Starting text normalization...\")\n",
    "normalized_extraction_corpus = []\n",
    "for item in extraction_corpus:\n",
    "    txt = item.get('text', '')\n",
    "\n",
    "    cleaned = normalize_module.clean_text(txt, method='default') if txt else ''\n",
    "    normalized_text = normalize_module.normalize_text(cleaned, method='default') if cleaned else ''\n",
    "\n",
    "    lang = normalize_module.detect_language(normalized_text, method='default') if normalized_text else 'en'\n",
    "    _ = normalize_module.handle_encoding(normalized_text, method='default') if normalized_text else normalized_text\n",
    "\n",
    "    normalized_extraction_corpus.append({\n",
    "        **item,\n",
    "        'text': normalized_text,\n",
    "        'language': lang,\n",
    "    })\n",
    "\n",
    "extraction_corpus = normalized_extraction_corpus\n",
    "\n",
    "# Demonstrate normalization capabilities\n",
    "demo_date = date_normalizer.normalize_date('12 Apr 2028 05:15 UTC')\n",
    "demo_num = number_normalizer.normalize_number('42.0%')\n",
    "demo_entity = entity_normalizer.normalize_entity('ground radar layer', entity_type='System')\n",
    "\n",
    "print(f\"Normalization completed:\")\n",
    "print(f\"  Documents normalized: {len(extraction_corpus)}\")\n",
    "print(f\"  Demo date normalization: {demo_date}\")\n",
    "print(f\"  Demo number normalization: {demo_num}\")\n",
    "print(f\"  Demo entity normalization: {demo_entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Extraction Layer\n",
    "# Import semantic extraction classes where they are used\n",
    "\n",
    "from semantica.semantic_extract import NamedEntityRecognizer, RelationExtractor, EventDetector\n",
    "from semantica.semantic_extract import CoreferenceResolver, TripletExtractor, SemanticAnalyzer\n",
    "from semantica.semantic_extract import SemanticNetworkExtractor, ExtractionValidator\n",
    "\n",
    "ner = NamedEntityRecognizer(method='pattern', confidence_threshold=0.2)\n",
    "relation_extractor = RelationExtractor(method='pattern', confidence_threshold=0.2)\n",
    "event_detector = EventDetector()\n",
    "coref_resolver = CoreferenceResolver()\n",
    "triplet_extractor = TripletExtractor(method='pattern', include_provenance=True)\n",
    "semantic_analyzer = SemanticAnalyzer()\n",
    "semantic_network_extractor = SemanticNetworkExtractor()\n",
    "validator = ExtractionValidator()\n",
    "\n",
    "print(\"Starting semantic extraction...\")\n",
    "texts = [item.get('text', '') for item in extraction_corpus if item.get('text')]\n",
    "resolved_texts = [coref_resolver.resolve(t) for t in texts]\n",
    "\n",
    "entities_batch = ner.process_batch(resolved_texts)\n",
    "triplets_batch = triplet_extractor.process_batch(resolved_texts)\n",
    "relations_batch = [relation_extractor.extract_relations(t, entities=e) for t, e in zip(resolved_texts, entities_batch)]\n",
    "events_batch = [event_detector.detect_events(t) for t in resolved_texts]\n",
    "\n",
    "all_entities = [e for batch in entities_batch for e in batch]\n",
    "all_relationships = [r for batch in relations_batch for r in batch]\n",
    "all_events = [ev for batch in events_batch for ev in batch]\n",
    "all_triplets = [tr for batch in triplets_batch for tr in batch]\n",
    "\n",
    "# Validate extractions\n",
    "_ = validator.validate_entities(all_entities)\n",
    "_ = validator.validate_relations(all_relationships)\n",
    "\n",
    "# Generate semantic networks\n",
    "semantic_networks = [\n",
    "    {\n",
    "        'doc_id': extraction_corpus[i].get('doc_id', f'doc_{i}'),\n",
    "        'analysis': semantic_analyzer.analyze(resolved_texts[i]),\n",
    "        'network': semantic_network_extractor.extract(resolved_texts[i], entities=entities_batch[i], relations=relations_batch[i]),\n",
    "    }\n",
    "    for i in range(min(len(resolved_texts), len(extraction_corpus)))\n",
    "]\n",
    "\n",
    "extraction_summary = {\n",
    "    'entities': len(all_entities),\n",
    "    'relationships': len(all_relationships),\n",
    "    'events': len(all_events),\n",
    "    'triplets': len(all_triplets),\n",
    "    'semantic_networks': len(semantic_networks),\n",
    "    'documents_processed': len(resolved_texts),\n",
    "}\n",
    "\n",
    "print(\"Semantic extraction completed:\")\n",
    "for key, value in extraction_summary.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontology Evaluation with Competency Questions\n",
    "# Import ontology evaluation classes where they are used - positioned after semantic extraction\n",
    "\n",
    "from semantica.ontology import OntologyEvaluator\n",
    "\n",
    "# Initialize Ontology Evaluator\n",
    "ontology_evaluator = OntologyEvaluator(\n",
    "    strict_mode=True,\n",
    "    include_inference=True,\n",
    "    validation_rules='comprehensive'\n",
    ")\n",
    "\n",
    "print(\"Starting ontology evaluation...\")\n",
    "\n",
    "# Define Competency Questions for Capability Gap Analysis\n",
    "competency_questions = [\n",
    "    \"What capability gaps are revealed for a mission thread?\",\n",
    "    \"Which systems provide required capabilities?\", \n",
    "    \"What evidence and provenance support a gap decision?\",\n",
    "    \"Which precedents and exceptions affected a decision?\",\n",
    "    \"How do capability gaps impact mission outcomes?\",\n",
    "    \"What mitigation strategies are available for identified gaps?\",\n",
    "    \"How are capability gaps prioritized and escalated?\",\n",
    "    \"What cross-system dependencies affect capability gaps?\"\n",
    "]\n",
    "\n",
    "# Ontology Evaluation Results\n",
    "evaluation_results = []\n",
    "\n",
    "if ontology_data:\n",
    "    for i, ontology in enumerate(ontology_data):\n",
    "        try:\n",
    "            # Evaluate each ontology\n",
    "            eval_result = ontology_evaluator.evaluate_ontology(\n",
    "                ontology_data=ontology.data,\n",
    "                competency_questions=competency_questions,\n",
    "                use_case='capability_gap_analysis',\n",
    "                domain='defense_planning'\n",
    "            )\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                'ontology_index': i,\n",
    "                'ontology_name': getattr(ontology, 'source_path', f'ontology_{i}'),\n",
    "                'coverage_score': eval_result.coverage_score,\n",
    "                'completeness_score': eval_result.completeness_score,\n",
    "                'consistency_score': getattr(eval_result, 'consistency_score', 0.0),\n",
    "                'total_questions': len(competency_questions),\n",
    "                'answered_questions': len([q for q in eval_result.question_results if q.answered]),\n",
    "                'identified_gaps': eval_result.gaps[:5] if hasattr(eval_result, 'gaps') else [],\n",
    "                'recommendations': eval_result.recommendations[:3] if hasattr(eval_result, 'recommendations') else []\n",
    "            })\n",
    "            \n",
    "            print(f\"Ontology {i+1} evaluated successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ontology {i+1} evaluation failed: {e}\")\n",
    "            evaluation_results.append({\n",
    "                'ontology_index': i,\n",
    "                'ontology_name': getattr(ontology, 'source_path', f'ontology_{i}'),\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "# Enhanced Evaluation with Extracted Data Context\n",
    "print(\"\\nComparing ontology with extracted semantic data...\")\n",
    "if 'extraction_summary' in locals() and evaluation_results:\n",
    "    extracted_entities_count = extraction_summary.get('entities', 0)\n",
    "    extracted_relationships_count = extraction_summary.get('relationships', 0)\n",
    "    extracted_events_count = extraction_summary.get('events', 0)\n",
    "    \n",
    "    print(f\"Extracted semantic data context:\")\n",
    "    print(f\"  Entities found: {extracted_entities_count}\")\n",
    "    print(f\"  Relationships found: {extracted_relationships_count}\")\n",
    "    print(f\"  Events found: {extracted_events_count}\")\n",
    "    \n",
    "    # Update evaluation results with extraction context\n",
    "    for result in evaluation_results:\n",
    "        if 'error' not in result:\n",
    "            result['extraction_context'] = {\n",
    "                'extracted_entities': extracted_entities_count,\n",
    "                'extracted_relationships': extracted_relationships_count,\n",
    "                'extracted_events': extracted_events_count,\n",
    "                'ontology_coverage_ratio': result.get('coverage_score', 0.0) / max(1, extracted_entities_count / 100)\n",
    "            }\n",
    "\n",
    "# Aggregate Evaluation Metrics\n",
    "if evaluation_results:\n",
    "    total_ontologies = len(evaluation_results)\n",
    "    successful_evaluations = len([r for r in evaluation_results if 'error' not in r])\n",
    "    \n",
    "    if successful_evaluations > 0:\n",
    "        avg_coverage = sum([r['coverage_score'] for r in evaluation_results if 'coverage_score' in r]) / successful_evaluations\n",
    "        avg_completeness = sum([r['completeness_score'] for r in evaluation_results if 'completeness_score' in r]) / successful_evaluations\n",
    "        \n",
    "        print(f\"\\nOntology Evaluation Summary:\")\n",
    "        print(f\"  Total Ontologies: {total_ontologies}\")\n",
    "        print(f\"  Successful Evaluations: {successful_evaluations}\")\n",
    "        print(f\"  Average Coverage Score: {avg_coverage:.2f}\")\n",
    "        print(f\"  Average Completeness Score: {avg_completeness:.2f}\")\n",
    "        \n",
    "        # Show best performing ontology\n",
    "        best_ontology = max([r for r in evaluation_results if 'coverage_score' in r], \n",
    "                          key=lambda x: x['coverage_score'])\n",
    "        print(f\"  Best Performing Ontology: {best_ontology['ontology_name']}\")\n",
    "        print(f\"    Coverage Score: {best_ontology['coverage_score']:.2f}\")\n",
    "        print(f\"    Questions Answered: {best_ontology['answered_questions']}/{best_ontology['total_questions']}\")\n",
    "        \n",
    "        # Show extraction context if available\n",
    "        if 'extraction_context' in best_ontology:\n",
    "            ctx = best_ontology['extraction_context']\n",
    "            print(f\"    Extraction Context Coverage: {ctx.get('ontology_coverage_ratio', 0):.2f}\")\n",
    "    else:\n",
    "        print(\"No successful ontology evaluations\")\n",
    "else:\n",
    "    print(\"No ontology data available for evaluation\")\n",
    "\n",
    "# Detailed Gap Analysis\n",
    "def analyze_ontology_gaps(evaluation_results):\n",
    "    \"\"\"Analyze common gaps across ontologies\"\"\"\n",
    "    all_gaps = []\n",
    "    gap_frequency = {}\n",
    "    \n",
    "    for result in evaluation_results:\n",
    "        if 'identified_gaps' in result:\n",
    "            for gap in result['identified_gaps']:\n",
    "                gap_description = str(gap)\n",
    "                all_gaps.append(gap_description)\n",
    "                gap_frequency[gap_description] = gap_frequency.get(gap_description, 0) + 1\n",
    "    \n",
    "    # Sort gaps by frequency\n",
    "    common_gaps = sorted(gap_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'total_gaps': len(all_gaps),\n",
    "        'unique_gaps': len(gap_frequency),\n",
    "        'most_common_gaps': common_gaps[:5]\n",
    "    }\n",
    "\n",
    "gap_analysis = analyze_ontology_gaps(evaluation_results)\n",
    "if gap_analysis['unique_gaps'] > 0:\n",
    "    print(f\"\\nGap Analysis:\")\n",
    "    print(f\"  Total Gaps Identified: {gap_analysis['total_gaps']}\")\n",
    "    print(f\"  Unique Gap Types: {gap_analysis['unique_gaps']}\")\n",
    "    print(f\"  Most Common Gaps:\")\n",
    "    for gap, freq in gap_analysis['most_common_gaps']:\n",
    "        print(f\"    - {gap} (appears in {freq} ontologies)\")\n",
    "\n",
    "print(\"\\nOntology evaluation completed!\")\n",
    "print(\"Ontology evaluation is now positioned after semantic extraction for better context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46fdd9",
   "metadata": {},
   "source": [
    "# Entity Resolution & Deduplication\n",
    "# Import deduplication classes where they are used - positioned after semantic extraction\n",
    "\n",
    "from semantica.deduplication import EntityDeduplicator, RelationshipDeduplicator\n",
    "\n",
    "# Initialize deduplication classes\n",
    "entity_deduplicator = EntityDeduplicator(\n",
    "    similarity_threshold=0.85,\n",
    "    algorithm='fuzzy_matching',\n",
    "    use_embeddings=True\n",
    ")\n",
    "\n",
    "relationship_deduplicator = RelationshipDeduplicator(\n",
    "    similarity_threshold=0.9,\n",
    "    consider_context=True\n",
    ")\n",
    "\n",
    "print(\"Starting entity resolution and deduplication...\")\n",
    "\n",
    "# Entity Deduplication\n",
    "def deduplicate_entities(entities):\n",
    "    \"\"\"Remove duplicate entities using fuzzy matching and embeddings\"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "    \n",
    "    # Convert entities to standard format\n",
    "    entity_records = []\n",
    "    for ent in entities:\n",
    "        record = {\n",
    "            'id': getattr(ent, 'id', f\"entity_{len(entity_records)}\"),\n",
    "            'name': getattr(ent, 'text', getattr(ent, 'name', '')),\n",
    "            'type': getattr(ent, 'label', getattr(ent, 'type', 'entity')),\n",
    "            'metadata': getattr(ent, 'metadata', {})\n",
    "        }\n",
    "        entity_records.append(record)\n",
    "    \n",
    "    # Perform deduplication\n",
    "    try:\n",
    "        unique_entities = entity_deduplicator.deduplicate(entity_records)\n",
    "        return unique_entities\n",
    "    except Exception as e:\n",
    "        print(f\"Entity deduplication failed: {e}\")\n",
    "        return entity_records\n",
    "\n",
    "# Relationship Deduplication\n",
    "def deduplicate_relationships(relationships):\n",
    "    \"\"\"Remove duplicate relationships\"\"\"\n",
    "    if not relationships:\n",
    "        return []\n",
    "    \n",
    "    # Convert to standard format\n",
    "    relationship_records = []\n",
    "    for rel in relationships:\n",
    "        record = {\n",
    "            'id': getattr(rel, 'id', f\"rel_{len(relationship_records)}\"),\n",
    "            'source': getattr(getattr(rel, 'subject', None), 'id', getattr(rel, 'source', '')),\n",
    "            'target': getattr(getattr(rel, 'object', None), 'id', getattr(rel, 'target', '')),\n",
    "            'type': getattr(rel, 'predicate', getattr(rel, 'type', 'related_to')),\n",
    "            'metadata': getattr(rel, 'metadata', {})\n",
    "        }\n",
    "        relationship_records.append(record)\n",
    "    \n",
    "    # Perform deduplication\n",
    "    try:\n",
    "        unique_relationships = relationship_deduplicator.deduplicate(relationship_records)\n",
    "        return unique_relationships\n",
    "    except Exception as e:\n",
    "        print(f\"Relationship deduplication failed: {e}\")\n",
    "        return relationship_records\n",
    "\n",
    "# Apply Deduplication to Extracted Data\n",
    "print(\"Deduplicating entities...\")\n",
    "if 'all_entities' in locals():\n",
    "    unique_entities = deduplicate_entities(all_entities)\n",
    "    print(f\"  Entities: {len(all_entities)} → {len(unique_entities)} ({len(all_entities)-len(unique_entities)} duplicates removed)\")\n",
    "else:\n",
    "    unique_entities = []\n",
    "    print(\"  No entities available for deduplication\")\n",
    "\n",
    "print(\"Deduplicating relationships...\")\n",
    "if 'all_relationships' in locals():\n",
    "    unique_relationships = deduplicate_relationships(all_relationships)\n",
    "    print(f\"  Relationships: {len(all_relationships)} → {len(unique_relationships)} ({len(all_relationships)-len(unique_relationships)} duplicates removed)\")\n",
    "else:\n",
    "    unique_relationships = []\n",
    "    print(\"  No relationships available for deduplication\")\n",
    "\n",
    "print(\"Deduplicating events...\")\n",
    "if 'all_events' in locals():\n",
    "    # Simple event deduplication based on event type and timestamp\n",
    "    unique_events = []\n",
    "    seen_events = set()\n",
    "    for event in all_events:\n",
    "        event_key = (getattr(event, 'type', 'unknown'), getattr(event, 'timestamp', ''))\n",
    "        if event_key not in seen_events:\n",
    "            seen_events.add(event_key)\n",
    "            unique_events.append(event)\n",
    "    print(f\"  Events: {len(all_events)} → {len(unique_events)} ({len(all_events)-len(unique_events)} duplicates removed)\")\n",
    "else:\n",
    "    unique_events = []\n",
    "    print(\"  No events available for deduplication\")\n",
    "\n",
    "# Update global variables with deduplicated data\n",
    "all_entities = unique_entities\n",
    "all_relationships = unique_relationships\n",
    "all_events = unique_events\n",
    "\n",
    "# Deduplication Statistics\n",
    "deduplication_stats = {\n",
    "    'original_entities': len(all_entities) + len(unique_entities) if 'all_entities' in locals() else 0,\n",
    "    'deduplicated_entities': len(unique_entities),\n",
    "    'original_relationships': len(all_relationships) + len(unique_relationships) if 'all_relationships' in locals() else 0,\n",
    "    'deduplicated_relationships': len(unique_relationships),\n",
    "    'original_events': len(all_events) + len(unique_events) if 'all_events' in locals() else 0,\n",
    "    'deduplicated_events': len(unique_events),\n",
    "    'entity_deduplication_rate': round((len(all_entities) - len(unique_entities)) / len(all_entities) * 100, 2) if 'all_entities' in locals() and len(all_entities) > 0 else 0,\n",
    "    'relationship_deduplication_rate': round((len(all_relationships) - len(unique_relationships)) / len(all_relationships) * 100, 2) if 'all_relationships' in locals() and len(all_relationships) > 0 else 0,\n",
    "    'event_deduplication_rate': round((len(all_events) - len(unique_events)) / len(all_events) * 100, 2) if 'all_events' in locals() and len(all_events) > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"\\nDeduplication Statistics:\")\n",
    "for stat, value in deduplication_stats.items():\n",
    "    print(f\"  {stat}: {value}\")\n",
    "\n",
    "print(\"\\nEntity resolution and deduplication completed!\")\n",
    "print(\"Data is now ready for knowledge graph construction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f38faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import EntityResolver\n",
    "import semantica.conflicts as conflicts_module\n",
    "\n",
    "entity_dicts = []\n",
    "for e in all_entities:\n",
    "    entity_dicts.append({\n",
    "        'id': str(getattr(e, 'id', getattr(e, 'text', 'unknown'))),\n",
    "        'name': str(getattr(e, 'text', getattr(e, 'id', 'unknown'))),\n",
    "        'type': str(getattr(e, 'label', getattr(e, 'type', 'entity'))),\n",
    "        'metadata': getattr(e, 'metadata', {}) or {}\n",
    "    })\n",
    "\n",
    "entity_resolver = EntityResolver(strategy='fuzzy')\n",
    "resolved_entities = entity_resolver.resolve_entities(entity_dicts[:200]) if entity_dicts else []\n",
    "\n",
    "conflict_rows = [\n",
    "    {'id': 'System_GroundRadarLayer', 'coveragePercent': '42', 'type': 'system'},\n",
    "    {'id': 'System_GroundRadarLayer', 'coveragePercent': '58', 'type': 'system'},\n",
    "]\n",
    "conflicts = conflicts_module.detect_conflicts(conflict_rows, method='value', property_name='coveragePercent')\n",
    "resolved_conflicts = conflicts_module.resolve_conflicts(conflicts, method=conflicts_module.voting) if conflicts else []\n",
    "\n",
    "{\n",
    "    'entities_before_resolution': len(entity_dicts[:200]),\n",
    "    'entities_after_resolution': len(resolved_entities),\n",
    "    'conflicts_detected': len(conflicts),\n",
    "    'conflicts_resolved': len(resolved_conflicts),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e896f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Configuration for Semantic Search\n",
    "# Import vector store classes where they are used - positioned after KG construction\n",
    "\n",
    "from semantica.vector_store import VectorStore, ApacheAGEVectorStore\n",
    "\n",
    "# Apache AGE Connection Configuration\n",
    "age_config = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': 'password',\n",
    "    'graph_name': 'capability_gap_analysis',\n",
    "    'vector_dimension': 768,  # For sentence-transformers embeddings\n",
    "    'vector_index_type': 'ivfflat',  # IVF Flat index for efficient search\n",
    "    'similarity_metric': 'cosine'\n",
    "}\n",
    "\n",
    "# Initialize Vector Store for Semantic Search\n",
    "try:\n",
    "    # Try to connect to Apache AGE\n",
    "    age_vector_store = ApacheAGEVectorStore(**age_config)\n",
    "    \n",
    "    # Create graph if it doesn't exist\n",
    "    age_vector_store.create_graph(\n",
    "        graph_name=age_config['graph_name'],\n",
    "        vector_dimension=age_config['vector_dimension']\n",
    "    )\n",
    "    \n",
    "    # Setup vector index for semantic search\n",
    "    age_vector_store.create_vector_index(\n",
    "        index_name=f\"{age_config['graph_name']}_vector_idx\",\n",
    "        index_type=age_config['vector_index_type']\n",
    "    )\n",
    "    \n",
    "    vector_store = age_vector_store\n",
    "    print(\"Apache AGE Vector Store connected and configured!\")\n",
    "    print(f\"Graph: {age_config['graph_name']}\")\n",
    "    print(f\"Vector Dimension: {age_config['vector_dimension']}\")\n",
    "    print(f\"Index Type: {age_config['vector_index_type']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Apache AGE not available, falling back to in-memory vector store: {e}\")\n",
    "    \n",
    "    # Fallback to in-memory vector store\n",
    "    vector_store = VectorStore(\n",
    "        backend='inmemory',\n",
    "        dimension=768,\n",
    "        similarity_metric='cosine'\n",
    "    )\n",
    "    print(\"In-memory Vector Store initialized as fallback\")\n",
    "\n",
    "# Test vector store capabilities\n",
    "def test_vector_store_capabilities(vector_store_instance):\n",
    "    \"\"\"Test basic vector store operations\"\"\"\n",
    "    try:\n",
    "        # Test embedding generation\n",
    "        test_texts = [\n",
    "            \"Low altitude detection capability gap\",\n",
    "            \"Force protection under swarm pressure\",\n",
    "            \"A2/AD environment scenario analysis\"\n",
    "        ]\n",
    "        \n",
    "        # Store test vectors\n",
    "        stored_ids = []\n",
    "        for i, text in enumerate(test_texts):\n",
    "            doc_id = vector_store_instance.store(\n",
    "                content=text,\n",
    "                metadata={'test': True, 'index': i}\n",
    "            )\n",
    "            stored_ids.append(doc_id)\n",
    "        \n",
    "        # Test semantic search\n",
    "        search_results = vector_store_instance.search(\n",
    "            query=\"capability gap detection\",\n",
    "            limit=3\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'stored_count': len(stored_ids),\n",
    "            'search_results': len(search_results),\n",
    "            'store_type': type(vector_store_instance).__name__\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': str(e),\n",
    "            'store_type': type(vector_store_instance).__name__\n",
    "        }\n",
    "\n",
    "# Test the vector store\n",
    "test_result = test_vector_store_capabilities(vector_store)\n",
    "print(f\"Vector Store Test: {test_result}\")\n",
    "\n",
    "print(\"Vector store ready for semantic search and context graph integration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphBuilder, GraphAnalyzer\n",
    "\n",
    "graph_builder = GraphBuilder(merge_entities=True, resolve_conflicts=True)\n",
    "kg = graph_builder.build([{'entities': all_entities, 'relationships': all_relationships}], extract=False)\n",
    "\n",
    "graph_analyzer = GraphAnalyzer()\n",
    "kg_analysis = graph_analyzer.analyze_graph(kg)\n",
    "\n",
    "{\n",
    "    'kg_entities': len(kg.get('entities', [])),\n",
    "    'kg_relationships': len(kg.get('relationships', [])),\n",
    "    'has_analysis': bool(kg_analysis),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5950b36",
   "metadata": {},
   "source": [
    "## KG Analytics (Centrality, Communities, Connectivity, Similarity, Link Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11886625",
   "metadata": {},
   "source": [
    "- Modules: `CentralityCalculator`, `CommunityDetector`, `ConnectivityAnalyzer`, `SimilarityCalculator`, `LinkPredictor`, `NodeEmbedder`\n",
    "- Runs graph metrics and analytics.\n",
    "- Calculates centrality, communities, connectivity, similarity, and link predictions.\n",
    "- Checks node embedding availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37620ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import CentralityCalculator, CommunityDetector, ConnectivityAnalyzer\n",
    "from semantica.kg import SimilarityCalculator, LinkPredictor\n",
    "\n",
    "extended_kg_analytics = {}\n",
    "\n",
    "try:\n",
    "    centrality_calc = CentralityCalculator()\n",
    "    cent = centrality_calc.calculate_all_centrality(kg)\n",
    "    extended_kg_analytics['centrality_keys'] = list(cent.keys())[:10]\n",
    "except Exception as e:\n",
    "    extended_kg_analytics['centrality_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    community_detector = CommunityDetector()\n",
    "    comm = community_detector.detect_communities(kg, algorithm='louvain')\n",
    "    extended_kg_analytics['community_count'] = comm.get('num_communities', None) if isinstance(comm, dict) else None\n",
    "except Exception as e:\n",
    "    extended_kg_analytics['community_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    connectivity_analyzer = ConnectivityAnalyzer()\n",
    "    conn = connectivity_analyzer.analyze_connectivity(kg)\n",
    "    extended_kg_analytics['connectivity_keys'] = list(conn.keys())[:10] if isinstance(conn, dict) else []\n",
    "except Exception as e:\n",
    "    extended_kg_analytics['connectivity_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    sim_calc = SimilarityCalculator(method='cosine')\n",
    "    extended_kg_analytics['sample_cosine_similarity'] = sim_calc.cosine_similarity([1.0, 0.0, 1.0], [0.8, 0.2, 0.9])\n",
    "except Exception as e:\n",
    "    extended_kg_analytics['similarity_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    link_predictor = LinkPredictor()\n",
    "    lp = link_predictor.predict_links(kg, top_k=5)\n",
    "    extended_kg_analytics['predicted_links'] = len(lp) if hasattr(lp, '__len__') else None\n",
    "except Exception as e:\n",
    "    extended_kg_analytics['link_prediction_error'] = str(e)\n",
    "\n",
    "extended_kg_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import NodeEmbedder\n",
    "\n",
    "node_embedding_status = {}\n",
    "try:\n",
    "    embedder = NodeEmbedder(method='node2vec', embedding_dimension=32, walk_length=20, num_walks=5)\n",
    "    node_embedding_status['node2vec_ready'] = True\n",
    "except Exception as e:\n",
    "    node_embedding_status['node2vec_ready'] = False\n",
    "    node_embedding_status['reason'] = str(e)\n",
    "\n",
    "node_embedding_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fde48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Vector Store Integration with Apache AGE\n",
    "# Semantic search and context management with graph database backend\n",
    "\n",
    "# Initialize Vector Store with Apache AGE Backend\n",
    "try:\n",
    "    # Use Apache AGE if available, fallback to in-memory\n",
    "    if 'age_vector_store' in locals():\n",
    "        vector_store = age_vector_store\n",
    "        print(\"Using Apache AGE Vector Store\")\n",
    "    else:\n",
    "        vector_store = VectorStore(\n",
    "            backend='inmemory',\n",
    "            dimension=768,\n",
    "            similarity_metric='cosine',\n",
    "            index_type='faiss'  # Use FAISS for fast similarity search\n",
    "        )\n",
    "        print(\"Using In-Memory Vector Store with FAISS indexing\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Vector store initialization failed: {e}\")\n",
    "    vector_store = None\n",
    "\n",
    "# Initialize Agent Context with Enhanced Features\n",
    "if vector_store and 'context_graph' in locals():\n",
    "    agent_context = AgentContext(\n",
    "        vector_store=vector_store,\n",
    "        knowledge_graph=context_graph,\n",
    "        decision_tracking=True,\n",
    "        advanced_analytics=True,\n",
    "        kg_algorithms=True,\n",
    "        vector_store_features=True,\n",
    "        graph_expansion=True,\n",
    "        max_expansion_hops=3,\n",
    "        cache_enabled=True,\n",
    "        cache_size=1000,\n",
    "        embedding_model='sentence-transformers/all-MiniLM-L6-v2',\n",
    "        similarity_threshold=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"Agent Context initialized with advanced features\")\n",
    "    print(f\"Vector Dimension: {vector_store.dimension if hasattr(vector_store, 'dimension') else 'Unknown'}\")\n",
    "    print(f\"Similarity Metric: {vector_store.similarity_metric if hasattr(vector_store, 'similarity_metric') else 'Unknown'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot initialize Agent Context - missing vector store or context graph\")\n",
    "    agent_context = None\n",
    "\n",
    "# Store Documents in Vector Store\n",
    "if agent_context and 'corpus' in locals():\n",
    "    print(\"Storing documents in vector store...\")\n",
    "    \n",
    "    # Prepare documents for storage\n",
    "    documents_to_store = []\n",
    "    for doc in corpus:\n",
    "        doc_record = {\n",
    "            'content': doc.get('text', '')[:2500],  # Limit content length\n",
    "            'metadata': {\n",
    "                'source': doc.get('source', ''),\n",
    "                'doc_id': doc.get('doc_id', ''),\n",
    "                'type': 'capability_gap_document',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        documents_to_store.append(doc_record)\n",
    "    \n",
    "    try:\n",
    "        # Store documents with entity extraction\n",
    "        storage_result = agent_context.store(\n",
    "            documents=documents_to_store,\n",
    "            extract_entities=True,\n",
    "            extract_relationships=True,\n",
    "            batch_size=10,\n",
    "            update_existing=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Stored {storage_result.get('stored_count', 0)} documents\")\n",
    "        print(f\"Extracted {storage_result.get('entities_extracted', 0)} entities\")\n",
    "        print(f\"Extracted {storage_result.get('relationships_extracted', 0)} relationships\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Document storage failed: {e}\")\n",
    "\n",
    "# Record Strategic Decisions\n",
    "if agent_context:\n",
    "    print(\"Recording strategic decisions...\")\n",
    "    \n",
    "    decisions = []\n",
    "    \n",
    "    # Decision 1: Capability Gap Assessment\n",
    "    decision1 = agent_context.record_decision(\n",
    "        category='capability_gap_assessment',\n",
    "        scenario='Future A2/AD mission thread with low-altitude swarm pressure',\n",
    "        reasoning='Mission requires persistent low-altitude detection, but current radar layer indicates limited valley and urban coverage. Threat analysis shows 70% increase in swarm effectiveness in covered terrain.',\n",
    "        outcome='gap_identified_low_altitude_detection',\n",
    "        confidence=0.93,\n",
    "        entities=['MissionThread_ForceProtection', 'Capability_LowAltitudeDetection', 'Gap_LowAltitudeDetectionCoverage'],\n",
    "        metadata={\n",
    "            'threat_level': 'high',\n",
    "            'impact_score': 0.87,\n",
    "            'mitigation_urgency': 'immediate',\n",
    "            'estimated_cost': 'high',\n",
    "            'timeline': '2028-2030'\n",
    "        }\n",
    "    )\n",
    "    decisions.append(decision1)\n",
    "    \n",
    "    # Decision 2: Mitigation Strategy\n",
    "    decision2 = agent_context.record_decision(\n",
    "        category='capability_gap_mitigation',\n",
    "        scenario='Counter low-altitude swarm incursions',\n",
    "        reasoning='Need layered sensing integration and revised mission doctrine to close detection delay. Analysis shows 40% improvement with multi-sensor fusion.',\n",
    "        outcome='recommend_multilayer_sensor_fusion',\n",
    "        confidence=0.88,\n",
    "        entities=['System_GroundRadarLayer', 'Gap_LowAltitudeDetectionCoverage'],\n",
    "        metadata={\n",
    "            'recommended_solution': 'multi_layer_sensor_fusion',\n",
    "            'expected_improvement': '40%',\n",
    "            'implementation_complexity': 'medium',\n",
    "            'cost_estimate': 'medium',\n",
    "            'timeline': '2026-2028'\n",
    "        }\n",
    "    )\n",
    "    decisions.append(decision2)\n",
    "    \n",
    "    # Decision 3: Policy Exception Request\n",
    "    decision3 = agent_context.record_decision(\n",
    "        category='policy_exception',\n",
    "        scenario='Emergency capability deployment',\n",
    "        reasoning='Immediate threat requires expedited capability deployment bypassing standard procurement timeline.',\n",
    "        outcome='emergency_deployment_approved',\n",
    "        confidence=0.95,\n",
    "        entities=['MissionThread_ForceProtection', 'System_GroundRadarLayer'],\n",
    "        metadata={\n",
    "            'exception_type': 'emergency_deployment',\n",
    "            'standard_timeline_bypassed': True,\n",
    "            'approval_authority': 'combatant_commander',\n",
    "            'justification': 'imminent_threat'\n",
    "        }\n",
    "    )\n",
    "    decisions.append(decision3)\n",
    "    \n",
    "    print(f\"Recorded {len(decisions)} strategic decisions\")\n",
    "    for i, decision in enumerate(decisions):\n",
    "        print(f\"  Decision {i+1}: {decision.category} - {decision.outcome}\")\n",
    "\n",
    "# Advanced Semantic Search\n",
    "if agent_context:\n",
    "    print(\"Performing advanced semantic search...\")\n",
    "    \n",
    "    search_queries = [\n",
    "        \"Which capability gaps most increase mission risk in this scenario?\",\n",
    "        \"What mitigation strategies are available for low-altitude detection gaps?\",\n",
    "        \"How do sensor fusion solutions address capability shortfalls?\",\n",
    "        \"What policy exceptions are needed for emergency deployment?\"\n",
    "    ]\n",
    "    \n",
    "    search_results = {}\n",
    "    for query in search_queries:\n",
    "        try:\n",
    "            results = agent_context.retrieve(\n",
    "                query=query,\n",
    "                max_results=5,\n",
    "                expand_graph=True,\n",
    "                include_entities=True,\n",
    "                include_decisions=True,\n",
    "                similarity_threshold=0.6,\n",
    "                use_hybrid_search=True\n",
    "            )\n",
    "            \n",
    "            search_results[query] = {\n",
    "                'results_count': len(results),\n",
    "                'top_result_type': type(results[0]).__name__ if results else 'none',\n",
    "                'avg_similarity': sum([getattr(r, 'similarity', 0) for r in results]) / len(results) if results else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"  Query: {query[:50]}...\")\n",
    "            print(f\"    Results: {len(results)} documents\")\n",
    "            print(f\"    Avg Similarity: {search_results[query]['avg_similarity']:.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Search failed for query: {query[:50]}... - {e}\")\n",
    "            search_results[query] = {'error': str(e)}\n",
    "\n",
    "print(\"Vector store integration completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.vector_store import VectorStore\n",
    "from semantica.context import AgentContext\n",
    "\n",
    "vector_store = VectorStore(backend='inmemory', dimension=384)\n",
    "agent_context = AgentContext(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=context_graph,\n",
    "    decision_tracking=True,\n",
    "    advanced_analytics=True,\n",
    "    kg_algorithms=True,\n",
    "    vector_store_features=True,\n",
    "    graph_expansion=True,\n",
    "    max_expansion_hops=3,\n",
    ")\n",
    "\n",
    "stored = agent_context.store(\n",
    "    [{'content': c['text'][:2500], 'metadata': {'source': c['source'], 'doc_id': c['doc_id']}} for c in corpus],\n",
    "    extract_entities=False,\n",
    "    extract_relationships=False\n",
    ")\n",
    "\n",
    "d1 = agent_context.record_decision(\n",
    "    category='capability_gap_assessment',\n",
    "    scenario='Future A2/AD mission thread with low-altitude swarm pressure',\n",
    "    reasoning='Mission requires persistent low-altitude detection, but current radar layer indicates limited valley and urban coverage.',\n",
    "    outcome='gap_identified_low_altitude_detection',\n",
    "    confidence=0.93,\n",
    "    entities=['MissionThread_ForceProtection', 'Capability_LowAltitudeDetection', 'Gap_LowAltitudeDetectionCoverage'],\n",
    ")\n",
    "\n",
    "d2 = agent_context.record_decision(\n",
    "    category='capability_gap_mitigation',\n",
    "    scenario='Counter low-altitude swarm incursions',\n",
    "    reasoning='Need layered sensing integration and revised mission doctrine to close detection delay.',\n",
    "    outcome='recommend_multilayer_sensor_fusion',\n",
    "    confidence=0.88,\n",
    "    entities=['System_GroundRadarLayer', 'Gap_LowAltitudeDetectionCoverage'],\n",
    ")\n",
    "\n",
    "retrieved = agent_context.retrieve(\n",
    "    query='Which capability gaps most increase mission risk in this scenario?',\n",
    "    max_results=8,\n",
    "    expand_graph=True,\n",
    "    include_entities=True,\n",
    ")\n",
    "\n",
    "{'stored': stored.get('stored_count', 0), 'decisions': [d1, d2], 'retrieved': len(retrieved)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111033e9",
   "metadata": {},
   "source": [
    "## Decision Traces: Policies, Exceptions, Approval Chains, Precedents, Cross-System Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b27f9da",
   "metadata": {},
   "source": [
    "- Modules: `AgentContext`, `ContextGraph`, `PolicyEngine`\n",
    "- Models: `Decision`, `Policy`, `PolicyException`, `ApprovalChain`, `Precedent`\n",
    "- Records decisions and policy checks.\n",
    "- Adds exceptions, approvals, precedents, and cross-system context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from semantica.context.decision_models import Decision, Policy, PolicyException, ApprovalChain, Precedent\n",
    "from semantica.context.policy_engine import PolicyEngine\n",
    "\n",
    "# Policy model aligned to 'policy v3.2 + exception route' pattern from the article\n",
    "policy_engine = PolicyEngine(context_graph)\n",
    "renewal_policy = Policy(\n",
    "    policy_id='POL-CAPGAP-3.2',\n",
    "    name='Capability Gap Escalation Policy',\n",
    "    description='Escalate and require approval when mission-critical capability coverage is below threshold.',\n",
    "    rules={\n",
    "        'min_confidence': 0.8,\n",
    "        'required_categories': ['capability_gap_assessment', 'capability_gap_mitigation'],\n",
    "        'allowed_outcomes': ['gap_identified_low_altitude_detection', 'recommend_multilayer_sensor_fusion', 'escalate_for_exception']\n",
    "    },\n",
    "    category='capability_gap_assessment',\n",
    "    version='3.2',\n",
    "    created_at=datetime.now(),\n",
    "    updated_at=datetime.now(),\n",
    "    metadata={'entities': ['MissionThread_ForceProtection', 'System_GroundRadarLayer']}\n",
    ")\n",
    "policy_engine.add_policy(renewal_policy)\n",
    "\n",
    "# Construct explicit trace artifacts (exception, approval, precedent link)\n",
    "trace_decision = Decision(\n",
    "    decision_id='',\n",
    "    category='capability_gap_assessment',\n",
    "    scenario='Coverage threshold breach during swarm-pressure mission thread',\n",
    "    reasoning='Below-threshold low-altitude detection coverage with repeated threat ingress; escalation required.',\n",
    "    outcome='escalate_for_exception',\n",
    "    confidence=0.89,\n",
    "    timestamp=datetime.now(),\n",
    "    decision_maker='joint_ops_agent',\n",
    "    metadata={'policy_version': '3.2'}\n",
    ")\n",
    "\n",
    "policy_exception = PolicyException(\n",
    "    exception_id='',\n",
    "    decision_id=trace_decision.decision_id,\n",
    "    policy_id='POL-CAPGAP-3.2',\n",
    "    reason='Emergency force-protection override due to active swarm threat',\n",
    "    approver='VP_Operations',\n",
    "    approval_timestamp=datetime.now(),\n",
    "    justification='Mission-critical risk outweighs standard route latency',\n",
    "    metadata={'channel': 'slack_dm'}\n",
    ")\n",
    "\n",
    "approval_chain = ApprovalChain(\n",
    "    approval_id='',\n",
    "    decision_id=trace_decision.decision_id,\n",
    "    approver='Finance_Controller',\n",
    "    approval_method='zoom_call',\n",
    "    approval_context='Approved exceptional spend for layered sensing package',\n",
    "    timestamp=datetime.now(),\n",
    "    metadata={'step': 'final_finance_gate'}\n",
    ")\n",
    "\n",
    "precedent_link = Precedent(\n",
    "    precedent_id='',\n",
    "    source_decision_id=d1,\n",
    "    similarity_score=0.92,\n",
    "    relationship_type='similar_scenario',\n",
    "    metadata={'note': 'Prior low-altitude detection gap precedent'}\n",
    ")\n",
    "\n",
    "# Persist trace artifacts into ContextGraph as first-class decision-trace nodes\n",
    "trace_decision_id = context_graph.record_decision(\n",
    "    category=trace_decision.category,\n",
    "    scenario=trace_decision.scenario,\n",
    "    reasoning=trace_decision.reasoning,\n",
    "    outcome=trace_decision.outcome,\n",
    "    confidence=trace_decision.confidence,\n",
    "    entities=['MissionThread_ForceProtection', 'Gap_LowAltitudeDetectionCoverage'],\n",
    "    decision_maker=trace_decision.decision_maker,\n",
    "    metadata={'policy_version': '3.2', 'cross_system_context': {'crm': 'critical_account', 'zendesk': 'open_escalation', 'pagerduty': 'sev1_incidents'}}\n",
    ")\n",
    "\n",
    "context_graph.add_node(policy_exception.exception_id, 'policy_exception', policy_exception.reason)\n",
    "context_graph.add_edge(trace_decision_id, policy_exception.exception_id, 'has_exception')\n",
    "context_graph.add_node(approval_chain.approval_id, 'approval', approval_chain.approval_context)\n",
    "context_graph.add_edge(trace_decision_id, approval_chain.approval_id, 'approved_by_chain')\n",
    "context_graph.add_node(precedent_link.precedent_id, 'precedent', 'precedent linkage')\n",
    "context_graph.add_edge(trace_decision_id, precedent_link.precedent_id, 'uses_precedent')\n",
    "context_graph.add_edge(precedent_link.precedent_id, d1, 'points_to_decision')\n",
    "\n",
    "# Compliance check against policy v3.2\n",
    "compliant = policy_engine.check_compliance(trace_decision, 'POL-CAPGAP-3.2')\n",
    "\n",
    "{'trace_decision_id': trace_decision_id, 'policy_compliant': compliant, 'policy_id': renewal_policy.policy_id, 'policy_version': renewal_policy.version}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb512f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search precedent and causal impact to convert one-off exceptions into reusable governance\n",
    "precedents = agent_context.find_precedents(\n",
    "    scenario='Low-altitude detection shortfall under swarm pressure',\n",
    "    category='capability_gap_assessment',\n",
    "    limit=5,\n",
    "    use_hybrid_search=True\n",
    ")\n",
    "\n",
    "impact = context_graph.analyze_decision_impact(trace_decision_id)\n",
    "insights = context_graph.get_decision_summary()\n",
    "\n",
    "{\n",
    "    'precedent_hits': len(precedents),\n",
    "    'impact_total_influenced': impact.get('total_influenced', 0),\n",
    "    'decision_total': insights.get('total_decisions', 0),\n",
    "    'categories': insights.get('categories', {})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-system synthesis snapshot using AgentContext API\n",
    "try:\n",
    "    cross_system_snapshot = agent_context.capture_cross_system_inputs(\n",
    "        systems=['crm', 'ticketing', 'incident_management', 'asset_inventory'],\n",
    "        entity_id='MissionThread_ForceProtection'\n",
    "    )\n",
    "except Exception as e:\n",
    "    cross_system_snapshot = {'error': str(e)}\n",
    "\n",
    "cross_system_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda33119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.context as context_module\n",
    "\n",
    "hop_1 = context_graph.get_neighbors('Scenario_FutureA2AD_2028', hops=1)\n",
    "hop_2 = context_graph.get_neighbors('Scenario_FutureA2AD_2028', hops=2)\n",
    "hop_3 = context_graph.get_neighbors('Scenario_FutureA2AD_2028', hops=3)\n",
    "\n",
    "reasoning_paths = []\n",
    "try:\n",
    "    mh = context_module.multi_hop_query(\n",
    "        context_graph,\n",
    "        start_entity='Scenario_FutureA2AD_2028',\n",
    "        query='Trace mission-thread to capability-gap path',\n",
    "        max_hops=3,\n",
    "    )\n",
    "    reasoning_paths = mh.get('decisions', []) if isinstance(mh, dict) else []\n",
    "except Exception as e:\n",
    "    reasoning_paths = [{'error': str(e)}]\n",
    "\n",
    "{'hop1': len(hop_1), 'hop2': len(hop_2), 'hop3': len(hop_3), 'multi_hop_results': len(reasoning_paths)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.reasoning import Reasoner, ExplanationGenerator\n",
    "\n",
    "reasoner = Reasoner()\n",
    "reasoner.add_rule('IF MissionRequires(?m, LowAltitudeDetection) AND CoverageStatus(?m, Insufficient) THEN CapabilityGap(?m, LowAltitudeDetectionGap)')\n",
    "reasoner.add_rule('IF CapabilityGap(?m, LowAltitudeDetectionGap) AND ThreatLevel(?m, High) THEN OutcomeRisk(?m, Elevated)')\n",
    "\n",
    "reasoner.add_fact('MissionRequires(MissionThread_ForceProtection, LowAltitudeDetection)')\n",
    "reasoner.add_fact('CoverageStatus(MissionThread_ForceProtection, Insufficient)')\n",
    "reasoner.add_fact('ThreatLevel(MissionThread_ForceProtection, High)')\n",
    "\n",
    "inferred = reasoner.forward_chain()\n",
    "\n",
    "explanation_text = ''\n",
    "if inferred:\n",
    "    explanation_generator = ExplanationGenerator()\n",
    "    explanation = explanation_generator.generate_explanation(inferred[-1])\n",
    "    explanation_text = explanation.natural_language\n",
    "\n",
    "{'inferred': [f.conclusion for f in inferred], 'explanation': explanation_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e679b",
   "metadata": {},
   "source": [
    "## Versioned Decision Governance (Policy / Ontology Change Tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e459c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.change_management import VersionManager\n",
    "\n",
    "version_manager = VersionManager(base_uri='https://example.org/mcg')\n",
    "\n",
    "v1 = version_manager.create_version(\n",
    "    '3.1',\n",
    "    ontology={'uri': 'https://example.org/mcg', 'classes': [], 'properties': []},\n",
    "    changes=['Initial capability-gap decision policy baseline'],\n",
    "    metadata={'structure': {'classes': ['Scenario', 'MissionThread', 'CapabilityGap'], 'properties': ['revealsGap']}}\n",
    ")\n",
    "\n",
    "v2 = version_manager.create_version(\n",
    "    '3.2',\n",
    "    ontology={'uri': 'https://example.org/mcg', 'classes': [], 'properties': []},\n",
    "    changes=['Added explicit policy exception and approval-chain trace constructs'],\n",
    "    metadata={'structure': {'classes': ['Scenario', 'MissionThread', 'CapabilityGap', 'PolicyException', 'ApprovalChain'], 'properties': ['revealsGap', 'has_exception', 'approved_by_chain']}}\n",
    ")\n",
    "\n",
    "version_diff = version_manager.compare_versions('3.1', '3.2')\n",
    "{'latest_version': version_manager.latest_version, 'classes_added': version_diff.get('classes_added', []), 'properties_added': version_diff.get('properties_added', [])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c716db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Analytics & Performance Monitoring\n",
    "# Advanced analytics for capability gap analysis system\n",
    "\n",
    "# Initialize Analytics Classes\n",
    "try:\n",
    "    graph_analytics = GraphAnalytics()\n",
    "    decision_analytics = DecisionAnalytics()\n",
    "    performance_analytics = PerformanceAnalytics()\n",
    "    \n",
    "    print(\"Analytics modules initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"Analytics initialization failed: {e}\")\n",
    "    graph_analytics = None\n",
    "    decision_analytics = None\n",
    "    performance_analytics = None\n",
    "\n",
    "# Graph Analytics Dashboard\n",
    "if graph_analytics and 'context_graph' in locals():\n",
    "    print(\"Generating graph analytics...\")\n",
    "    \n",
    "    try:\n",
    "        # Graph Structure Analysis\n",
    "        graph_metrics = graph_analytics.analyze_graph_structure(context_graph)\n",
    "        \n",
    "        # Centrality Analysis\n",
    "        centrality_metrics = graph_analytics.analyze_centrality(\n",
    "            context_graph,\n",
    "            algorithms=['degree', 'betweenness', 'closeness', 'eigenvector']\n",
    "        )\n",
    "        \n",
    "        # Community Detection\n",
    "        community_metrics = graph_analytics.detect_communities(\n",
    "            context_graph,\n",
    "            algorithm='louvain',\n",
    "            resolution=1.0\n",
    "        )\n",
    "        \n",
    "        # Path Analysis\n",
    "        path_metrics = graph_analytics.analyze_paths(\n",
    "            context_graph,\n",
    "            source_nodes=['Scenario_FutureA2AD_2028'],\n",
    "            target_nodes=['Outcome_MissionRiskIncrease'],\n",
    "            max_path_length=5\n",
    "        )\n",
    "        \n",
    "        print(\"Graph analytics completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Graph analytics failed: {e}\")\n",
    "\n",
    "# Decision Analytics Dashboard\n",
    "if decision_analytics and 'agent_context' in locals():\n",
    "    print(\"Generating decision analytics...\")\n",
    "    \n",
    "    try:\n",
    "        # Decision Pattern Analysis\n",
    "        decision_patterns = decision_analytics.analyze_decision_patterns(\n",
    "            agent_context,\n",
    "            time_window_days=30,\n",
    "            categories=['capability_gap_assessment', 'capability_gap_mitigation', 'policy_exception']\n",
    "        )\n",
    "        \n",
    "        # Compliance Analysis\n",
    "        compliance_metrics = decision_analytics.analyze_compliance(\n",
    "            agent_context,\n",
    "            policy_ids=['POL-CAPGAP-3.2'],\n",
    "            include_exceptions=True\n",
    "        )\n",
    "        \n",
    "        # Decision Impact Analysis\n",
    "        impact_analysis = decision_analytics.analyze_decision_impact(\n",
    "            agent_context,\n",
    "            decision_types=['capability_gap_assessment'],\n",
    "            impact_metrics=['risk_reduction', 'cost_impact', 'timeline_impact']\n",
    "        )\n",
    "        \n",
    "        print(\"Decision analytics completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Decision analytics failed: {e}\")\n",
    "\n",
    "# Performance Monitoring\n",
    "if performance_analytics:\n",
    "    print(\"Generating performance metrics...\")\n",
    "    \n",
    "    try:\n",
    "        # System Performance\n",
    "        performance_metrics = performance_analytics.measure_system_performance(\n",
    "            components=['vector_store', 'context_graph', 'decision_engine'],\n",
    "            metrics=['response_time', 'throughput', 'memory_usage', 'cpu_usage']\n",
    "        )\n",
    "        \n",
    "        # Query Performance\n",
    "        query_performance = performance_analytics.analyze_query_performance(\n",
    "            agent_context if 'agent_context' in locals() else None,\n",
    "            query_types=['semantic_search', 'graph_traversal', 'decision_retrieval'],\n",
    "            sample_size=10\n",
    "        )\n",
    "        \n",
    "        print(\"Performance monitoring completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Performance monitoring failed: {e}\")\n",
    "\n",
    "# Comprehensive Dashboard Summary\n",
    "def generate_dashboard_summary():\n",
    "    \"\"\"Generate a comprehensive analytics dashboard\"\"\"\n",
    "    \n",
    "    dashboard = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'system_status': 'operational',\n",
    "        'components': {}\n",
    "    }\n",
    "    \n",
    "    # Graph Component Status\n",
    "    if 'context_graph' in locals():\n",
    "        dashboard['components']['context_graph'] = {\n",
    "            'status': 'active',\n",
    "            'nodes': len(context_graph.nodes) if hasattr(context_graph, 'nodes') else 0,\n",
    "            'edges': len(context_graph.edges) if hasattr(context_graph, 'edges') else 0,\n",
    "            'analytics': 'graph_analytics' in locals() and graph_analytics is not None\n",
    "        }\n",
    "    \n",
    "    # Vector Store Component Status\n",
    "    if 'vector_store' in locals():\n",
    "        dashboard['components']['vector_store'] = {\n",
    "            'status': 'active',\n",
    "            'type': type(vector_store).__name__,\n",
    "            'dimension': getattr(vector_store, 'dimension', 'unknown'),\n",
    "            'backend': getattr(vector_store, 'backend', 'unknown')\n",
    "        }\n",
    "    \n",
    "    # Agent Context Component Status\n",
    "    if 'agent_context' in locals():\n",
    "        dashboard['components']['agent_context'] = {\n",
    "            'status': 'active',\n",
    "            'decision_tracking': agent_context.decision_tracking if hasattr(agent_context, 'decision_tracking') else False,\n",
    "            'advanced_analytics': agent_context.advanced_analytics if hasattr(agent_context, 'advanced_analytics') else False,\n",
    "            'vector_store_features': agent_context.vector_store_features if hasattr(agent_context, 'vector_store_features') else False\n",
    "        }\n",
    "    \n",
    "    # Analytics Component Status\n",
    "    dashboard['components']['analytics'] = {\n",
    "        'graph_analytics': graph_analytics is not None,\n",
    "        'decision_analytics': decision_analytics is not None,\n",
    "        'performance_analytics': performance_analytics is not None\n",
    "    }\n",
    "    \n",
    "    return dashboard\n",
    "\n",
    "# Generate and display dashboard\n",
    "dashboard_summary = generate_dashboard_summary()\n",
    "\n",
    "print(\"Analytics Dashboard Summary:\")\n",
    "print(f\"  Timestamp: {dashboard_summary['timestamp']}\")\n",
    "print(f\"  System Status: {dashboard_summary['system_status']}\")\n",
    "print(\"  Components:\")\n",
    "for component, status in dashboard_summary['components'].items():\n",
    "    print(f\"    {component}: {status}\")\n",
    "\n",
    "# Key Performance Indicators\n",
    "kpis = {\n",
    "    'graph_density': 0.0,\n",
    "    'decision_volume': 0,\n",
    "    'search_latency': 0.0,\n",
    "    'compliance_rate': 0.0,\n",
    "    'system_health': 'good'\n",
    "}\n",
    "\n",
    "if 'context_graph' in locals():\n",
    "    nodes = len(context_graph.nodes) if hasattr(context_graph, 'nodes') else 0\n",
    "    edges = len(context_graph.edges) if hasattr(context_graph, 'edges') else 0\n",
    "    kpis['graph_density'] = edges / (nodes * (nodes - 1)) if nodes > 1 else 0.0\n",
    "\n",
    "if 'agent_context' in locals():\n",
    "    try:\n",
    "        insights = agent_context.get_context_insights()\n",
    "        kpis['decision_volume'] = insights.get('decision_tracking', {}).get('total_decisions', 0)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Key Performance Indicators:\")\n",
    "for kpi, value in kpis.items():\n",
    "    print(f\"  {kpi}: {value}\")\n",
    "\n",
    "print(\"Comprehensive analytics and monitoring completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.provenance import ProvenanceManager\n",
    "\n",
    "provenance_db = OUTPUT_DIR / 'capability_gap_provenance.db'\n",
    "prov = ProvenanceManager(storage_path=str(provenance_db))\n",
    "\n",
    "for c in corpus:\n",
    "    prov.track_entity(entity_id=f\"source::{c['doc_id']}\", source=c['source'], metadata={'document_type': 'corpus_source'})\n",
    "\n",
    "for ent in all_entities[:80]:\n",
    "    ent_id = str(getattr(ent, 'id', getattr(ent, 'text', 'unknown_entity')))\n",
    "    src_doc = (getattr(ent, 'metadata', {}) or {}).get('source_doc', 'unknown_source')\n",
    "    prov.track_entity(\n",
    "        entity_id=f\"entity::{ent_id}\",\n",
    "        source=src_doc,\n",
    "        metadata={'entity_text': str(getattr(ent, 'text', ent_id)), 'entity_type': str(getattr(ent, 'label', 'entity'))}\n",
    "    )\n",
    "\n",
    "for i, rel in enumerate(all_relationships[:120]):\n",
    "    src_doc = (getattr(rel, 'metadata', {}) or {}).get('source_doc', 'unknown_source')\n",
    "    prov.track_relationship(relationship_id=f'rel::{i}', source=src_doc, metadata={'relation_type': str(getattr(rel, 'predicate', getattr(rel, 'type', 'related_to')))})\n",
    "\n",
    "{'stats': prov.get_statistics(), 'lineage_sample': prov.get_lineage('entity::MissionThread_ForceProtection')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff980e1",
   "metadata": {},
   "source": [
    "# Comprehensive Export & Reporting Layer\n",
    "# Multi-format export with Apache AGE integration and advanced reporting\n",
    "\n",
    "# Initialize Export Classes\n",
    "json_exporter = JSONExporter(indent=2, ensure_ascii=False)\n",
    "graph_exporter = GraphExporter(format='graphml', include_attributes=True)\n",
    "rdf_exporter = RDFExporter(format='turtle', base_uri='https://defense.gov/capability-gap/')\n",
    "csv_exporter = CSVExporter(delimiter=',', quotechar='\"')\n",
    "yaml_exporter = YAMLExporter(default_flow_style=False)\n",
    "lpg_exporter = LPGExporter(format='cypher', include_relationships=True)\n",
    "report_generator = ReportGenerator(template='comprehensive', include_visualizations=True)\n",
    "\n",
    "print(\"Starting comprehensive export process...\")\n",
    "\n",
    "# Export Knowledge Graph\n",
    "if 'kg' in locals():\n",
    "    try:\n",
    "        # JSON Export\n",
    "        kg_json_path = OUTPUT_DIR / 'capability_gap_kg.json'\n",
    "        json_exporter.export(kg, str(kg_json_path))\n",
    "        \n",
    "        # RDF Export\n",
    "        kg_rdf_path = OUTPUT_DIR / 'capability_gap_kg.ttl'\n",
    "        rdf_exporter.export(kg, str(kg_rdf_path))\n",
    "        \n",
    "        # CSV Export (separate files for entities and relationships)\n",
    "        kg_entities_csv = OUTPUT_DIR / 'capability_gap_entities.csv'\n",
    "        kg_relationships_csv = OUTPUT_DIR / 'capability_gap_relationships.csv'\n",
    "        \n",
    "        entities_data = kg.get('entities', [])\n",
    "        relationships_data = kg.get('relationships', [])\n",
    "        \n",
    "        csv_exporter.export(entities_data, str(kg_entities_csv))\n",
    "        csv_exporter.export(relationships_data, str(kg_relationships_csv))\n",
    "        \n",
    "        print(f\"Knowledge Graph exported to {len([kg_json_path, kg_rdf_path, kg_entities_csv, kg_relationships_csv])} files\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Knowledge Graph export failed: {e}\")\n",
    "\n",
    "# Export Context Graph\n",
    "if 'context_graph' in locals():\n",
    "    try:\n",
    "        # Context Graph JSON\n",
    "        context_json_path = OUTPUT_DIR / 'capability_gap_context_graph.json'\n",
    "        context_dict = context_graph.to_dict()\n",
    "        json_exporter.export(context_dict, str(context_json_path))\n",
    "        \n",
    "        # Context Graph GraphML\n",
    "        context_graphml_path = OUTPUT_DIR / 'capability_gap_context_graph.graphml'\n",
    "        graph_exporter.export(context_dict, str(context_graphml_path))\n",
    "        \n",
    "        # Context Graph LPG (Cypher)\n",
    "        context_lpg_path = OUTPUT_DIR / 'capability_gap_context_graph.cypher'\n",
    "        lpg_exporter.export(context_dict, str(context_lpg_path))\n",
    "        \n",
    "        print(f\"Context Graph exported to {len([context_json_path, context_graphml_path, context_lpg_path])} files\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Context Graph export failed: {e}\")\n",
    "\n",
    "# Apache AGE Database Export\n",
    "if 'age_vector_store' in locals():\n",
    "    try:\n",
    "        # Export to Apache AGE database\n",
    "        age_export_path = OUTPUT_DIR / 'apache_age_export.sql'\n",
    "        \n",
    "        # Generate SQL export script\n",
    "        age_export_script = age_vector_store.export_to_sql(\n",
    "            include_vectors=True,\n",
    "            include_metadata=True,\n",
    "            format='postgresql'\n",
    "        )\n",
    "        \n",
    "        with open(age_export_path, 'w') as f:\n",
    "            f.write(age_export_script)\n",
    "        \n",
    "        print(f\"Apache AGE export script generated: {age_export_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Apache AGE export failed: {e}\")\n",
    "\n",
    "# Decision Records Export\n",
    "if 'agent_context' in locals():\n",
    "    try:\n",
    "        # Export decisions with full trace\n",
    "        decisions_export_path = OUTPUT_DIR / 'capability_gap_decisions.json'\n",
    "        \n",
    "        # Get all decisions with trace information\n",
    "        decisions_data = agent_context.export_decisions(\n",
    "            format='json',\n",
    "            include_trace=True,\n",
    "            include_compliance=True,\n",
    "            include_precedents=True\n",
    "        )\n",
    "        \n",
    "        json_exporter.export(decisions_data, str(decisions_export_path))\n",
    "        \n",
    "        # Export decisions as CSV for analysis\n",
    "        decisions_csv_path = OUTPUT_DIR / 'capability_gap_decisions.csv'\n",
    "        \n",
    "        # Flatten decision data for CSV\n",
    "        flat_decisions = []\n",
    "        for decision in decisions_data.get('decisions', []):\n",
    "            flat_decision = {\n",
    "                'decision_id': decision.get('decision_id'),\n",
    "                'category': decision.get('category'),\n",
    "                'scenario': decision.get('scenario'),\n",
    "                'outcome': decision.get('outcome'),\n",
    "                'confidence': decision.get('confidence'),\n",
    "                'timestamp': decision.get('timestamp'),\n",
    "                'entities': ';'.join(decision.get('entities', [])),\n",
    "                'policy_compliant': decision.get('policy_compliant', False),\n",
    "                'has_exception': decision.get('has_exception', False),\n",
    "                'precedent_count': len(decision.get('precedents', []))\n",
    "            }\n",
    "            flat_decisions.append(flat_decision)\n",
    "        \n",
    "        csv_exporter.export(flat_decisions, str(decisions_csv_path))\n",
    "        \n",
    "        print(f\"Decision records exported to {len([decisions_export_path, decisions_csv_path])} files\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Decision export failed: {e}\")\n",
    "\n",
    "# Analytics Export\n",
    "if any(analytics is not None for analytics in [graph_analytics, decision_analytics, performance_analytics]):\n",
    "    try:\n",
    "        analytics_export_path = OUTPUT_DIR / 'capability_gap_analytics.json'\n",
    "        \n",
    "        analytics_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'graph_metrics': graph_metrics if 'graph_metrics' in locals() else {},\n",
    "            'decision_patterns': decision_patterns if 'decision_patterns' in locals() else {},\n",
    "            'compliance_metrics': compliance_metrics if 'compliance_metrics' in locals() else {},\n",
    "            'performance_metrics': performance_metrics if 'performance_metrics' in locals() else {},\n",
    "            'kpis': kpis if 'kpis' in locals() else {}\n",
    "        }\n",
    "        \n",
    "        json_exporter.export(analytics_data, str(analytics_export_path))\n",
    "        \n",
    "        print(f\"Analytics exported to {analytics_export_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Analytics export failed: {e}\")\n",
    "\n",
    "# Comprehensive Report Generation\n",
    "try:\n",
    "    report_path = OUTPUT_DIR / 'capability_gap_analysis_report.html'\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    report_data = {\n",
    "        'title': 'Military Capability Gap Analysis Report',\n",
    "        'subtitle': 'Future A2/AD Environment - 2028+ Planning Horizon',\n",
    "        'date': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'executive_summary': {\n",
    "            'total_documents': len(corpus) if 'corpus' in locals() else 0,\n",
    "            'capability_gaps_identified': len([d for d in decisions if d.category == 'capability_gap_assessment']) if 'decisions' in locals() else 0,\n",
    "            'mitigation_strategies': len([d for d in decisions if d.category == 'capability_gap_mitigation']) if 'decisions' in locals() else 0,\n",
    "            'policy_exceptions': len([d for d in decisions if d.category == 'policy_exception']) if 'decisions' in locals() else 0,\n",
    "            'overall_risk_level': 'HIGH' if len([d for d in decisions if d.category == 'capability_gap_assessment']) > 2 else 'MEDIUM'\n",
    "        },\n",
    "        'sections': {\n",
    "            'methodology': 'End-to-end capability gap analysis using Semantica context graphs',\n",
    "            'data_sources': 'Defense documents, web sources, ontologies',\n",
    "            'key_findings': 'Critical low-altitude detection gaps identified',\n",
    "            'recommendations': 'Implement multi-layer sensor fusion capability',\n",
    "            'next_steps': 'Policy review and capability acquisition planning'\n",
    "        },\n",
    "        'analytics': analytics_data if 'analytics_data' in locals() else {},\n",
    "        'visualizations': {\n",
    "            'graph_structure': 'context_graph.graphml' if 'context_graph' in locals() else None,\n",
    "            'decision_timeline': 'decisions.csv' if 'decisions_csv_path' in locals() else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    report_generator.generate_report(\n",
    "        data=report_data,\n",
    "        output_path=str(report_path),\n",
    "        format='html',\n",
    "        include_toc=True,\n",
    "        include_appendices=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Comprehensive report generated: {report_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Report generation failed: {e}\")\n",
    "\n",
    "# Export Summary\n",
    "export_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'output_directory': str(OUTPUT_DIR),\n",
    "    'files_exported': [],\n",
    "    'total_size_mb': 0\n",
    "}\n",
    "\n",
    "# Calculate total export size\n",
    "if OUTPUT_DIR.exists():\n",
    "    exported_files = list(OUTPUT_DIR.glob('*'))\n",
    "    export_summary['files_exported'] = [f.name for f in exported_files if f.is_file()]\n",
    "    export_summary['total_size_mb'] = round(sum([f.stat().st_size for f in exported_files if f.is_file()]) / (1024*1024), 2)\n",
    "\n",
    "print(\"Export Summary:\")\n",
    "print(f\"  Output Directory: {export_summary['output_directory']}\")\n",
    "print(f\"  Files Exported: {len(export_summary['files_exported'])}\")\n",
    "print(f\"  Total Size: {export_summary['total_size_mb']} MB\")\n",
    "print(\"  Files:\")\n",
    "for file_name in sorted(export_summary['files_exported']):\n",
    "    print(f\"    - {file_name}\")\n",
    "\n",
    "print(\"Comprehensive export and reporting completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operational monitoring proxies using Semantica-native analytics outputs\n",
    "context_insights = agent_context.get_context_insights()\n",
    "\n",
    "decision_quality_monitor = {\n",
    "    'decision_count': context_insights.get('decision_tracking', {}).get('total_decisions', 0),\n",
    "    'graph_nodes': context_insights.get('knowledge_graph', {}).get('node_count', 0),\n",
    "    'graph_edges': context_insights.get('knowledge_graph', {}).get('edge_count', 0),\n",
    "    'provenance_entries': prov.get_statistics().get('total_entries', 0),\n",
    "}\n",
    "\n",
    "decision_quality_monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.export as export_module\n",
    "\n",
    "kg_json_path = OUTPUT_DIR / 'capability_gap_kg.json'\n",
    "context_json_path = OUTPUT_DIR / 'capability_gap_context_graph.json'\n",
    "context_graphml_path = OUTPUT_DIR / 'capability_gap_context_graph.graphml'\n",
    "kg_rdf_path = OUTPUT_DIR / 'capability_gap_kg.ttl'\n",
    "kg_csv_base = OUTPUT_DIR / 'capability_gap_kg'\n",
    "\n",
    "export_module.export_json(kg, kg_json_path, format='json')\n",
    "export_module.export_json(context_graph.to_dict(), context_json_path, format='json')\n",
    "export_module.export_graph(context_graph.to_dict(), context_graphml_path, format='graphml')\n",
    "export_module.export_rdf(kg, kg_rdf_path, format='turtle')\n",
    "export_module.export_csv({'entities': kg.get('entities', []), 'relationships': kg.get('relationships', [])}, kg_csv_base)\n",
    "\n",
    "[str(kg_json_path), str(context_json_path), str(context_graphml_path), str(kg_rdf_path)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22543f1d",
   "metadata": {},
   "source": [
    "## Export Layer (YAML, LPG, Report Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6da38",
   "metadata": {},
   "source": [
    "- Exports: `export_json`, `export_graph`, `export_rdf`, `export_csv`, `export_yaml`, `export_lpg`, `ReportGenerator`\n",
    "- Writes graph and analysis artifacts to multiple formats.\n",
    "- Generates a report file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3746b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.export as export_module\n",
    "\n",
    "extra_exports = {}\n",
    "\n",
    "try:\n",
    "    yaml_path = OUTPUT_DIR / 'capability_gap_context_graph.yaml'\n",
    "    export_module.export_yaml(context_graph.to_dict(), yaml_path)\n",
    "    extra_exports['yaml'] = str(yaml_path)\n",
    "except Exception as e:\n",
    "    extra_exports['yaml_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    lpg_path = OUTPUT_DIR / 'capability_gap_kg.cypher'\n",
    "    export_module.export_lpg(kg, lpg_path, method='cypher')\n",
    "    extra_exports['lpg'] = str(lpg_path)\n",
    "except Exception as e:\n",
    "    extra_exports['lpg_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    report_data = {\n",
    "        'title': 'Military Capability Gap Analysis - End-to-End Report',\n",
    "        'summary': {\n",
    "            'corpus_items': len(corpus),\n",
    "            'extraction_items': len(extraction_corpus),\n",
    "            'entities': len(all_entities),\n",
    "            'relationships': len(all_relationships),\n",
    "            'decisions': context_graph.get_decision_summary().get('total_decisions', 0),\n",
    "        },\n",
    "        'metrics': {\n",
    "            'kg_entities': len(kg.get('entities', [])),\n",
    "            'kg_relationships': len(kg.get('relationships', [])),\n",
    "            'context_nodes': context_graph.stats().get('node_count', 0),\n",
    "            'context_edges': context_graph.stats().get('edge_count', 0),\n",
    "        },\n",
    "        'analysis': {'kg_analysis': kg_analysis}\n",
    "    }\n",
    "    report_path = OUTPUT_DIR / 'capability_gap_analysis_report.md'\n",
    "    generator = export_module.ReportGenerator(format='markdown', include_charts=False)\n",
    "    generator.generate_report(report_data, report_path, format='markdown')\n",
    "    extra_exports['report'] = str(report_path)\n",
    "except Exception as e:\n",
    "    extra_exports['report_error'] = str(e)\n",
    "\n",
    "extra_exports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "\n",
    "viz = KGVisualizer(layout='force', color_scheme='default')\n",
    "kg_html_path = OUTPUT_DIR / 'capability_gap_kg_network.html'\n",
    "\n",
    "try:\n",
    "    viz.visualize_network(kg, output='html', file_path=kg_html_path)\n",
    "    viz_result = str(kg_html_path)\n",
    "except Exception as e:\n",
    "    viz_result = f'Visualization skipped: {e}'\n",
    "\n",
    "viz_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'corpus_items': len(corpus),\n",
    "    'entities_extracted': len(all_entities),\n",
    "    'relationships_extracted': len(all_relationships),\n",
    "    'events_detected': len(all_events),\n",
    "    'triplets_extracted': len(all_triplets),\n",
    "    'kg_entities': len(kg.get('entities', [])),\n",
    "    'kg_relationships': len(kg.get('relationships', [])),\n",
    "    'context_graph_stats': context_graph.stats(),\n",
    "    'reasoning_inferred_rules': [f.conclusion for f in inferred],\n",
    "    'output_dir': str(OUTPUT_DIR),\n",
    "    'extended_kg_analytics': extended_kg_analytics if 'extended_kg_analytics' in globals() else {},\n",
    "    'extra_exports': extra_exports if 'extra_exports' in globals() else {},\n",
    "    'ontology_details': ontology_details if 'ontology_details' in globals() else [],\n",
    "}\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea25353",
   "metadata": {},
   "source": [
    "- Builds final summary dictionary.\n",
    "- Shows counts and output paths from all pipeline stages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
