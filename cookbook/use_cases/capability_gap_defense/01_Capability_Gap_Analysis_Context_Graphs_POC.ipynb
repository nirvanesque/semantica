{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729d3236",
   "metadata": {},
   "source": [
    "# Military Capability Gap Analysis with Semantica Context Graphs\n",
    "\n",
    "## Use case scope\n",
    "- Capability gap analysis for defense planning under future scenarios.\n",
    "- End-to-end flow from source documents and ontologies to decision traces and exports.\n",
    "- Context graph pattern: Scenario -> Mission Thread -> Events -> Systems -> Capabilities -> Gaps -> Decisions -> Outcomes.\n",
    "\n",
    "## Questions answered in this notebook\n",
    "- What capability gaps are present for a given mission thread?\n",
    "- Which evidence and sources support each gap?\n",
    "- Which precedents, exceptions, and approvals were used for decisions?\n",
    "- What multi-hop paths connect scenario context to risk outcomes?\n",
    "\n",
    "## Semantica features used in this use case\n",
    "- Ingestion: `FileIngestor`, `WebIngestor`, `OntologyIngestor`\n",
    "- Parsing: `PDFParser`, `DocumentParser`, optional `DoclingParser`\n",
    "- Ontology: `ingest_ontology`, `OntologyEvaluator`\n",
    "- Split: `TextSplitter`, `SemanticChunker`, `StructuralChunker`\n",
    "- Normalization: `TextNormalizer`, `EntityNormalizer`, `DateNormalizer`, `NumberNormalizer`, `LanguageDetector`, `EncodingHandler`, `TextCleaner`\n",
    "- Semantic extraction: `NamedEntityRecognizer`, `RelationExtractor`, `EventDetector`, `CoreferenceResolver`, `TripletExtractor`, `SemanticAnalyzer`, `SemanticNetworkExtractor`, `ExtractionValidator`\n",
    "- KG: `GraphBuilder`, `GraphAnalyzer`, `CentralityCalculator`, `CommunityDetector`, `ConnectivityAnalyzer`, `SimilarityCalculator`, `LinkPredictor`, `PathFinder`, `EntityResolver`\n",
    "- Context and decisions: `ContextGraph`, `AgentContext`, `PolicyEngine`, `Decision`, `Policy`, `PolicyException`, `ApprovalChain`, `Precedent`\n",
    "- Reasoning: `Reasoner`, `ExplanationGenerator`\n",
    "- Provenance and governance: `ProvenanceManager`, `VersionManager`\n",
    "- Export and reporting: `export_json`, `export_graph`, `export_rdf`, `export_csv`, `export_yaml`, `export_lpg`, `ReportGenerator`\n",
    "- Visualization: `KGVisualizer`\n",
    "\n",
    "## Expected outputs\n",
    "- Capability-gap context graph and knowledge graph artifacts.\n",
    "- Decision trace records with policy, exception, approval, and precedent links.\n",
    "- Multi-format exports (JSON, RDF, GraphML, CSV, YAML, LPG, report).\n",
    "- Summary metrics for ingestion, extraction, graph analytics, reasoning, provenance, and export stages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5e471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad777f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "USE_CASE_DIR = BASE_DIR / 'cookbook' / 'use_cases' / 'capability_gap_defense'\n",
    "DATA_DIR = USE_CASE_DIR / 'data'\n",
    "OUTPUT_DIR = USE_CASE_DIR / 'outputs'\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_DIR, DATA_DIR, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbce867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.ingest as ingest_module\n",
    "\n",
    "required_files = [\n",
    "    'rand_competing_without_fighting_2022.pdf',\n",
    "    'us_navy_it_strategic_plan_fy2023.pdf',\n",
    "    'prov.ttl',\n",
    "    'd3fend.ttl',\n",
    "    'military_capability_gap_ontology.ttl',\n",
    "    'military_capability_gap_instances.ttl',\n",
    "]\n",
    "\n",
    "present_files = sorted([f.name for f in DATA_DIR.glob('*')])\n",
    "missing_files = [f for f in required_files if f not in present_files]\n",
    "\n",
    "web_sources = [\n",
    "    'https://www.rand.org/pubs/research_reports/RRA733-1.html',\n",
    "    'https://foundationcapital.com/context-graphs/',\n",
    "]\n",
    "\n",
    "web_seed_contents = []\n",
    "for url in web_sources:\n",
    "    try:\n",
    "        web_seed_contents.append(ingest_module.ingest_web(url, method='url'))\n",
    "    except Exception as e:\n",
    "        print(f'Web ingestion failed for {url}: {e}')\n",
    "\n",
    "{'present_files': present_files, 'missing_files': missing_files, 'web_seed_docs': len(web_seed_contents)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f73b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.ingest as ingest_module\n",
    "from semantica.ingest import FileIngestor, WebIngestor, OntologyIngestor\n",
    "\n",
    "file_ingestor = FileIngestor()\n",
    "file_objects = file_ingestor.ingest_directory(DATA_DIR, recursive=False, read_content=False)\n",
    "\n",
    "# method wrappers via module namespace (no direct function import)\n",
    "file_objects_via_method = ingest_module.ingest_file(DATA_DIR, method='directory', recursive=False, read_content=False)\n",
    "\n",
    "web_ingestor = WebIngestor()\n",
    "web_contents = []\n",
    "for url in ['https://www.rand.org/pubs/research_reports/RRA733-1.html']:\n",
    "    try:\n",
    "        web_contents.append(web_ingestor.ingest_url(url))\n",
    "    except Exception as e:\n",
    "        print(f'Web ingestion failed for {url}: {e}')\n",
    "\n",
    "web_contents_via_method = []\n",
    "for url in ['https://www.rand.org/pubs/research_reports/RRA733-1.html']:\n",
    "    try:\n",
    "        web_contents_via_method.append(ingest_module.ingest_web(url, method='url'))\n",
    "    except Exception as e:\n",
    "        print(f'Web method ingestion failed for {url}: {e}')\n",
    "\n",
    "ontology_ingestor = OntologyIngestor()\n",
    "ontology_data = ontology_ingestor.ingest_directory(DATA_DIR, recursive=False)\n",
    "ontology_data_via_method = ingest_module.ingest_ontology(DATA_DIR, method='directory', recursive=False)\n",
    "\n",
    "{\n",
    "    'files': len(file_objects),\n",
    "    'files_via_method': len(file_objects_via_method) if isinstance(file_objects_via_method, list) else 1,\n",
    "    'web_docs': len(web_contents),\n",
    "    'web_docs_via_method': len(web_contents_via_method),\n",
    "    'ontologies': len(ontology_data),\n",
    "    'ontologies_via_method': len(ontology_data_via_method) if isinstance(ontology_data_via_method, list) else 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13308e8d",
   "metadata": {},
   "source": [
    "- Modules: `FileIngestor`, `WebIngestor`, `OntologyIngestor`\n",
    "- Loads local files.\n",
    "- Fetches web content.\n",
    "- Ingests ontology files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf5d0b",
   "metadata": {},
   "source": [
    "## Ontology Ingestion Detail (Schema + Instance Coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f146ca81",
   "metadata": {},
   "source": [
    "- Modules: `ingest_ontology`, `OntologyEvaluator`\n",
    "- Reads ontology files and basic schema info.\n",
    "- Runs competency-question coverage check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.ontology as ontology_module\n",
    "\n",
    "ontology_files = sorted([p for p in DATA_DIR.glob('*.ttl')])\n",
    "ontology_details = []\n",
    "for of in ontology_files:\n",
    "    try:\n",
    "        od = ontology_module.ingest_ontology(of, method='file')\n",
    "        if isinstance(od, list):\n",
    "            for item in od:\n",
    "                ontology_details.append({\n",
    "                    'file': of.name,\n",
    "                    'classes': len(item.data.get('classes', [])),\n",
    "                    'properties': len(item.data.get('properties', [])),\n",
    "                })\n",
    "        else:\n",
    "            ontology_details.append({\n",
    "                'file': of.name,\n",
    "                'classes': len(od.data.get('classes', [])),\n",
    "                'properties': len(od.data.get('properties', [])),\n",
    "            })\n",
    "    except Exception as e:\n",
    "        ontology_details.append({'file': of.name, 'error': str(e)})\n",
    "\n",
    "ontology_details[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77094c01",
   "metadata": {},
   "source": [
    "## Ontology Coverage Check for Capability Gap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e07eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ontology import OntologyEvaluator\n",
    "\n",
    "ontology_eval = OntologyEvaluator()\n",
    "\n",
    "# Evaluate first ingested ontology if available (schema adequacy for use-case questions)\n",
    "ontology_eval_result = None\n",
    "if ontology_data:\n",
    "    ontology_eval_result = ontology_eval.evaluate_ontology(\n",
    "        ontology_data[0].data,\n",
    "        competency_questions=[\n",
    "            'What capability gaps are revealed for a mission thread?',\n",
    "            'Which systems provide required capabilities?',\n",
    "            'What evidence and provenance support a gap decision?',\n",
    "            'Which precedents and exceptions affected a decision?'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "if ontology_eval_result:\n",
    "    {\n",
    "        'coverage_score': ontology_eval_result.coverage_score,\n",
    "        'completeness_score': ontology_eval_result.completeness_score,\n",
    "        'gaps': ontology_eval_result.gaps[:5],\n",
    "    }\n",
    "else:\n",
    "    {'coverage_score': None, 'completeness_score': None, 'gaps': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d973dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.parse as parse_module\n",
    "from semantica.parse import PDFParser\n",
    "\n",
    "pdf_parser = PDFParser()\n",
    "pdf_docs = []\n",
    "for pdf_path in sorted(DATA_DIR.glob('*.pdf')):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            if f.read(4) != b'%PDF':\n",
    "                print(f'Skipping non-PDF payload: {pdf_path.name}')\n",
    "                continue\n",
    "\n",
    "        parsed = parse_module.parse_pdf(pdf_path, method='default', pages=list(range(0, 12)))\n",
    "        if not isinstance(parsed, dict) or ('full_text' not in parsed and 'text' not in parsed):\n",
    "            parsed = pdf_parser.parse(pdf_path, pages=list(range(0, 12)))\n",
    "\n",
    "        text = parsed.get('full_text', parsed.get('text', ''))\n",
    "        if text:\n",
    "            pdf_docs.append({\n",
    "                'doc_id': pdf_path.stem,\n",
    "                'source': str(pdf_path),\n",
    "                'text': text[:50000],\n",
    "                'metadata': parsed.get('metadata', {}),\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f'PDF parse failed for {pdf_path.name}: {e}')\n",
    "\n",
    "len(pdf_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551ef61",
   "metadata": {},
   "source": [
    "## Multi-Format Parsing: DocumentParser + Optional DoclingParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f694157",
   "metadata": {},
   "source": [
    "- Modules: `PDFParser`, `DocumentParser`, optional `DoclingParser`\n",
    "- Parses PDF/documents.\n",
    "- Extracts text and metadata.\n",
    "- Uses Docling parser if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.parse import DocumentParser\n",
    "import semantica.parse as parse_module\n",
    "\n",
    "doc_parser = DocumentParser()\n",
    "doc_parser_preview = {}\n",
    "\n",
    "if pdf_docs:\n",
    "    sample_pdf = Path(pdf_docs[0]['source'])\n",
    "    try:\n",
    "        parsed_doc = parse_module.parse_document(sample_pdf, method='default')\n",
    "        if not isinstance(parsed_doc, dict):\n",
    "            parsed_doc = doc_parser.parse_document(sample_pdf)\n",
    "        doc_parser_preview = {\n",
    "            'source': sample_pdf.name,\n",
    "            'keys': list(parsed_doc.keys())[:10],\n",
    "            'text_chars': len(parsed_doc.get('full_text', parsed_doc.get('text', '')) or ''),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        doc_parser_preview = {'source': sample_pdf.name, 'error': str(e)}\n",
    "\n",
    "docling_preview = {'docling_available': bool(getattr(parse_module, 'DOCLING_AVAILABLE', False))}\n",
    "if getattr(parse_module, 'DOCLING_AVAILABLE', False) and pdf_docs:\n",
    "    try:\n",
    "        docling_parser = parse_module.DoclingParser(export_format='markdown')\n",
    "        dres = docling_parser.parse(Path(pdf_docs[0]['source']))\n",
    "        docling_preview['keys'] = list(dres.keys())[:10]\n",
    "        docling_preview['text_chars'] = len(dres.get('full_text', dres.get('text', '')) or '')\n",
    "    except Exception as e:\n",
    "        docling_preview['error'] = str(e)\n",
    "\n",
    "{'document_parser': doc_parser_preview, 'docling_parser': docling_preview}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "web_items = web_contents + web_contents_via_method + web_seed_contents\n",
    "\n",
    "corpus = (\n",
    "    [\n",
    "        {'doc_id': d['doc_id'], 'source': d['source'], 'text': d['text']}\n",
    "        for d in pdf_docs\n",
    "    ]\n",
    "    + [\n",
    "        {\n",
    "            'doc_id': f'web_{i}',\n",
    "            'source': getattr(w, 'url', f'web_source_{i}'),\n",
    "            'text': (getattr(w, 'content', str(w)) or '')[:30000],\n",
    "        }\n",
    "        for i, w in enumerate(web_items)\n",
    "    ]\n",
    "    + [\n",
    "        {\n",
    "            'doc_id': Path(ont.source_path).stem,\n",
    "            'source': ont.source_path,\n",
    "            'text': json.dumps(ont.data, ensure_ascii=True)[:40000],\n",
    "        }\n",
    "        for ont in ontology_data\n",
    "    ]\n",
    ")\n",
    "\n",
    "{'corpus_items': len(corpus), 'sample': [c['doc_id'] for c in corpus[:5]]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24299fa3",
   "metadata": {},
   "source": [
    "## Orchestration-Path Chunking (Decision-Time Context Capture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c06c62b",
   "metadata": {},
   "source": [
    "- Modules: `TextSplitter`, `PipelineBuilder`\n",
    "- Splits documents into chunks.\n",
    "- Defines pipeline steps for ingest, split, extract, graph, export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.split import TextSplitter\n",
    "\n",
    "splitter = TextSplitter(method='recursive', chunk_size=1800, chunk_overlap=250)\n",
    "\n",
    "texts = [doc.get('text', '') for doc in corpus]\n",
    "chunks_by_doc = splitter.split_batch(texts)\n",
    "\n",
    "chunked_docs = []\n",
    "for doc, chunks in zip(corpus, chunks_by_doc):\n",
    "    for idx, ch in enumerate(chunks or []):\n",
    "        chunked_docs.append({\n",
    "            'doc_id': f\"{doc['doc_id']}::chunk_{idx}\",\n",
    "            'source': doc['source'],\n",
    "            'text': ch.text if hasattr(ch, 'text') else str(ch),\n",
    "            'parent_doc_id': doc['doc_id'],\n",
    "        })\n",
    "\n",
    "extraction_corpus = chunked_docs if chunked_docs else corpus\n",
    "\n",
    "{'chunked_docs': len(chunked_docs), 'extraction_docs': len(extraction_corpus)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf87ac1",
   "metadata": {},
   "source": [
    "## Split Strategies (Semantic / Structural / Entity-Aware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a4248",
   "metadata": {},
   "source": [
    "- Modules: `SemanticChunker`, `StructuralChunker`, `TextSplitter` (`entity_aware`)\n",
    "- Runs multiple split strategies.\n",
    "- Compares chunk counts/output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413768a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.split import SemanticChunker, StructuralChunker\n",
    "\n",
    "split_strategy_preview = {}\n",
    "if corpus:\n",
    "    sample_text = corpus[0]['text'][:12000]\n",
    "    try:\n",
    "        semantic_chunker = SemanticChunker(chunk_size=1200, chunk_overlap=200)\n",
    "        sem_chunks = semantic_chunker.chunk(sample_text)\n",
    "        split_strategy_preview['semantic_chunks'] = len(sem_chunks)\n",
    "    except Exception as e:\n",
    "        split_strategy_preview['semantic_chunks_error'] = str(e)\n",
    "\n",
    "    try:\n",
    "        structural_chunker = StructuralChunker(chunk_size=1200, chunk_overlap=150)\n",
    "        st_chunks = structural_chunker.chunk(sample_text)\n",
    "        split_strategy_preview['structural_chunks'] = len(st_chunks)\n",
    "    except Exception as e:\n",
    "        split_strategy_preview['structural_chunks_error'] = str(e)\n",
    "\n",
    "    try:\n",
    "        ea_splitter = TextSplitter(method=['entity_aware', 'recursive'], chunk_size=1200, chunk_overlap=150)\n",
    "        ea_chunks = ea_splitter.split(sample_text)\n",
    "        split_strategy_preview['entity_aware_chunks'] = len(ea_chunks)\n",
    "    except Exception as e:\n",
    "        split_strategy_preview['entity_aware_chunks_error'] = str(e)\n",
    "\n",
    "split_strategy_preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.pipeline import PipelineBuilder\n",
    "\n",
    "# Represent execution-path orchestration explicitly (the layer where decision traces should be captured)\n",
    "pipeline = (\n",
    "    PipelineBuilder()\n",
    "    .add_step('ingest_sources', 'ingest', sources=len(corpus))\n",
    "    .add_step('chunk_context', 'split', method='recursive')\n",
    "    .add_step('semantic_extract', 'extract', entity_relation_event_triplet=True)\n",
    "    .add_step('build_context_graph', 'context_graph')\n",
    "    .add_step('policy_and_trace', 'decision_trace_capture')\n",
    "    .add_step('export_and_observe', 'export_observability')\n",
    "    .connect_steps('ingest_sources', 'chunk_context')\n",
    "    .connect_steps('chunk_context', 'semantic_extract')\n",
    "    .connect_steps('semantic_extract', 'build_context_graph')\n",
    "    .connect_steps('build_context_graph', 'policy_and_trace')\n",
    "    .connect_steps('policy_and_trace', 'export_and_observe')\n",
    "    .build(name='capability_gap_orchestration_path')\n",
    ")\n",
    "\n",
    "{'pipeline': pipeline.name, 'steps': [s.name for s in pipeline.steps]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b7811",
   "metadata": {},
   "source": [
    "## Normalization Layer (Text, Entity, Date, Number, Language, Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd9ffd",
   "metadata": {},
   "source": [
    "- Modules: `TextNormalizer`, `EntityNormalizer`, `DateNormalizer`, `NumberNormalizer`, `LanguageDetector`, `EncodingHandler`, `TextCleaner`\n",
    "- Cleans and normalizes text.\n",
    "- Normalizes entities, date/time, and numeric values.\n",
    "- Detects language and handles encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.normalize as normalize_module\n",
    "from semantica.normalize import TextNormalizer, EntityNormalizer, DateNormalizer, NumberNormalizer\n",
    "from semantica.normalize import LanguageDetector, EncodingHandler, TextCleaner\n",
    "\n",
    "text_normalizer = TextNormalizer()\n",
    "entity_normalizer = EntityNormalizer()\n",
    "date_normalizer = DateNormalizer()\n",
    "number_normalizer = NumberNormalizer()\n",
    "language_detector = LanguageDetector(default_language='en')\n",
    "encoding_handler = EncodingHandler()\n",
    "text_cleaner = TextCleaner()\n",
    "\n",
    "normalized_extraction_corpus = []\n",
    "for item in extraction_corpus:\n",
    "    txt = item.get('text', '')\n",
    "\n",
    "    cleaned = normalize_module.clean_text(txt, method='default') if txt else ''\n",
    "    normalized_text = normalize_module.normalize_text(cleaned, method='default') if cleaned else ''\n",
    "\n",
    "    lang = normalize_module.detect_language(normalized_text, method='default') if normalized_text else 'en'\n",
    "    _ = normalize_module.handle_encoding(normalized_text, method='default') if normalized_text else normalized_text\n",
    "\n",
    "    normalized_extraction_corpus.append({\n",
    "        **item,\n",
    "        'text': normalized_text,\n",
    "        'language': lang,\n",
    "    })\n",
    "\n",
    "extraction_corpus = normalized_extraction_corpus\n",
    "\n",
    "demo_date = date_normalizer.normalize_date('12 Apr 2028 05:15 UTC')\n",
    "demo_num = number_normalizer.normalize_number('42.0%')\n",
    "demo_entity = entity_normalizer.normalize_entity('ground radar layer', entity_type='System')\n",
    "\n",
    "{'normalized_docs': len(extraction_corpus), 'demo_date': str(demo_date), 'demo_number': demo_num, 'demo_entity': demo_entity}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.semantic_extract import NamedEntityRecognizer, RelationExtractor, EventDetector\n",
    "from semantica.semantic_extract import CoreferenceResolver, TripletExtractor, SemanticAnalyzer\n",
    "from semantica.semantic_extract import SemanticNetworkExtractor, ExtractionValidator\n",
    "\n",
    "ner = NamedEntityRecognizer(method='pattern', confidence_threshold=0.2)\n",
    "relation_extractor = RelationExtractor(method='pattern', confidence_threshold=0.2)\n",
    "event_detector = EventDetector()\n",
    "coref_resolver = CoreferenceResolver()\n",
    "triplet_extractor = TripletExtractor(method='pattern', include_provenance=True)\n",
    "semantic_analyzer = SemanticAnalyzer()\n",
    "semantic_network_extractor = SemanticNetworkExtractor()\n",
    "validator = ExtractionValidator()\n",
    "\n",
    "texts = [item.get('text', '') for item in extraction_corpus if item.get('text')]\n",
    "resolved_texts = [coref_resolver.resolve(t) for t in texts]\n",
    "\n",
    "entities_batch = ner.process_batch(resolved_texts)\n",
    "triplets_batch = triplet_extractor.process_batch(resolved_texts)\n",
    "relations_batch = [relation_extractor.extract_relations(t, entities=e) for t, e in zip(resolved_texts, entities_batch)]\n",
    "events_batch = [event_detector.detect_events(t) for t in resolved_texts]\n",
    "\n",
    "all_entities = [e for batch in entities_batch for e in batch]\n",
    "all_relationships = [r for batch in relations_batch for r in batch]\n",
    "all_events = [ev for batch in events_batch for ev in batch]\n",
    "all_triplets = [tr for batch in triplets_batch for tr in batch]\n",
    "\n",
    "_ = validator.validate_entities(all_entities)\n",
    "_ = validator.validate_relations(all_relationships)\n",
    "\n",
    "semantic_networks = [\n",
    "    {\n",
    "        'doc_id': extraction_corpus[i].get('doc_id', f'doc_{i}'),\n",
    "        'analysis': semantic_analyzer.analyze(resolved_texts[i]),\n",
    "        'network': semantic_network_extractor.extract(resolved_texts[i], entities=entities_batch[i], relations=relations_batch[i]),\n",
    "    }\n",
    "    for i in range(min(len(resolved_texts), len(extraction_corpus)))\n",
    "]\n",
    "\n",
    "{\n",
    "    'entities': len(all_entities),\n",
    "    'relationships': len(all_relationships),\n",
    "    'events': len(all_events),\n",
    "    'triplets': len(all_triplets),\n",
    "    'semantic_networks': len(semantic_networks),\n",
    "    'documents_processed': len(resolved_texts),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46fdd9",
   "metadata": {},
   "source": [
    "## Data Quality Controls: Deduplication + Conflict Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f38faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import EntityResolver\n",
    "import semantica.conflicts as conflicts_module\n",
    "\n",
    "entity_dicts = []\n",
    "for e in all_entities:\n",
    "    entity_dicts.append({\n",
    "        'id': str(getattr(e, 'id', getattr(e, 'text', 'unknown'))),\n",
    "        'name': str(getattr(e, 'text', getattr(e, 'id', 'unknown'))),\n",
    "        'type': str(getattr(e, 'label', getattr(e, 'type', 'entity'))),\n",
    "        'metadata': getattr(e, 'metadata', {}) or {}\n",
    "    })\n",
    "\n",
    "entity_resolver = EntityResolver(strategy='fuzzy')\n",
    "resolved_entities = entity_resolver.resolve_entities(entity_dicts[:200]) if entity_dicts else []\n",
    "\n",
    "conflict_rows = [\n",
    "    {'id': 'System_GroundRadarLayer', 'coveragePercent': '42', 'type': 'system'},\n",
    "    {'id': 'System_GroundRadarLayer', 'coveragePercent': '58', 'type': 'system'},\n",
    "]\n",
    "conflicts = conflicts_module.detect_conflicts(conflict_rows, method='value', property_name='coveragePercent')\n",
    "resolved_conflicts = conflicts_module.resolve_conflicts(conflicts, method=conflicts_module.voting) if conflicts else []\n",
    "\n",
    "{\n",
    "    'entities_before_resolution': len(entity_dicts[:200]),\n",
    "    'entities_after_resolution': len(resolved_entities),\n",
    "    'conflicts_detected': len(conflicts),\n",
    "    'conflicts_resolved': len(resolved_conflicts),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import GraphBuilder, GraphAnalyzer\n",
    "\n",
    "graph_builder = GraphBuilder(merge_entities=True, resolve_conflicts=True)\n",
    "kg = graph_builder.build([{'entities': all_entities, 'relationships': all_relationships}], extract=False)\n",
    "\n",
    "graph_analyzer = GraphAnalyzer()\n",
    "kg_analysis = graph_analyzer.analyze_graph(kg)\n",
    "\n",
    "{\n",
    "    'kg_entities': len(kg.get('entities', [])),\n",
    "    'kg_relationships': len(kg.get('relationships', [])),\n",
    "    'has_analysis': bool(kg_analysis),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5950b36",
   "metadata": {},
   "source": [
    "## KG Analytics (Centrality, Communities, Connectivity, Similarity, Link Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11886625",
   "metadata": {},
   "source": [
    "- Modules: `CentralityCalculator`, `CommunityDetector`, `ConnectivityAnalyzer`, `SimilarityCalculator`, `LinkPredictor`, `NodeEmbedder`\n",
    "- Runs graph metrics and analytics.\n",
    "- Calculates centrality, communities, connectivity, similarity, and link predictions.\n",
    "- Checks node embedding availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37620ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import CentralityCalculator, CommunityDetector, ConnectivityAnalyzer\n",
    "from semantica.kg import SimilarityCalculator, LinkPredictor\n",
    "\n",
    "extended_kg_analytics = {}\n",
    "\n",
    "try:\n",
    "    centrality_calc = CentralityCalculator()\n",
    "    cent = centrality_calc.calculate_all_centrality(kg)\n",
    "    extended_kg_analytics['centrality_keys'] = list(cent.keys())[:10]\n",
    "except Exception as e:\n",
    "    extended_kg_analytics['centrality_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    community_detector = CommunityDetector()\n",
    "    comm = community_detector.detect_communities(kg, algorithm='louvain')\n",
    "    extended_kg_analytics['community_count'] = comm.get('num_communities', None) if isinstance(comm, dict) else None\n",
    "except Exception as e:\n",
    "    extended_kg_analytics['community_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    connectivity_analyzer = ConnectivityAnalyzer()\n",
    "    conn = connectivity_analyzer.analyze_connectivity(kg)\n",
    "    extended_kg_analytics['connectivity_keys'] = list(conn.keys())[:10] if isinstance(conn, dict) else []\n",
    "except Exception as e:\n",
    "    extended_kg_analytics['connectivity_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    sim_calc = SimilarityCalculator(method='cosine')\n",
    "    extended_kg_analytics['sample_cosine_similarity'] = sim_calc.cosine_similarity([1.0, 0.0, 1.0], [0.8, 0.2, 0.9])\n",
    "except Exception as e:\n",
    "    extended_kg_analytics['similarity_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    link_predictor = LinkPredictor()\n",
    "    lp = link_predictor.predict_links(kg, top_k=5)\n",
    "    extended_kg_analytics['predicted_links'] = len(lp) if hasattr(lp, '__len__') else None\n",
    "except Exception as e:\n",
    "    extended_kg_analytics['link_prediction_error'] = str(e)\n",
    "\n",
    "extended_kg_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.kg import NodeEmbedder\n",
    "\n",
    "node_embedding_status = {}\n",
    "try:\n",
    "    embedder = NodeEmbedder(method='node2vec', embedding_dimension=32, walk_length=20, num_walks=5)\n",
    "    node_embedding_status['node2vec_ready'] = True\n",
    "except Exception as e:\n",
    "    node_embedding_status['node2vec_ready'] = False\n",
    "    node_embedding_status['reason'] = str(e)\n",
    "\n",
    "node_embedding_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fde48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.context import ContextGraph\n",
    "\n",
    "context_graph = ContextGraph(advanced_analytics=True, centrality_analysis=True, community_detection=True)\n",
    "\n",
    "seed_nodes = [\n",
    "    {'id': 'Scenario_FutureA2AD_2028', 'type': 'scenario', 'properties': {'content': 'Future A2/AD escalation scenario'}},\n",
    "    {'id': 'MissionThread_ForceProtection', 'type': 'mission_thread', 'properties': {'content': 'Protect forward operating assets under drone saturation'}},\n",
    "    {'id': 'Event_LowAltitudeSwarmIncursions', 'type': 'event', 'properties': {'content': 'Repeated low-altitude swarm incursions'}},\n",
    "    {'id': 'System_GroundRadarLayer', 'type': 'system', 'properties': {'content': 'Ground radar surveillance layer'}},\n",
    "    {'id': 'Capability_LowAltitudeDetection', 'type': 'capability', 'properties': {'content': 'Low altitude detection capability'}},\n",
    "    {'id': 'Outcome_MissionRiskIncrease', 'type': 'outcome', 'properties': {'content': 'Rising mission risk and delayed response'}},\n",
    "    {'id': 'Gap_LowAltitudeDetectionCoverage', 'type': 'capability_gap', 'properties': {'content': 'Insufficient low-altitude detection coverage'}},\n",
    "]\n",
    "\n",
    "seed_edges = [\n",
    "    {'source_id': 'Scenario_FutureA2AD_2028', 'target_id': 'MissionThread_ForceProtection', 'type': 'has_mission_thread'},\n",
    "    {'source_id': 'MissionThread_ForceProtection', 'target_id': 'Event_LowAltitudeSwarmIncursions', 'type': 'includes_event'},\n",
    "    {'source_id': 'Event_LowAltitudeSwarmIncursions', 'target_id': 'System_GroundRadarLayer', 'type': 'stresses_system'},\n",
    "    {'source_id': 'System_GroundRadarLayer', 'target_id': 'Capability_LowAltitudeDetection', 'type': 'provides_capability'},\n",
    "    {'source_id': 'Capability_LowAltitudeDetection', 'target_id': 'Outcome_MissionRiskIncrease', 'type': 'affects_outcome'},\n",
    "    {'source_id': 'MissionThread_ForceProtection', 'target_id': 'Gap_LowAltitudeDetectionCoverage', 'type': 'reveals_gap'},\n",
    "]\n",
    "\n",
    "context_graph.add_nodes(seed_nodes)\n",
    "context_graph.add_edges(seed_edges)\n",
    "\n",
    "entity_nodes = [\n",
    "    {\n",
    "        'id': str(getattr(ent, 'id', getattr(ent, 'text', f'Entity_{idx}'))),\n",
    "        'type': str(getattr(ent, 'label', getattr(ent, 'type', 'entity'))),\n",
    "        'properties': {'content': str(getattr(ent, 'text', getattr(ent, 'id', f'Entity_{idx}')))},\n",
    "    }\n",
    "    for idx, ent in enumerate(all_entities[:60])\n",
    "]\n",
    "context_graph.add_nodes(entity_nodes)\n",
    "\n",
    "scenario_edges = [\n",
    "    {'source_id': 'Scenario_FutureA2AD_2028', 'target_id': n['id'], 'type': 'contextualizes'}\n",
    "    for n in entity_nodes\n",
    "]\n",
    "context_graph.add_edges(scenario_edges)\n",
    "\n",
    "relation_edges = [\n",
    "    {\n",
    "        'source_id': str(getattr(getattr(rel, 'subject', None), 'id', None) or getattr(rel, 'source', 'unknown_source')),\n",
    "        'target_id': str(getattr(getattr(rel, 'object', None), 'id', None) or getattr(rel, 'target', 'unknown_target')),\n",
    "        'type': str(getattr(rel, 'predicate', None) or getattr(rel, 'type', 'related_to')),\n",
    "    }\n",
    "    for rel in all_relationships[:120]\n",
    "]\n",
    "context_graph.add_edges(relation_edges)\n",
    "\n",
    "context_graph.stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.vector_store import VectorStore\n",
    "from semantica.context import AgentContext\n",
    "\n",
    "vector_store = VectorStore(backend='inmemory', dimension=384)\n",
    "agent_context = AgentContext(\n",
    "    vector_store=vector_store,\n",
    "    knowledge_graph=context_graph,\n",
    "    decision_tracking=True,\n",
    "    advanced_analytics=True,\n",
    "    kg_algorithms=True,\n",
    "    vector_store_features=True,\n",
    "    graph_expansion=True,\n",
    "    max_expansion_hops=3,\n",
    ")\n",
    "\n",
    "stored = agent_context.store(\n",
    "    [{'content': c['text'][:2500], 'metadata': {'source': c['source'], 'doc_id': c['doc_id']}} for c in corpus],\n",
    "    extract_entities=False,\n",
    "    extract_relationships=False\n",
    ")\n",
    "\n",
    "d1 = agent_context.record_decision(\n",
    "    category='capability_gap_assessment',\n",
    "    scenario='Future A2/AD mission thread with low-altitude swarm pressure',\n",
    "    reasoning='Mission requires persistent low-altitude detection, but current radar layer indicates limited valley and urban coverage.',\n",
    "    outcome='gap_identified_low_altitude_detection',\n",
    "    confidence=0.93,\n",
    "    entities=['MissionThread_ForceProtection', 'Capability_LowAltitudeDetection', 'Gap_LowAltitudeDetectionCoverage'],\n",
    ")\n",
    "\n",
    "d2 = agent_context.record_decision(\n",
    "    category='capability_gap_mitigation',\n",
    "    scenario='Counter low-altitude swarm incursions',\n",
    "    reasoning='Need layered sensing integration and revised mission doctrine to close detection delay.',\n",
    "    outcome='recommend_multilayer_sensor_fusion',\n",
    "    confidence=0.88,\n",
    "    entities=['System_GroundRadarLayer', 'Gap_LowAltitudeDetectionCoverage'],\n",
    ")\n",
    "\n",
    "retrieved = agent_context.retrieve(\n",
    "    query='Which capability gaps most increase mission risk in this scenario?',\n",
    "    max_results=8,\n",
    "    expand_graph=True,\n",
    "    include_entities=True,\n",
    ")\n",
    "\n",
    "{'stored': stored.get('stored_count', 0), 'decisions': [d1, d2], 'retrieved': len(retrieved)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111033e9",
   "metadata": {},
   "source": [
    "## Decision Traces: Policies, Exceptions, Approval Chains, Precedents, Cross-System Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b27f9da",
   "metadata": {},
   "source": [
    "- Modules: `AgentContext`, `ContextGraph`, `PolicyEngine`\n",
    "- Models: `Decision`, `Policy`, `PolicyException`, `ApprovalChain`, `Precedent`\n",
    "- Records decisions and policy checks.\n",
    "- Adds exceptions, approvals, precedents, and cross-system context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from semantica.context.decision_models import Decision, Policy, PolicyException, ApprovalChain, Precedent\n",
    "from semantica.context.policy_engine import PolicyEngine\n",
    "\n",
    "# Policy model aligned to 'policy v3.2 + exception route' pattern from the article\n",
    "policy_engine = PolicyEngine(context_graph)\n",
    "renewal_policy = Policy(\n",
    "    policy_id='POL-CAPGAP-3.2',\n",
    "    name='Capability Gap Escalation Policy',\n",
    "    description='Escalate and require approval when mission-critical capability coverage is below threshold.',\n",
    "    rules={\n",
    "        'min_confidence': 0.8,\n",
    "        'required_categories': ['capability_gap_assessment', 'capability_gap_mitigation'],\n",
    "        'allowed_outcomes': ['gap_identified_low_altitude_detection', 'recommend_multilayer_sensor_fusion', 'escalate_for_exception']\n",
    "    },\n",
    "    category='capability_gap_assessment',\n",
    "    version='3.2',\n",
    "    created_at=datetime.now(),\n",
    "    updated_at=datetime.now(),\n",
    "    metadata={'entities': ['MissionThread_ForceProtection', 'System_GroundRadarLayer']}\n",
    ")\n",
    "policy_engine.add_policy(renewal_policy)\n",
    "\n",
    "# Construct explicit trace artifacts (exception, approval, precedent link)\n",
    "trace_decision = Decision(\n",
    "    decision_id='',\n",
    "    category='capability_gap_assessment',\n",
    "    scenario='Coverage threshold breach during swarm-pressure mission thread',\n",
    "    reasoning='Below-threshold low-altitude detection coverage with repeated threat ingress; escalation required.',\n",
    "    outcome='escalate_for_exception',\n",
    "    confidence=0.89,\n",
    "    timestamp=datetime.now(),\n",
    "    decision_maker='joint_ops_agent',\n",
    "    metadata={'policy_version': '3.2'}\n",
    ")\n",
    "\n",
    "policy_exception = PolicyException(\n",
    "    exception_id='',\n",
    "    decision_id=trace_decision.decision_id,\n",
    "    policy_id='POL-CAPGAP-3.2',\n",
    "    reason='Emergency force-protection override due to active swarm threat',\n",
    "    approver='VP_Operations',\n",
    "    approval_timestamp=datetime.now(),\n",
    "    justification='Mission-critical risk outweighs standard route latency',\n",
    "    metadata={'channel': 'slack_dm'}\n",
    ")\n",
    "\n",
    "approval_chain = ApprovalChain(\n",
    "    approval_id='',\n",
    "    decision_id=trace_decision.decision_id,\n",
    "    approver='Finance_Controller',\n",
    "    approval_method='zoom_call',\n",
    "    approval_context='Approved exceptional spend for layered sensing package',\n",
    "    timestamp=datetime.now(),\n",
    "    metadata={'step': 'final_finance_gate'}\n",
    ")\n",
    "\n",
    "precedent_link = Precedent(\n",
    "    precedent_id='',\n",
    "    source_decision_id=d1,\n",
    "    similarity_score=0.92,\n",
    "    relationship_type='similar_scenario',\n",
    "    metadata={'note': 'Prior low-altitude detection gap precedent'}\n",
    ")\n",
    "\n",
    "# Persist trace artifacts into ContextGraph as first-class decision-trace nodes\n",
    "trace_decision_id = context_graph.record_decision(\n",
    "    category=trace_decision.category,\n",
    "    scenario=trace_decision.scenario,\n",
    "    reasoning=trace_decision.reasoning,\n",
    "    outcome=trace_decision.outcome,\n",
    "    confidence=trace_decision.confidence,\n",
    "    entities=['MissionThread_ForceProtection', 'Gap_LowAltitudeDetectionCoverage'],\n",
    "    decision_maker=trace_decision.decision_maker,\n",
    "    metadata={'policy_version': '3.2', 'cross_system_context': {'crm': 'critical_account', 'zendesk': 'open_escalation', 'pagerduty': 'sev1_incidents'}}\n",
    ")\n",
    "\n",
    "context_graph.add_node(policy_exception.exception_id, 'policy_exception', policy_exception.reason)\n",
    "context_graph.add_edge(trace_decision_id, policy_exception.exception_id, 'has_exception')\n",
    "context_graph.add_node(approval_chain.approval_id, 'approval', approval_chain.approval_context)\n",
    "context_graph.add_edge(trace_decision_id, approval_chain.approval_id, 'approved_by_chain')\n",
    "context_graph.add_node(precedent_link.precedent_id, 'precedent', 'precedent linkage')\n",
    "context_graph.add_edge(trace_decision_id, precedent_link.precedent_id, 'uses_precedent')\n",
    "context_graph.add_edge(precedent_link.precedent_id, d1, 'points_to_decision')\n",
    "\n",
    "# Compliance check against policy v3.2\n",
    "compliant = policy_engine.check_compliance(trace_decision, 'POL-CAPGAP-3.2')\n",
    "\n",
    "{'trace_decision_id': trace_decision_id, 'policy_compliant': compliant, 'policy_id': renewal_policy.policy_id, 'policy_version': renewal_policy.version}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb512f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search precedent and causal impact to convert one-off exceptions into reusable governance\n",
    "precedents = agent_context.find_precedents(\n",
    "    scenario='Low-altitude detection shortfall under swarm pressure',\n",
    "    category='capability_gap_assessment',\n",
    "    limit=5,\n",
    "    use_hybrid_search=True\n",
    ")\n",
    "\n",
    "impact = context_graph.analyze_decision_impact(trace_decision_id)\n",
    "insights = context_graph.get_decision_summary()\n",
    "\n",
    "{\n",
    "    'precedent_hits': len(precedents),\n",
    "    'impact_total_influenced': impact.get('total_influenced', 0),\n",
    "    'decision_total': insights.get('total_decisions', 0),\n",
    "    'categories': insights.get('categories', {})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-system synthesis snapshot using AgentContext API\n",
    "try:\n",
    "    cross_system_snapshot = agent_context.capture_cross_system_inputs(\n",
    "        systems=['crm', 'ticketing', 'incident_management', 'asset_inventory'],\n",
    "        entity_id='MissionThread_ForceProtection'\n",
    "    )\n",
    "except Exception as e:\n",
    "    cross_system_snapshot = {'error': str(e)}\n",
    "\n",
    "cross_system_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda33119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.context as context_module\n",
    "\n",
    "hop_1 = context_graph.get_neighbors('Scenario_FutureA2AD_2028', hops=1)\n",
    "hop_2 = context_graph.get_neighbors('Scenario_FutureA2AD_2028', hops=2)\n",
    "hop_3 = context_graph.get_neighbors('Scenario_FutureA2AD_2028', hops=3)\n",
    "\n",
    "reasoning_paths = []\n",
    "try:\n",
    "    mh = context_module.multi_hop_query(\n",
    "        context_graph,\n",
    "        start_entity='Scenario_FutureA2AD_2028',\n",
    "        query='Trace mission-thread to capability-gap path',\n",
    "        max_hops=3,\n",
    "    )\n",
    "    reasoning_paths = mh.get('decisions', []) if isinstance(mh, dict) else []\n",
    "except Exception as e:\n",
    "    reasoning_paths = [{'error': str(e)}]\n",
    "\n",
    "{'hop1': len(hop_1), 'hop2': len(hop_2), 'hop3': len(hop_3), 'multi_hop_results': len(reasoning_paths)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.reasoning import Reasoner, ExplanationGenerator\n",
    "\n",
    "reasoner = Reasoner()\n",
    "reasoner.add_rule('IF MissionRequires(?m, LowAltitudeDetection) AND CoverageStatus(?m, Insufficient) THEN CapabilityGap(?m, LowAltitudeDetectionGap)')\n",
    "reasoner.add_rule('IF CapabilityGap(?m, LowAltitudeDetectionGap) AND ThreatLevel(?m, High) THEN OutcomeRisk(?m, Elevated)')\n",
    "\n",
    "reasoner.add_fact('MissionRequires(MissionThread_ForceProtection, LowAltitudeDetection)')\n",
    "reasoner.add_fact('CoverageStatus(MissionThread_ForceProtection, Insufficient)')\n",
    "reasoner.add_fact('ThreatLevel(MissionThread_ForceProtection, High)')\n",
    "\n",
    "inferred = reasoner.forward_chain()\n",
    "\n",
    "explanation_text = ''\n",
    "if inferred:\n",
    "    explanation_generator = ExplanationGenerator()\n",
    "    explanation = explanation_generator.generate_explanation(inferred[-1])\n",
    "    explanation_text = explanation.natural_language\n",
    "\n",
    "{'inferred': [f.conclusion for f in inferred], 'explanation': explanation_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e679b",
   "metadata": {},
   "source": [
    "## Versioned Decision Governance (Policy / Ontology Change Tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e459c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.change_management import VersionManager\n",
    "\n",
    "version_manager = VersionManager(base_uri='https://example.org/mcg')\n",
    "\n",
    "v1 = version_manager.create_version(\n",
    "    '3.1',\n",
    "    ontology={'uri': 'https://example.org/mcg', 'classes': [], 'properties': []},\n",
    "    changes=['Initial capability-gap decision policy baseline'],\n",
    "    metadata={'structure': {'classes': ['Scenario', 'MissionThread', 'CapabilityGap'], 'properties': ['revealsGap']}}\n",
    ")\n",
    "\n",
    "v2 = version_manager.create_version(\n",
    "    '3.2',\n",
    "    ontology={'uri': 'https://example.org/mcg', 'classes': [], 'properties': []},\n",
    "    changes=['Added explicit policy exception and approval-chain trace constructs'],\n",
    "    metadata={'structure': {'classes': ['Scenario', 'MissionThread', 'CapabilityGap', 'PolicyException', 'ApprovalChain'], 'properties': ['revealsGap', 'has_exception', 'approved_by_chain']}}\n",
    ")\n",
    "\n",
    "version_diff = version_manager.compare_versions('3.1', '3.2')\n",
    "{'latest_version': version_manager.latest_version, 'classes_added': version_diff.get('classes_added', []), 'properties_added': version_diff.get('properties_added', [])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.provenance import ProvenanceManager\n",
    "\n",
    "provenance_db = OUTPUT_DIR / 'capability_gap_provenance.db'\n",
    "prov = ProvenanceManager(storage_path=str(provenance_db))\n",
    "\n",
    "for c in corpus:\n",
    "    prov.track_entity(entity_id=f\"source::{c['doc_id']}\", source=c['source'], metadata={'document_type': 'corpus_source'})\n",
    "\n",
    "for ent in all_entities[:80]:\n",
    "    ent_id = str(getattr(ent, 'id', getattr(ent, 'text', 'unknown_entity')))\n",
    "    src_doc = (getattr(ent, 'metadata', {}) or {}).get('source_doc', 'unknown_source')\n",
    "    prov.track_entity(\n",
    "        entity_id=f\"entity::{ent_id}\",\n",
    "        source=src_doc,\n",
    "        metadata={'entity_text': str(getattr(ent, 'text', ent_id)), 'entity_type': str(getattr(ent, 'label', 'entity'))}\n",
    "    )\n",
    "\n",
    "for i, rel in enumerate(all_relationships[:120]):\n",
    "    src_doc = (getattr(rel, 'metadata', {}) or {}).get('source_doc', 'unknown_source')\n",
    "    prov.track_relationship(relationship_id=f'rel::{i}', source=src_doc, metadata={'relation_type': str(getattr(rel, 'predicate', getattr(rel, 'type', 'related_to')))})\n",
    "\n",
    "{'stats': prov.get_statistics(), 'lineage_sample': prov.get_lineage('entity::MissionThread_ForceProtection')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff980e1",
   "metadata": {},
   "source": [
    "## Observability-Style Monitoring for Agent Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operational monitoring proxies using Semantica-native analytics outputs\n",
    "context_insights = agent_context.get_context_insights()\n",
    "\n",
    "decision_quality_monitor = {\n",
    "    'decision_count': context_insights.get('decision_tracking', {}).get('total_decisions', 0),\n",
    "    'graph_nodes': context_insights.get('knowledge_graph', {}).get('node_count', 0),\n",
    "    'graph_edges': context_insights.get('knowledge_graph', {}).get('edge_count', 0),\n",
    "    'provenance_entries': prov.get_statistics().get('total_entries', 0),\n",
    "}\n",
    "\n",
    "decision_quality_monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.export as export_module\n",
    "\n",
    "kg_json_path = OUTPUT_DIR / 'capability_gap_kg.json'\n",
    "context_json_path = OUTPUT_DIR / 'capability_gap_context_graph.json'\n",
    "context_graphml_path = OUTPUT_DIR / 'capability_gap_context_graph.graphml'\n",
    "kg_rdf_path = OUTPUT_DIR / 'capability_gap_kg.ttl'\n",
    "kg_csv_base = OUTPUT_DIR / 'capability_gap_kg'\n",
    "\n",
    "export_module.export_json(kg, kg_json_path, format='json')\n",
    "export_module.export_json(context_graph.to_dict(), context_json_path, format='json')\n",
    "export_module.export_graph(context_graph.to_dict(), context_graphml_path, format='graphml')\n",
    "export_module.export_rdf(kg, kg_rdf_path, format='turtle')\n",
    "export_module.export_csv({'entities': kg.get('entities', []), 'relationships': kg.get('relationships', [])}, kg_csv_base)\n",
    "\n",
    "[str(kg_json_path), str(context_json_path), str(context_graphml_path), str(kg_rdf_path)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22543f1d",
   "metadata": {},
   "source": [
    "## Export Layer (YAML, LPG, Report Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6da38",
   "metadata": {},
   "source": [
    "- Exports: `export_json`, `export_graph`, `export_rdf`, `export_csv`, `export_yaml`, `export_lpg`, `ReportGenerator`\n",
    "- Writes graph and analysis artifacts to multiple formats.\n",
    "- Generates a report file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3746b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantica.export as export_module\n",
    "\n",
    "extra_exports = {}\n",
    "\n",
    "try:\n",
    "    yaml_path = OUTPUT_DIR / 'capability_gap_context_graph.yaml'\n",
    "    export_module.export_yaml(context_graph.to_dict(), yaml_path)\n",
    "    extra_exports['yaml'] = str(yaml_path)\n",
    "except Exception as e:\n",
    "    extra_exports['yaml_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    lpg_path = OUTPUT_DIR / 'capability_gap_kg.cypher'\n",
    "    export_module.export_lpg(kg, lpg_path, method='cypher')\n",
    "    extra_exports['lpg'] = str(lpg_path)\n",
    "except Exception as e:\n",
    "    extra_exports['lpg_error'] = str(e)\n",
    "\n",
    "try:\n",
    "    report_data = {\n",
    "        'title': 'Military Capability Gap Analysis - End-to-End Report',\n",
    "        'summary': {\n",
    "            'corpus_items': len(corpus),\n",
    "            'extraction_items': len(extraction_corpus),\n",
    "            'entities': len(all_entities),\n",
    "            'relationships': len(all_relationships),\n",
    "            'decisions': context_graph.get_decision_summary().get('total_decisions', 0),\n",
    "        },\n",
    "        'metrics': {\n",
    "            'kg_entities': len(kg.get('entities', [])),\n",
    "            'kg_relationships': len(kg.get('relationships', [])),\n",
    "            'context_nodes': context_graph.stats().get('node_count', 0),\n",
    "            'context_edges': context_graph.stats().get('edge_count', 0),\n",
    "        },\n",
    "        'analysis': {'kg_analysis': kg_analysis}\n",
    "    }\n",
    "    report_path = OUTPUT_DIR / 'capability_gap_analysis_report.md'\n",
    "    generator = export_module.ReportGenerator(format='markdown', include_charts=False)\n",
    "    generator.generate_report(report_data, report_path, format='markdown')\n",
    "    extra_exports['report'] = str(report_path)\n",
    "except Exception as e:\n",
    "    extra_exports['report_error'] = str(e)\n",
    "\n",
    "extra_exports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "\n",
    "viz = KGVisualizer(layout='force', color_scheme='default')\n",
    "kg_html_path = OUTPUT_DIR / 'capability_gap_kg_network.html'\n",
    "\n",
    "try:\n",
    "    viz.visualize_network(kg, output='html', file_path=kg_html_path)\n",
    "    viz_result = str(kg_html_path)\n",
    "except Exception as e:\n",
    "    viz_result = f'Visualization skipped: {e}'\n",
    "\n",
    "viz_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'corpus_items': len(corpus),\n",
    "    'entities_extracted': len(all_entities),\n",
    "    'relationships_extracted': len(all_relationships),\n",
    "    'events_detected': len(all_events),\n",
    "    'triplets_extracted': len(all_triplets),\n",
    "    'kg_entities': len(kg.get('entities', [])),\n",
    "    'kg_relationships': len(kg.get('relationships', [])),\n",
    "    'context_graph_stats': context_graph.stats(),\n",
    "    'reasoning_inferred_rules': [f.conclusion for f in inferred],\n",
    "    'output_dir': str(OUTPUT_DIR),\n",
    "    'extended_kg_analytics': extended_kg_analytics if 'extended_kg_analytics' in globals() else {},\n",
    "    'extra_exports': extra_exports if 'extra_exports' in globals() else {},\n",
    "    'ontology_details': ontology_details if 'ontology_details' in globals() else [],\n",
    "}\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea25353",
   "metadata": {},
   "source": [
    "- Builds final summary dictionary.\n",
    "- Shows counts and output paths from all pipeline stages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
