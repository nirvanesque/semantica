{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a **production-grade GraphRAG pipeline** using the `Semantica` framework. We go beyond simple tutorials to show how to build, analyze, and query a rich knowledge base using advanced graph algorithms and hybrid retrieval strategies.\n",
    "\n",
    "### ðŸŽ¯ What You Will Learn\n",
    "\n",
    "1.  **Orchestration**: Using `Semantica` core to manage the full data lifecycle.\n",
    "2.  **Advanced Analysis**: Using `GraphAnalyzer`, `CentralityCalculator`, and `CommunityDetector` to understand data structure.\n",
    "3.  **Ontology Generation**: Automatically deriving a schema (classes & properties) from unstructured text.\n",
    "4.  **Hybrid Search**: Combining Vector Similarity with Graph Traversal for superior context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ› ï¸ Dependencies\n",
    "!pip install -qU semantica networkx matplotlib plotly pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Professional Setup & Imports\n",
    "\n",
    "We organize imports by their functional role in the Semantica ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# --- Core Framework ---\n",
    "from semantica.core import Semantica, ConfigManager\n",
    "\n",
    "# --- Knowledge Graph & Analytics ---\n",
    "from semantica.kg import (\n",
    "    GraphBuilder, \n",
    "    GraphAnalyzer, \n",
    "    CentralityCalculator, \n",
    "    CommunityDetector\n",
    ")\n",
    "\n",
    "# --- Ontology & Schema ---\n",
    "from semantica.ontology import OntologyGenerator\n",
    "\n",
    "# --- Visualization ---\n",
    "from semantica.visualization import KGVisualizer\n",
    "\n",
    "print(\"âœ… Semantica Enterprise Modules Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Realistic Data Simulation\n",
    "\n",
    "Instead of \"Hello World\", let's use a dense technical text. We'll simulate ingesting a research paper abstract on **\"Graph Neural Networks for Knowledge Graph Completion\"**. This provides rich entities (Models, Metrics, Concepts) and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_paper_text = \"\"\"\n",
    "Title: Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs.\n",
    "Abstract:\n",
    "Knowledge Graphs (KGs) are collections of structured facts, often represented as triplets (head, relation, tail).\n",
    "However, most real-world KGs like Freebase, DBPedia, and YAGO are incomplete.\n",
    "Link Prediction is the task of inferring missing facts based on existing ones.\n",
    "We propose a new model, Graph Attention Network for Knowledge Graphs (GAT-KG), which utilizes a multi-head attention mechanism.\n",
    "Unlike TransE or DistMult which rely on translational assumptions, GAT-KG captures the structural information of the local neighborhood.\n",
    "Our encoder aggregates messages from neighbors, assigning different attention weights to different relationships.\n",
    "The decoder uses a standard ConvKB layer to score the plausibility of triplets.\n",
    "We evaluate our method on standard benchmarks: FB15k-237 and WN18RR.\n",
    "Results show that GAT-KG outperforms state-of-the-art baselines like RotatE and TuckER by 5% in Hits@10.\n",
    "Ablation studies confirm that the attention mechanism is crucial for handling sparse graphs.\n",
    "\"\"\"\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "source_path = os.path.join(data_dir, \"gnn_research.txt\")\n",
    "\n",
    "with open(source_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(research_paper_text)\n",
    "\n",
    "print(f\"ðŸ“„ Research paper saved to: {source_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Management\n",
    "\n",
    "We configure the system for **precision**. \n",
    "*   `entity_resolution_strategy: fuzzy`: Handles minor typos (e.g., \"Conv-KB\" vs \"ConvKB\").\n",
    "*   `extraction`: We specify a capable LLM model to ensure we catch complex technical terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    \"project_name\": \"ResearchGraph\",\n",
    "    \"embedding\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"model\": \"text-embedding-3-small\"\n",
    "    },\n",
    "    \"knowledge_graph\": {\n",
    "        \"backend\": \"networkx\",\n",
    "        \"merge_entities\": True,\n",
    "        \"entity_resolution_strategy\": \"fuzzy\",\n",
    "        \"similarity_threshold\": 0.85\n",
    "    },\n",
    "    \"extraction\": {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "config = ConfigManager().load_from_dict(config_data)\n",
    "semantica = Semantica(config=config)\n",
    "semantica.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Knowledge Base\n",
    "\n",
    "We execute the orchestrated pipeline. This single call replaces manual parsing, cleaning, chunking, NER, RE, and embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Building Knowledge Base from Research Paper...\")\n",
    "\n",
    "result = semantica.build_knowledge_base(\n",
    "    sources=[source_path],\n",
    "    embeddings=True,\n",
    "    graph=True,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "kg = result[\"knowledge_graph\"]\n",
    "print(f\"\\nâœ… Extraction Complete.\")\n",
    "print(f\"Nodes: {kg.number_of_nodes()} | Edges: {kg.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Graph Analytics\n",
    "\n",
    "A knowledge graph is more than just storage; it's a structure we can analyze to find insights.\n",
    "\n",
    "### A. Centrality (Finding Key Concepts)\n",
    "Who are the \"VIPs\" in this paper? We use Degree Centrality to find the most connected entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_calc = CentralityCalculator()\n",
    "centrality_scores = centrality_calc.calculate_degree_centrality(kg)\n",
    "\n",
    "# Convert to DataFrame for nice display\n",
    "df_centrality = pd.DataFrame(centrality_scores.get(\"rankings\", []))\n",
    "if not df_centrality.empty:\n",
    "    print(\"ðŸ† Top 5 Most Central Concepts:\")\n",
    "    print(df_centrality.head(5)[[\"node\", \"score\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Community Detection (Topic Modeling)\n",
    "Can we group these entities into topics? We use the **Louvain Algorithm** to detect communities within the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = CommunityDetector()\n",
    "communities_result = detector.detect_communities(kg, algorithm=\"louvain\")\n",
    "communities = communities_result.get(\"communities\", [])\n",
    "\n",
    "print(f\"ðŸ” Detected {len(communities)} distinct communities (topics).\")\n",
    "\n",
    "for i, comm in enumerate(communities[:3]):  # Show first 3\n",
    "    print(f\"\\nGroup {i+1}: {', '.join(list(comm)[:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ontology Generation\n",
    "\n",
    "One of Semantica's most powerful features is **Reverse Engineering the Schema**. We can look at the graph we just built and ask: \"What is the underlying data model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_gen = OntologyGenerator()\n",
    "ontology = ontology_gen.generate_from_graph(kg)\n",
    "\n",
    "print(\"ðŸ§¬ Inferred Ontology Classes:\")\n",
    "if \"classes\" in ontology:\n",
    "    for cls in ontology[\"classes\"][:5]:\n",
    "        print(f\"  - {cls.get('name')}: {cls.get('description', 'No description inferred')}\")\n",
    "else:\n",
    "    print(\"  (Ontology generation requires sufficient data volume for inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hybrid GraphRAG Inference\n",
    "\n",
    "Finally, we use the system to answer a complex question. We simulate the logic of a Hybrid Search:\n",
    "1.  **Vector**: Find the relevant text chunk.\n",
    "2.  **Graph**: Find the *neighbors* of the key entities to understand the context (e.g., what benchmarks were used?)\n",
    "3.  **Synthesis**: (Conceptually) combine these for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What benchmarks were used to evaluate GAT-KG?\"\n",
    "print(f\"â“ Query: '{query}'\")\n",
    "\n",
    "# 1. Vector Search\n",
    "vector_store = semantica.vector_store\n",
    "embedding_gen = semantica.embedding_generator\n",
    "q_vec = embedding_gen.generate_embeddings(query, data_type=\"text\")\n",
    "vec_results = vector_store.search(q_vec, k=1)\n",
    "\n",
    "print(\"\\nðŸ”¹ [Vector Retrieval] Found Context:\")\n",
    "top_chunk = vec_results[0].get('metadata', {}).get('text', '') if vec_results else \"No match\"\n",
    "print(f\"  \\\"{top_chunk[:100]}...\\\"\")\n",
    "\n",
    "# 2. Graph Traversal\n",
    "print(\"\\nðŸ”¸ [Graph Retrieval] Exploring Neighborhoods:\")\n",
    "# Find 'GAT-KG' in the graph and see what it connects to\n",
    "target_node = None\n",
    "for node in kg.nodes():\n",
    "    if \"GAT-KG\" in str(node):\n",
    "        target_node = node\n",
    "        break\n",
    "\n",
    "if target_node:\n",
    "    neighbors = list(kg[target_node])\n",
    "    print(f\"  Entity '{target_node}' is connected to:\")\n",
    "    for n in neighbors:\n",
    "        relation = kg[target_node][n].get('type', 'related_to')\n",
    "        print(f\"   --[{relation}]--> {n}\")\n",
    "else:\n",
    "    print(\"  (Entity 'GAT-KG' not explicitly found in graph nodes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization\n",
    "\n",
    "We visualize the final graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = KGVisualizer()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "pos = nx.spring_layout(kg, k=0.8, seed=42)\n",
    "\n",
    "# Draw with more style\n",
    "nx.draw_networkx_nodes(kg, pos, node_size=1500, node_color=\"#6a0dad\", alpha=0.8) # Semantica Purple\n",
    "nx.draw_networkx_edges(kg, pos, width=1.5, alpha=0.4, edge_color=\"gray\", arrows=True)\n",
    "nx.draw_networkx_labels(kg, pos, font_size=9, font_color=\"white\", font_weight=\"bold\")\n",
    "\n",
    "edge_labels = nx.get_edge_attributes(kg, 'type')\n",
    "nx.draw_networkx_edge_labels(kg, pos, edge_labels=edge_labels, font_size=7)\n",
    "\n",
    "plt.title(\"Research Paper Knowledge Graph\", fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
