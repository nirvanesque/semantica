{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/use_cases/blockchain/01_DeFi_Protocol_Intelligence.ipynb)\n",
    "\n",
    "# DeFi Protocol Intelligence - Risk Assessment & Ontology Reasoning\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **DeFi protocol intelligence** using Semantica with focus on **risk assessment**, **ontology-based reasoning**, and **relationship analysis**. The pipeline ingests DeFi data from multiple sources, extracts protocol entities, builds knowledge graphs, and assesses risks using graph reasoning.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Risk Assessment Focus**: Emphasizes KG construction and reasoning for risk evaluation\n",
    "- **Ontology-Based Reasoning**: Uses domain ontologies for DeFi protocol analysis\n",
    "- **Relationship Analysis**: Analyzes protocol relationships and dependencies\n",
    "- **Comprehensive Data Sources**: Multiple RSS feeds, APIs, and databases\n",
    "- **Modular Architecture**: Direct use of Semantica modules without core orchestrator\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Ingest DeFi data from multiple sources (RSS feeds, APIs, databases)\n",
    "- Extract DeFi entities (Protocols, Tokens, Pools, Transactions, Risks)\n",
    "- Build and analyze DeFi knowledge graphs\n",
    "- Generate and utilize DeFi ontologies\n",
    "- Perform risk assessment using graph reasoning\n",
    "- Store and query DeFi data using vector stores and graph stores\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Data Ingestion] --> B[Document Parsing]\n",
    "    B --> C[Text Processing]\n",
    "    C --> D[Entity Extraction]\n",
    "    D --> E[Relationship Extraction]\n",
    "    E --> F[Deduplication]\n",
    "    F --> G[Conflict Detection]\n",
    "    G --> H[Knowledge Graph]\n",
    "    H --> I[Embeddings]\n",
    "    I --> J[Vector Store]\n",
    "    H --> K[Ontology Generation]\n",
    "    K --> L[Reasoning & Risk]\n",
    "    J --> M[GraphRAG Queries]\n",
    "    L --> M\n",
    "    H --> N[Graph Store]\n",
    "    K --> O[Triplet Store]\n",
    "    M --> P[Visualization]\n",
    "    N --> P\n",
    "    O --> P\n",
    "    P --> Q[Export]\n",
    "```\n",
    "\n",
    "## Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~gno (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ython-socketio (c:\\Users\\Mohd Kaif\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU semantica networkx matplotlib plotly pandas faiss-cpu beautifulsoup4 groq sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"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\")\n",
    "\n",
    "# Configuration constants\n",
    "EMBEDDING_DIMENSION = 384\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting DeFi Data from Multiple Sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting from 6 feed sources...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='font-family: monospace;'><h4>üß† Semantica - üìä Current Progress</h4><table style='width: 100%; border-collapse: collapse;'><tr><th>Status</th><th>Action</th><th>Module</th><th>Submodule</th><th>File</th><th>Time</th></tr><tr><td>‚úÖ</td><td>Semantica is deduplicating</td><td>üîÑ deduplication</td><td>SimilarityCalculator</td><td>-</td><td>0.02s</td></tr><tr><td>‚úÖ</td><td>Semantica is indexing</td><td>üìä vector_store</td><td>VectorStore</td><td>-</td><td>0.00s</td></tr><tr><td>‚úÖ</td><td>Semantica is generating</td><td>üìö ontology</td><td>OntologyGenerator</td><td>-</td><td>0.09s</td></tr><tr><td>‚úÖ</td><td>Semantica is generating</td><td>üìö ontology</td><td>ClassInferrer</td><td>-</td><td>0.02s</td></tr><tr><td>‚úÖ</td><td>Semantica is generating</td><td>üìö ontology</td><td>PropertyGenerator</td><td>-</td><td>0.01s</td></tr><tr><td>‚úÖ</td><td>Semantica is reasoning</td><td>ü§î reasoning</td><td>Reasoner</td><td>-</td><td>0.01s</td></tr><tr><td>‚úÖ</td><td>Semantica is storing</td><td>üóÑÔ∏è triplet_store</td><td>BulkLoader</td><td>-</td><td>6.02s</td></tr><tr><td>‚ùå</td><td>Semantica is processing</td><td>üîó context</td><td>ContextRetriever</td><td>-</td><td>8.75s</td></tr><tr><td>‚úÖ</td><td>Semantica is embedding</td><td>üíæ embeddings</td><td>TextEmbedder</td><td>-</td><td>0.00s</td></tr><tr><td>‚úÖ</td><td>Semantica is processing</td><td>üîó context</td><td>AgentMemory</td><td>-</td><td>0.03s</td></tr></table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [1/6] CoinDesk: 25 documents\n",
      "  [2/6] CoinTelegraph: 30 documents\n",
      "  [3/6] Decrypt: 51 documents\n",
      "  [4/6] The Block: 19 documents\n",
      "  [5/6] CryptoSlate: 10 documents\n",
      "  [6/6] CryptoNews: 20 documents\n",
      "Ingested 155 documents\n"
     ]
    }
   ],
   "source": [
    "from semantica.ingest import FeedIngestor, FileIngestor, WebIngestor\n",
    "import os\n",
    "from contextlib import redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "feed_sources = [\n",
    "    # Crypto News RSS Feeds\n",
    "    (\"CoinDesk\", \"https://www.coindesk.com/arc/outboundfeeds/rss/\"),\n",
    "    (\"CoinTelegraph\", \"https://cointelegraph.com/rss\"),\n",
    "    (\"Decrypt\", \"https://decrypt.co/feed\"),\n",
    "    (\"The Block\", \"https://www.theblock.co/rss.xml\"),\n",
    "    (\"CryptoSlate\", \"https://cryptoslate.com/feed/\"),\n",
    "    (\"CryptoNews\", \"https://cryptonews.com/news/feed/\"),\n",
    "]\n",
    "\n",
    "feed_ingestor = FeedIngestor()\n",
    "all_documents = []\n",
    "\n",
    "print(f\"Ingesting from {len(feed_sources)} feed sources...\")\n",
    "for i, (feed_name, feed_url) in enumerate(feed_sources, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            feed_data = feed_ingestor.ingest_feed(feed_url, validate=False)\n",
    "        \n",
    "        feed_count = 0\n",
    "        for item in feed_data.items:\n",
    "            if not item.content:\n",
    "                item.content = item.description or item.title or \"\"\n",
    "            if item.content:\n",
    "                if not hasattr(item, 'metadata'):\n",
    "                    item.metadata = {}\n",
    "                item.metadata['source'] = feed_name\n",
    "                all_documents.append(item)\n",
    "                feed_count += 1\n",
    "        \n",
    "        if feed_count > 0:\n",
    "            print(f\"  [{i}/{len(feed_sources)}] {feed_name}: {feed_count} documents\")\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not all_documents:\n",
    "    defi_data = \"\"\"\n",
    "    Uniswap is a decentralized exchange protocol with high liquidity pools. It uses automated market makers (AMMs) for token swaps.\n",
    "    Aave is a lending protocol that offers variable and stable interest rates. Users can deposit assets to earn yield.\n",
    "    Compound is a money market protocol for lending and borrowing cryptocurrencies. It uses algorithmic interest rates.\n",
    "    MakerDAO uses collateralized debt positions (CDPs) for stablecoin generation. DAI is the stablecoin created.\n",
    "    Curve Finance is a decentralized exchange optimized for stablecoin trading with low slippage.\n",
    "    Yearn Finance aggregates yield farming strategies across multiple DeFi protocols.\n",
    "    SushiSwap is a decentralized exchange and automated market maker with yield farming features.\n",
    "    Balancer is a protocol for programmable liquidity and automated portfolio management.\n",
    "    \"\"\"\n",
    "    with open(\"data/defi_protocols.txt\", \"w\") as f:\n",
    "        f.write(defi_data)\n",
    "    file_ingestor = FileIngestor()\n",
    "    all_documents = file_ingestor.ingest(\"data/defi_protocols.txt\")\n",
    "\n",
    "documents = all_documents\n",
    "print(f\"Ingested {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 155 documents...\n",
      "  Parsed 50/155 documents...\n",
      "  Parsed 100/155 documents...\n",
      "  Parsed 150/155 documents...\n",
      "  Parsed 155/155 documents...\n"
     ]
    }
   ],
   "source": [
    "from semantica.parse import DocumentParser\n",
    "\n",
    "parser = DocumentParser()\n",
    "\n",
    "print(f\"Parsing {len(documents)} documents...\")\n",
    "parsed_documents = []\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    try:\n",
    "        parsed = parser.parse(\n",
    "            doc.content if hasattr(doc, 'content') else str(doc),\n",
    "            content_type=\"text\"\n",
    "        )\n",
    "        parsed_documents.append(parsed)\n",
    "    except Exception:\n",
    "        parsed_documents.append(doc)\n",
    "    if i % 50 == 0 or i == len(documents):\n",
    "        print(f\"  Parsed {i}/{len(documents)} documents...\")\n",
    "\n",
    "documents = parsed_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing and Chunking DeFi Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing 155 documents...\n",
      "  Normalized 50/155 documents...\n",
      "  Normalized 100/155 documents...\n",
      "  Normalized 150/155 documents...\n",
      "  Normalized 155/155 documents...\n",
      "Chunking 155 documents...\n",
      "  Chunked 50/155 documents (50 chunks so far)\n",
      "  Chunked 100/155 documents (101 chunks so far)\n",
      "  Chunked 150/155 documents (151 chunks so far)\n",
      "  Chunked 155/155 documents (156 chunks so far)\n",
      "Created 156 chunks from 155 documents\n"
     ]
    }
   ],
   "source": [
    "from semantica.normalize import TextNormalizer\n",
    "from semantica.split import TextSplitter\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "splitter = TextSplitter(\n",
    "    method=\"entity_aware\",\n",
    "    ner_method=\"spacy\",\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "print(f\"Normalizing {len(documents)} documents...\")\n",
    "normalized_documents = []\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    normalized_text = normalizer.normalize(\n",
    "        doc.content if hasattr(doc, 'content') else str(doc),\n",
    "        clean_html=True,\n",
    "        normalize_entities=True,\n",
    "        remove_extra_whitespace=True,\n",
    "        lowercase=False\n",
    "    )\n",
    "    normalized_documents.append(normalized_text)\n",
    "    if i % 50 == 0 or i == len(documents):\n",
    "        print(f\"  Normalized {i}/{len(documents)} documents...\")\n",
    "\n",
    "print(f\"Chunking {len(normalized_documents)} documents...\")\n",
    "chunked_documents = []\n",
    "for i, doc_text in enumerate(normalized_documents, 1):\n",
    "    try:\n",
    "        with redirect_stderr(StringIO()):\n",
    "            chunks = splitter.split(doc_text)\n",
    "        chunked_documents.extend(chunks)\n",
    "    except Exception:\n",
    "        simple_splitter = TextSplitter(method=\"recursive\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "        chunks = simple_splitter.split(doc_text)\n",
    "        chunked_documents.extend(chunks)\n",
    "    if i % 50 == 0 or i == len(normalized_documents):\n",
    "        print(f\"  Chunked {i}/{len(normalized_documents)} documents ({len(chunked_documents)} chunks so far)\")\n",
    "\n",
    "print(f\"Created {len(chunked_documents)} chunks from {len(normalized_documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting DeFi Entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting entities from 156 chunks...\n",
      "  Processed 20/156 chunks (26 entities found)\n",
      "  Processed 40/156 chunks (98 entities found)\n",
      "  Processed 60/156 chunks (150 entities found)\n",
      "  Processed 80/156 chunks (176 entities found)\n",
      "  Processed 100/156 chunks (331 entities found)\n",
      "  Processed 120/156 chunks (411 entities found)\n",
      "  Processed 140/156 chunks (497 entities found)\n",
      "  Processed 156/156 chunks (560 entities found)\n",
      "Extracted 115 protocols, 324 tokens, 45 risks\n"
     ]
    }
   ],
   "source": [
    "from semantica.semantic_extract import NERExtractor\n",
    "\n",
    "# Use Semantica's built-in fallback chain: try LLM first, then ML (spaCy)\n",
    "entity_extractor = NERExtractor(\n",
    "    method=[\"llm\", \"ml\"],  # Automatic fallback if LLM fails\n",
    "    provider=\"groq\",\n",
    "    llm_model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "all_entities = []\n",
    "error_count = 0\n",
    "print(f\"Extracting entities from {len(chunked_documents)} chunks...\")\n",
    "for i, chunk in enumerate(chunked_documents, 1):\n",
    "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
    "    try:\n",
    "        entities = entity_extractor.extract_entities(\n",
    "            chunk_text,\n",
    "            entity_types=[\"Protocol\", \"Token\", \"Pool\", \"Transaction\", \"Risk\"]\n",
    "        )\n",
    "        all_entities.extend(entities)\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        if error_count <= 3:\n",
    "            print(f\"  Warning: Error processing chunk {i}: {str(e)[:100]}\")\n",
    "    \n",
    "    if i % 20 == 0 or i == len(chunked_documents):\n",
    "        print(f\"  Processed {i}/{len(chunked_documents)} chunks ({len(all_entities)} entities found)\")\n",
    "\n",
    "if error_count > 0:\n",
    "    print(f\"  Note: {error_count} chunks had errors during extraction\")\n",
    "\n",
    "protocols = [e for e in all_entities if e.label == \"Protocol\" or \"protocol\" in e.label.lower()]\n",
    "tokens = [e for e in all_entities if e.label == \"Token\" or \"token\" in e.label.lower()]\n",
    "risks = [e for e in all_entities if e.label == \"Risk\" or \"risk\" in e.label.lower()]\n",
    "\n",
    "print(f\"Extracted {len(protocols)} protocols, {len(tokens)} tokens, {len(risks)} risks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting DeFi Relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting relationships from 156 chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 20/156 chunks (37 relationships found)\n",
      "  Processed 40/156 chunks (56 relationships found)\n",
      "  Processed 60/156 chunks (72 relationships found)\n",
      "  Processed 80/156 chunks (119 relationships found)\n",
      "  Processed 100/156 chunks (183 relationships found)\n",
      "  Processed 120/156 chunks (226 relationships found)\n",
      "  Processed 140/156 chunks (301 relationships found)\n",
      "  Processed 156/156 chunks (351 relationships found)\n",
      "Extracted 351 relationships\n"
     ]
    }
   ],
   "source": [
    "from semantica.semantic_extract import RelationExtractor\n",
    "\n",
    "# Use ML-based dependency parsing to avoid rate limits\n",
    "relation_extractor = RelationExtractor(\n",
    "    method=\"dependency\",  # ML/NLP method - no API calls needed\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "all_relationships = []\n",
    "error_count = 0\n",
    "print(f\"Extracting relationships from {len(chunked_documents)} chunks...\")\n",
    "\n",
    "for i, chunk in enumerate(chunked_documents, 1):\n",
    "    chunk_text = chunk.text if hasattr(chunk, 'text') else str(chunk)\n",
    "    try:\n",
    "        relationships = relation_extractor.extract_relations(\n",
    "            chunk_text,\n",
    "            entities=all_entities,\n",
    "            relation_types=[\"uses\", \"governs\", \"provides\", \"has_risk\", \"interacts_with\", \"depends_on\"],\n",
    "            verbose=True\n",
    "        )\n",
    "        all_relationships.extend(relationships)\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        if error_count <= 3:\n",
    "            print(f\"  Warning: Error on chunk {i}: {str(e)[:100]}\")\n",
    "    \n",
    "    if i % 20 == 0 or i == len(chunked_documents):\n",
    "        print(f\"  Processed {i}/{len(chunked_documents)} chunks ({len(all_relationships)} relationships found)\")\n",
    "\n",
    "if error_count > 0:\n",
    "    print(f\"  Note: {error_count} chunks had errors during relation extraction\")\n",
    "\n",
    "print(f\"Extracted {len(all_relationships)} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving Duplicate Entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting and Resolving Conflicts\n",
    "\n",
    "‚Ä¢ **Entity & Relationship Conflict Detection**: Detects conflicts in both entity properties (protocol names, addresses) and relationships (protocol interactions, dependencies) from multiple data sources to ensure data consistency across the DeFi knowledge graph.\n",
    "\n",
    "‚Ä¢ **Credibility-Weighted Resolution**: Uses credibility-weighted strategy that considers source reliability and extraction confidence scores, prioritizing high-confidence sources for critical DeFi protocol information while aggregating evidence from multiple sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting entity conflicts in 560 entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Value conflict detected: bitcoin.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: crypto.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: stablecoin.confidence has conflicting values: ['0.9', '1.0']\n",
      "Value conflict detected: UNI.confidence has conflicting values: ['1', '1.0']\n",
      "Value conflict detected: XRP.confidence has conflicting values: ['1', '1.0']\n",
      "Value conflict detected: Ethereum.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: Bitcoin.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: governance.confidence has conflicting values: ['0.9', '0.8']\n",
      "Value conflict detected: Ether.confidence has conflicting values: ['1', '1.0']\n",
      "Value conflict detected: NFTs.confidence has conflicting values: ['0.9', '1.0']\n",
      "Value conflict detected: Aave.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: DAO.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: BTC.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: ETFs.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: Solana.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: ETF.confidence has conflicting values: ['0.9', '1.0']\n",
      "Value conflict detected: ETH.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: BNB.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: SOL.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: LINK.confidence has conflicting values: ['0.9', '1.0']\n",
      "Value conflict detected: gold.confidence has conflicting values: ['0.9', '1.0']\n",
      "Value conflict detected: silver.confidence has conflicting values: ['0.9', '1.0']\n",
      "Value conflict detected: memecoins.confidence has conflicting values: ['1', '1.0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Value conflict detected: Tether.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: Trust Wallet.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: Dogecoin.confidence has conflicting values: ['1', '1.0']\n",
      "Value conflict detected: Crypto.confidence has conflicting values: ['0.9', '1.0']\n",
      "Value conflict detected: Cardano.confidence has conflicting values: ['1', '1.0']\n",
      "Value conflict detected: btc.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: eth.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: bnb.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: sol.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: cc.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: Coinbase.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: Circle.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: Xdc.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: sky.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: tao.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: Ripple.confidence has conflicting values: ['0.9', '1.0']\n",
      "Value conflict detected: SEC.confidence has conflicting values: ['0.9', '1.0']\n",
      "Value conflict detected: stablecoins.confidence has conflicting values: ['1', '0.9']\n",
      "Value conflict detected: cryptocurrency.confidence has conflicting values: ['0.9', '1.0']\n",
      "Value conflict detected: CryptoSlate.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: Cryptonews.confidence has conflicting values: ['1', '0.9', '1.0']\n",
      "Value conflict detected: crypto.label has conflicting values: ['Token', 'Risk', 'Protocol']\n",
      "Value conflict detected: Ethereum.label has conflicting values: ['Token', 'Protocol']\n",
      "Value conflict detected: Aave.label has conflicting values: ['Token', 'Protocol']\n",
      "Value conflict detected: ETFs.label has conflicting values: ['Token', 'Pool']\n",
      "Value conflict detected: Solana.label has conflicting values: ['Token', 'Protocol']\n",
      "Value conflict detected: Wall Street.label has conflicting values: ['Token', 'Pool']\n",
      "Value conflict detected: Cardano.label has conflicting values: ['Token', 'Protocol']\n",
      "Value conflict detected: eth.label has conflicting values: ['Token', 'Protocol']\n",
      "Value conflict detected: Coinbase.label has conflicting values: ['Pool', 'Protocol']\n",
      "Value conflict detected: Circle.label has conflicting values: ['Token', 'Pool']\n",
      "Value conflict detected: Ripple.label has conflicting values: ['Token', 'Protocol']\n",
      "Value conflict detected: SEC.label has conflicting values: ['Pool', 'Risk']\n",
      "Value conflict detected: BlackRock.label has conflicting values: ['Pool', 'Protocol']\n",
      "Value conflict detected: CryptoSlate.label has conflicting values: ['Pool', 'Risk', 'Protocol']\n",
      "Value conflict detected: Cryptonews.label has conflicting values: ['Token', 'Pool', 'Protocol']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 59 entity conflicts\n",
      "Detecting relationship conflicts in 351 relationships...\n",
      "Detected 0 relationship conflicts\n",
      "Resolving 59 conflicts using credibility-weighted strategy...\n",
      "Resolved 59 conflicts\n",
      "Applied resolutions to 560 entities and 351 relationships\n"
     ]
    }
   ],
   "source": [
    "from semantica.conflicts import ConflictDetector, ConflictResolver\n",
    "\n",
    "conflict_detector = ConflictDetector()\n",
    "conflict_resolver = ConflictResolver()\n",
    "\n",
    "# Convert entities to dictionaries for conflict detection\n",
    "entity_dicts = [\n",
    "    {\n",
    "        \"id\": e.id if hasattr(e, 'id') else e.text,\n",
    "        \"text\": e.text,\n",
    "        \"label\": e.label,\n",
    "        \"type\": e.label,\n",
    "        \"confidence\": e.confidence if hasattr(e, 'confidence') else 1.0,\n",
    "        \"metadata\": e.metadata if hasattr(e, 'metadata') else {}\n",
    "    }\n",
    "    for e in all_entities\n",
    "]\n",
    "\n",
    "# Convert relationships to dictionaries for conflict detection\n",
    "relationship_dicts = [\n",
    "    {\n",
    "        \"id\": f\"{r.subject.text}_{r.predicate}_{r.object.text}\",\n",
    "        \"source_id\": r.subject.text,\n",
    "        \"target_id\": r.object.text,\n",
    "        \"type\": r.predicate,\n",
    "        \"subject\": r.subject.text,\n",
    "        \"object\": r.object.text,\n",
    "        \"predicate\": r.predicate,\n",
    "        \"confidence\": r.confidence if hasattr(r, 'confidence') else 1.0,\n",
    "        \"metadata\": r.metadata if hasattr(r, 'metadata') else {}\n",
    "    }\n",
    "    for r in all_relationships\n",
    "]\n",
    "\n",
    "# Detect conflicts in both entities and relationships\n",
    "all_conflicts = []\n",
    "\n",
    "# 1. Detect entity conflicts (duplicate protocols, conflicting properties)\n",
    "print(f\"Detecting entity conflicts in {len(entity_dicts)} entities...\")\n",
    "entity_conflicts = conflict_detector.detect_entity_conflicts(entity_dicts)\n",
    "all_conflicts.extend(entity_conflicts)\n",
    "print(f\"Detected {len(entity_conflicts)} entity conflicts\")\n",
    "\n",
    "# 2. Detect relationship conflicts (conflicting protocol interactions)\n",
    "print(f\"Detecting relationship conflicts in {len(relationship_dicts)} relationships...\")\n",
    "relationship_conflicts = conflict_detector.detect_relationship_conflicts(relationship_dicts)\n",
    "all_conflicts.extend(relationship_conflicts)\n",
    "print(f\"Detected {len(relationship_conflicts)} relationship conflicts\")\n",
    "\n",
    "# Resolve all conflicts using credibility-weighted strategy\n",
    "resolved_entities = entity_dicts.copy()\n",
    "resolved_relationships = relationship_dicts.copy()\n",
    "\n",
    "if all_conflicts:\n",
    "    print(f\"Resolving {len(all_conflicts)} conflicts using credibility-weighted strategy...\")\n",
    "    resolved = conflict_resolver.resolve_conflicts(\n",
    "        all_conflicts,\n",
    "        strategy=\"credibility_weighted\"  # Weight by source credibility and confidence\n",
    "    )\n",
    "    \n",
    "    # Apply resolved values back to entities and relationships\n",
    "    for result in resolved:\n",
    "        if result.resolved and result.resolved_value is not None:\n",
    "            if result.metadata.get(\"entity_id\"):\n",
    "                # Entity conflict - update entity\n",
    "                entity_id = result.metadata.get(\"entity_id\")\n",
    "                property_name = result.metadata.get(\"property_name\")\n",
    "                for entity in resolved_entities:\n",
    "                    if entity.get(\"id\") == entity_id and property_name:\n",
    "                        entity[property_name] = result.resolved_value\n",
    "            elif result.metadata.get(\"relationship_id\"):\n",
    "                # Relationship conflict - update relationship\n",
    "                rel_id = result.metadata.get(\"relationship_id\")\n",
    "                property_name = result.metadata.get(\"property_name\")\n",
    "                for rel in resolved_relationships:\n",
    "                    if rel.get(\"id\") == rel_id and property_name:\n",
    "                        rel[property_name] = result.resolved_value\n",
    "    \n",
    "    print(f\"Resolved {len([r for r in resolved if r.resolved])} conflicts\")\n",
    "    print(f\"Applied resolutions to {len(resolved_entities)} entities and {len(resolved_relationships)} relationships\")\n",
    "else:\n",
    "    print(\"No conflicts detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building DeFi Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 560 entities, 351 relationships\n"
     ]
    }
   ],
   "source": [
    "from semantica.kg import GraphBuilder\n",
    "\n",
    "# Conflicts already resolved - disable conflict detection in GraphBuilder\n",
    "graph_builder = GraphBuilder(\n",
    "    entity_resolution_strategy=\"fuzzy\",\n",
    "    resolve_conflicts=False  # Conflicts already resolved in previous cell\n",
    ")\n",
    "\n",
    "kg_sources = [{\n",
    "    \"entities\": [\n",
    "        {\"id\": e.get(\"id\", e.get(\"text\")), \"text\": e.get(\"text\"), \"type\": e.get(\"type\", e.get(\"label\"))}\n",
    "        for e in resolved_entities\n",
    "    ],\n",
    "    \"relationships\": [\n",
    "        {\n",
    "            \"source\": r.get(\"source_id\", r.get(\"subject\")),\n",
    "            \"target\": r.get(\"target_id\", r.get(\"object\")),\n",
    "            \"type\": r.get(\"type\", r.get(\"predicate\"))\n",
    "        }\n",
    "        for r in resolved_relationships\n",
    "    ]\n",
    "}]\n",
    "\n",
    "kg = graph_builder.build(kg_sources)\n",
    "\n",
    "entities_count = len(kg.get('entities', []))\n",
    "relationships_count = len(kg.get('relationships', []))\n",
    "print(f\"Graph: {entities_count} entities, {relationships_count} relationships\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings for Protocols and Tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fastembed not available. Install with: pip install fastembed. Using fallback embedding method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 115 protocols and 324 tokens...\n",
      "Generated 115 protocol embeddings and 324 token embeddings\n"
     ]
    }
   ],
   "source": [
    "from semantica.embeddings import EmbeddingGenerator\n",
    "\n",
    "embedding_gen = EmbeddingGenerator(\n",
    "    provider=\"sentence_transformers\",\n",
    "    model=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "print(f\"Generating embeddings for {len(protocols)} protocols and {len(tokens)} tokens...\")\n",
    "protocol_texts = [p.text for p in protocols]\n",
    "protocol_embeddings = embedding_gen.generate_embeddings(protocol_texts)\n",
    "\n",
    "token_texts = [t.text for t in tokens]\n",
    "token_embeddings = embedding_gen.generate_embeddings(token_texts)\n",
    "\n",
    "print(f\"Generated {len(protocol_embeddings)} protocol embeddings and {len(token_embeddings)} token embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fastembed not available. Install with: pip install fastembed. Using fallback embedding method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 115 protocol vectors and 324 token vectors...\n",
      "Stored 115 protocol vectors and 324 token vectors\n"
     ]
    }
   ],
   "source": [
    "from semantica.vector_store import VectorStore\n",
    "\n",
    "vector_store = VectorStore(backend=\"faiss\", dimension=EMBEDDING_DIMENSION)\n",
    "\n",
    "print(f\"Storing {len(protocol_embeddings)} protocol vectors and {len(token_embeddings)} token vectors...\")\n",
    "protocol_ids = vector_store.store_vectors(\n",
    "    vectors=protocol_embeddings,\n",
    "    metadata=[{\"type\": \"protocol\", \"name\": p.text, \"label\": p.label} for p in protocols]\n",
    ")\n",
    "\n",
    "token_ids = vector_store.store_vectors(\n",
    "    vectors=token_embeddings,\n",
    "    metadata=[{\"type\": \"token\", \"name\": t.text, \"label\": t.label} for t in tokens]\n",
    ")\n",
    "\n",
    "print(f\"Stored {len(protocol_ids)} protocol vectors and {len(token_ids)} token vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating DeFi Ontology\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated DeFi ontology with 5 classes\n"
     ]
    }
   ],
   "source": [
    "from semantica.ontology import OntologyGenerator\n",
    "\n",
    "ontology_gen = OntologyGenerator(base_uri=\"https://defi.example.org/ontology/\")\n",
    "ontology = ontology_gen.generate_from_graph(kg)\n",
    "\n",
    "print(f\"Generated DeFi ontology with {len(ontology.get('classes', []))} classes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning and Risk Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred 0 facts\n",
      "Found 19 risk paths\n"
     ]
    }
   ],
   "source": [
    "from semantica.reasoning import Reasoner\n",
    "from semantica.kg import GraphAnalyzer\n",
    "\n",
    "reasoner = Reasoner()\n",
    "reasoner.add_rule(\"IF Protocol has_risk Risk AND Risk severity high THEN Protocol risk_level critical\")\n",
    "reasoner.add_rule(\"IF Protocol depends_on Protocol AND Protocol has_risk Risk THEN Protocol inherits Risk\")\n",
    "\n",
    "inferred_facts = reasoner.infer_facts(kg)\n",
    "\n",
    "# Find paths from Protocols to Risks using GraphAnalyzer\n",
    "graph_analyzer = GraphAnalyzer(kg)\n",
    "protocols = [e.get(\"id\") or e.get(\"text\") for e in kg.get(\"entities\", []) if e.get(\"type\") == \"Protocol\"]\n",
    "risks = [e.get(\"id\") or e.get(\"text\") for e in kg.get(\"entities\", []) if e.get(\"type\") == \"Risk\"]\n",
    "\n",
    "risk_paths = []\n",
    "for protocol in protocols[:10]:\n",
    "    for risk in risks[:5]:\n",
    "        path = graph_analyzer.connectivity_analyzer.calculate_shortest_paths(kg, source=protocol, target=risk)\n",
    "        if path.get(\"exists\") and path.get(\"distance\", -1) <= 2:\n",
    "            risk_paths.append(path)\n",
    "\n",
    "print(f\"Inferred {len(inferred_facts)} facts\")\n",
    "print(f\"Found {len(risk_paths)} risk paths\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Knowledge Graph (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph store configured (commented out for demo)\n"
     ]
    }
   ],
   "source": [
    "from semantica.graph_store import GraphStore\n",
    "\n",
    "# Optional: Store to persistent graph database\n",
    "# graph_store = GraphStore(backend=\"neo4j\", uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\")\n",
    "# graph_store.store_graph(kg)\n",
    "\n",
    "print(\"Graph store configured (commented out for demo)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Ontology as RDF Triplets (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not connect to Blazegraph: HTTPConnectionPool(host='localhost', port=9999): Max retries exceeded with url: /blazegraph/namespace/kb/sparql?query=SELECT+%2A+WHERE+%7B+%3Fs+%3Fp+%3Fo+%7D+LIMIT+1 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FA2D9CB610>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "Batch 0 failed, retrying: Not connected to Blazegraph\n",
      "Batch 0 failed, retrying: Not connected to Blazegraph\n",
      "Batch 0 failed after 3 attempts: Not connected to Blazegraph\n",
      "Batch 1 failed, retrying: Not connected to Blazegraph\n",
      "Batch 1 failed, retrying: Not connected to Blazegraph\n",
      "Batch 1 failed after 3 attempts: Not connected to Blazegraph\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† Could not connect to Blazegraph: 'LoadProgress' object has no attribute 'failed_batches'\n",
      "   To use triplet store, start Blazegraph on localhost:9999\n",
      "   Skipping triplet storage for this demo\n"
     ]
    }
   ],
   "source": [
    "from semantica.triplet_store import TripletStore\n",
    "\n",
    "# Store knowledge graph and ontology as RDF triplets\n",
    "# Note: Requires Blazegraph running on localhost:9999\n",
    "try:\n",
    "    triplet_store = TripletStore(backend=\"blazegraph\", endpoint=\"http://localhost:9999/blazegraph\")\n",
    "    result = triplet_store.store(knowledge_graph=kg, ontology=ontology)\n",
    "    \n",
    "    if result.get('success'):\n",
    "        print(f\"‚úì Stored {result.get('processed', 0)}/{result.get('total', 0)} triplets successfully\")\n",
    "    else:\n",
    "        print(f\"‚ö† Stored {result.get('processed', 0)}/{result.get('total', 0)} triplets ({result.get('failed', 0)} failed)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Could not connect to Blazegraph: {str(e)[:100]}\")\n",
    "    print(\"   To use triplet store, start Blazegraph on localhost:9999\")\n",
    "    print(\"   Skipping triplet storage for this demo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG: Hybrid Vector + Graph Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m context \u001b[38;5;241m=\u001b[39m AgentContext(vector_store\u001b[38;5;241m=\u001b[39mvector_store, knowledge_graph\u001b[38;5;241m=\u001b[39mkg)\n\u001b[0;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat protocols have high risk?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_entities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_relationships\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphRAG query: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRetrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m results:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\semantica\\semantica\\context\\agent_context.py:409\u001b[0m, in \u001b[0;36mAgentContext.retrieve\u001b[1;34m(self, query, max_results, use_graph, min_score, conversation_id, user_id, include_entities, include_relationships, expand_graph, deduplicate, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m user_id\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_graph \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retriever:\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# GraphRAG: Use ContextRetriever (hybrid retrieval)\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_graph_expansion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_relevance_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# Convert RetrievedContext to dicts\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context_to_dict(r, include_entities, include_relationships)\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    420\u001b[0m     ]\n",
      "File \u001b[1;32m~\\semantica\\semantica\\context\\context_retriever.py:216\u001b[0m, in \u001b[0;36mContextRetriever.retrieve\u001b[1;34m(self, query, max_results, use_graph_expansion, min_relevance_score, **options)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Combine and rank results\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_tracker\u001b[38;5;241m.\u001b[39mupdate_tracking(\n\u001b[0;32m    214\u001b[0m     tracking_id, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRanking and merging results...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m )\n\u001b[1;32m--> 216\u001b[0m ranked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rank_and_merge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Filter by minimum score\u001b[39;00m\n\u001b[0;32m    219\u001b[0m filtered_results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    220\u001b[0m     r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ranked_results \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_relevance_score\n\u001b[0;32m    221\u001b[0m ]\n",
      "File \u001b[1;32m~\\semantica\\semantica\\context\\context_retriever.py:701\u001b[0m, in \u001b[0;36mContextRetriever._rank_and_merge\u001b[1;34m(self, results, query)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Rank and merge results from multiple sources with GraphRAG optimization.\"\"\"\u001b[39;00m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Separate results by source\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m vector_results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    702\u001b[0m graph_results \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph:\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    703\u001b[0m memory_results \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory:\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[1;32m~\\semantica\\semantica\\context\\context_retriever.py:701\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Rank and merge results from multiple sources with GraphRAG optimization.\"\"\"\u001b[39;00m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Separate results by source\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m vector_results \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector:\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    702\u001b[0m graph_results \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph:\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    703\u001b[0m memory_results \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39msource\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory:\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "from semantica.context import AgentContext\n",
    "\n",
    "context = AgentContext(vector_store=vector_store, knowledge_graph=kg)\n",
    "\n",
    "query = \"What protocols have high risk?\"\n",
    "results = context.retrieve(\n",
    "    query,\n",
    "    max_results=10,\n",
    "    use_graph=True,\n",
    "    expand_graph=True,\n",
    "    include_entities=True,\n",
    "    include_relationships=True\n",
    ")\n",
    "\n",
    "print(f\"GraphRAG query: '{query}'\")\n",
    "print(f\"\\nRetrieved {len(results)} results:\\n\")\n",
    "for i, result in enumerate(results[:5], 1):\n",
    "    print(f\"{i}. Score: {result.get('score', 0):.3f}\")\n",
    "    print(f\"   Content: {result.get('content', '')[:200]}...\")\n",
    "    if result.get('related_entities'):\n",
    "        print(f\"   Related entities: {len(result['related_entities'])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the DeFi Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "\n",
    "visualizer = KGVisualizer()\n",
    "visualizer.visualize(\n",
    "    kg,\n",
    "    output_path=\"defi_protocol_kg.html\",\n",
    "    layout=\"spring\",\n",
    "    node_size=20\n",
    ")\n",
    "\n",
    "print(\"Visualization saved to defi_protocol_kg.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.export import GraphExporter\n",
    "\n",
    "exporter = GraphExporter()\n",
    "exporter.export(kg, output_path=\"defi_protocol_kg.json\", format=\"json\")\n",
    "exporter.export(kg, output_path=\"defi_protocol_kg.graphml\", format=\"graphml\")\n",
    "exporter.export(ontology, output_path=\"defi_ontology.ttl\", format=\"rdf\")\n",
    "\n",
    "print(\"Exported knowledge graph to JSON and GraphML formats\")\n",
    "print(\"Exported ontology to RDF/TTL format\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
