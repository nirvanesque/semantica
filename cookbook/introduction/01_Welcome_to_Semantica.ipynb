{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Hawksight-AI/semantica/blob/main/cookbook/introduction/01_Welcome_to_Semantica.ipynb)\n",
    "\n",
    "# Welcome to Semantica\n",
    "\n",
    "**Open Source Framework for Semantic Layer & Knowledge Engineering**\n",
    "\n",
    "Semantica is a Python framework for transforming raw, messy, multi-source data into **semantic layers** and **knowledge graphs** that are ready to power GraphRAG, AI agents, multi-agent systems, and analytical applications.\n",
    "\n",
    "This notebook is an executable introduction. It combines:\n",
    "\n",
    "- High-level explanation of what Semantica is and why it exists\n",
    "- A structured tour of the architecture and key modules\n",
    "- Small, runnable code snippets that show the end-to-end flow\n",
    "\n",
    "**You should use this notebook to understand the big picture, not to learn every API in depth.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Semantica?\n",
    "\n",
    "Semantica is a **semantic intelligence and knowledge engineering framework**. It helps you:\n",
    "\n",
    "- Build **knowledge graphs** from unstructured and semi-structured data\n",
    "- Create a unified **semantic layer** on top of diverse data sources\n",
    "- Power **GraphRAG**, AI agents, and multi-agent systems with structured knowledge\n",
    "- Incorporate **temporal and quality-aware reasoning** into your applications\n",
    "\n",
    "### Core Capabilities\n",
    "\n",
    "- **Universal ingestion**: Files, web, feeds, databases, repositories, streams\n",
    "- **Rich parsing**: PDFs, Office documents, HTML, JSON, CSV, images, code\n",
    "- **Normalization**: Cleaning, language detection, entity normalization, date/number standardization\n",
    "- **Semantic extraction**: Named entities, relationships, events, semantic networks\n",
    "- **Knowledge graph construction**: Property graphs from entities and relations\n",
    "- **Embeddings and vector search**: Text and graph embeddings, hybrid retrieval\n",
    "- **Reasoning and ontology**: Rule-based inference, ontology generation and validation\n",
    "- **Visualization and analytics**: Graph visualizations and quality metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who Is Semantica For?\n",
    "\n",
    "- **AI/ML engineers** building GraphRAG systems, agents, and tools that need long-term memory\n",
    "- **Data engineers** orchestrating semantic enrichment pipelines over large, heterogeneous datasets\n",
    "- **Knowledge engineers and ontologists** designing and maintaining formal knowledge structures\n",
    "- **Researchers and analysts** creating domain knowledge graphs from documents and data feeds\n",
    "- **Product and platform teams** embedding semantic intelligence into applications and services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "Semantica is organized as three conceptual layers and multiple concrete modules.\n",
    "\n",
    "### Layers\n",
    "\n",
    "- **Input Layer**\n",
    "  - Connects to files, web pages, APIs, databases, email, feeds, repositories, and streams\n",
    "  - Normalizes these different sources into a unified internal representation\n",
    "\n",
    "- **Semantic Layer**\n",
    "  - Performs parsing, cleaning, semantic extraction, graph construction, embeddings, and reasoning\n",
    "  - This is where **unstructured data becomes structured knowledge**\n",
    "\n",
    "- **Output Layer**\n",
    "  - Exposes knowledge graphs, embeddings, ontologies, and analytics\n",
    "  - Integrates with vector stores, graph databases, and downstream applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Modules at a Glance\n",
    "\n",
    "Below is a conceptual overview of the core modules. Later cells show a small end-to-end example that stitches several of them together.\n",
    "\n",
    "- **Ingest**\n",
    "  - Components: `FileIngestor`, `WebIngestor`, `FeedIngestor`, `StreamIngestor`, `DBIngestor`, `EmailIngestor`, `RepoIngestor`, `MCPIngestor`\n",
    "  - Responsibility: Bring data from many sources into the pipeline\n",
    "\n",
    "- **Parse**\n",
    "  - Components: `DocumentParser`, format-specific parsers like `PDFParser`, `HTMLParser`, `JSONParser`, `ExcelParser`\n",
    "  - Responsibility: Turn raw content into structured document objects\n",
    "\n",
    "- **Normalize**\n",
    "  - Components: `TextNormalizer`, `TextCleaner`, `EntityNormalizer`, `DateNormalizer`, `NumberNormalizer`\n",
    "  - Responsibility: Clean text, standardize entities and values, prepare for extraction\n",
    "\n",
    "- **Semantic extract**\n",
    "  - Components: `NERExtractor`, `RelationExtractor`, `SemanticAnalyzer`, `SemanticNetworkExtractor`\n",
    "  - Responsibility: Identify entities and relationships that will become nodes and edges in the graph\n",
    "\n",
    "- **Knowledge graph (KG)**\n",
    "  - Components: `GraphBuilder`, `GraphAnalyzer`, `GraphValidator`, `EntityResolver`, `ConflictDetector`\n",
    "  - Responsibility: Build, analyze, and validate the knowledge graph\n",
    "\n",
    "- **Embeddings and vector store**\n",
    "  - Components: `EmbeddingGenerator`, `VectorStore`, `HybridSearch`\n",
    "  - Responsibility: Generate vector representations and enable semantic search\n",
    "\n",
    "- **Graph store**\n",
    "  - Components: `GraphStore`, adapters for backends like Neo4j or FalkorDB\n",
    "  - Responsibility: Persist and query graphs in external databases\n",
    "\n",
    "- **Reasoning and ontology**\n",
    "  - Components: `InferenceEngine`, `RuleManager`, `OntologyGenerator`, `OntologyValidator`\n",
    "  - Responsibility: Apply rules, infer new facts, and maintain formal ontologies\n",
    "\n",
    "- **Visualization**\n",
    "  - Components: `KGVisualizer`, `EmbeddingVisualizer`, `QualityVisualizer`, `AnalyticsVisualizer`\n",
    "  - Responsibility: Inspect, debug, and present graphs, embeddings, and quality metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts (High-Level)\n",
    "\n",
    "- **Knowledge graph**\n",
    "  - Nodes represent entities such as people, organizations, locations, events, or concepts\n",
    "  - Edges represent relationships such as `works_for`, `located_in`, `founded_by`\n",
    "  - Properties capture attributes and metadata such as timestamps, sources, and confidence\n",
    "\n",
    "- **Entities and relationships**\n",
    "  - Entities are extracted from text and data using NER\n",
    "  - Relationships connect entities and are extracted using pattern-based, model-based, or LLM-based methods\n",
    "\n",
    "- **Embeddings**\n",
    "  - Numerical vectors that encode semantic meaning of text or graph structures\n",
    "  - Used for semantic search, clustering, and similarity-based retrieval\n",
    "\n",
    "- **GraphRAG**\n",
    "  - Combines vector search with graph traversal\n",
    "  - Uses both embeddings and graph structure to retrieve rich, context-aware information\n",
    "\n",
    "- **Ontology**\n",
    "  - A formal model of classes, relationships, and constraints in a domain\n",
    "  - Used to standardize meaning, enable reasoning, and integrate heterogeneous data\n",
    "\n",
    "- **Quality and governance**\n",
    "  - Quality metrics (completeness, consistency, accuracy, coverage)\n",
    "  - Conflict detection and resolution at the knowledge graph level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "You can install Semantica from PyPI. In this notebook, we use a pip cell so it can run in local Jupyter or Colab.\n",
    "\n",
    "Equivalent shell commands:\n",
    "\n",
    "```bash\n",
    "pip install semantica\n",
    "pip install semantica[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"semantica[all]\"\n",
    "import semantica\n",
    "semantica.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Configuration\n",
    "\n",
    "Semantica uses configuration for API keys, embedding providers, and knowledge graph options. The example below mirrors a typical configuration while staying simple enough for a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"SEMANTICA_API_KEY\"] = \"your_openai_key\"\n",
    "os.environ[\"SEMANTICA_EMBEDDING_PROVIDER\"] = \"openai\"\n",
    "os.environ[\"SEMANTICA_MODEL_NAME\"] = \"gpt-4\"\n",
    "\n",
    "config_text = \"\"\"api_keys:\n",
    "  openai: your_key_here\n",
    "  anthropic: your_key_here\n",
    "embedding:\n",
    "  provider: openai\n",
    "  model: text-embedding-3-large\n",
    "  dimensions: 3072\n",
    "knowledge_graph:\n",
    "  backend: networkx\n",
    "  temporal: true\n",
    "\"\"\"\n",
    "Path(\"config.yaml\").write_text(config_text, encoding=\"utf-8\")\n",
    "Path(\"config.yaml\").read_text(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start: Build a Tiny Knowledge Base\n",
    "\n",
    "The simplest way to use Semantica is the high-level `build` helper. It ingests data, runs a default pipeline, and returns a dictionary that includes the knowledge graph, embeddings, metadata, and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica import build\n",
    "from pathlib import Path\n",
    "\n",
    "docs_dir = Path(\"welcome_docs\")\n",
    "docs_dir.mkdir(exist_ok=True)\n",
    "text_path = docs_dir / \"apple.txt\"\n",
    "text_content = (\n",
    "    \"Apple Inc. was founded by Steve Jobs, Steve Wozniak and Ronald Wayne in\"\n",
    "    \" Cupertino, California.\"\n",
    ")\n",
    "text_path.write_text(text_content, encoding=\"utf-8\")\n",
    "\n",
    "result = build(str(docs_dir), embeddings=False, graph=True)\n",
    "sorted(result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal End-to-End Pipeline\n",
    "\n",
    "The next example shows how to explicitly use several modules in sequence. This mirrors the architecture discussed earlier:\n",
    "\n",
    "1. Ingest a directory of documents\n",
    "2. Parse them into structured documents\n",
    "3. Normalize text\n",
    "4. Extract entities and relationships\n",
    "5. Build and analyze a knowledge graph\n",
    "6. Create embeddings and store them in a vector store\n",
    "7. Run a hybrid semantic search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ingest import FileIngestor\n",
    "from semantica.parse import DocumentParser\n",
    "from semantica.normalize import TextNormalizer\n",
    "from semantica.semantic_extract import NERExtractor, RelationExtractor\n",
    "from semantica.kg import GraphBuilder, GraphAnalyzer\n",
    "from semantica.embeddings import EmbeddingGenerator\n",
    "from semantica.vector_store import VectorStore, HybridSearch\n",
    "\n",
    "ingestor = FileIngestor()\n",
    "documents = ingestor.ingest(str(docs_dir))\n",
    "\n",
    "parser = DocumentParser()\n",
    "parsed_docs = parser.parse(documents)\n",
    "\n",
    "normalizer = TextNormalizer()\n",
    "normalized_docs = normalizer.normalize(parsed_docs)\n",
    "\n",
    "ner = NERExtractor()\n",
    "entities = ner.extract(normalized_docs)\n",
    "rel_extractor = RelationExtractor()\n",
    "relationships = rel_extractor.extract(normalized_docs, entities)\n",
    "\n",
    "builder = GraphBuilder()\n",
    "kg = builder.build(entities, relationships)\n",
    "analyzer = GraphAnalyzer()\n",
    "metrics = analyzer.analyze(kg)\n",
    "\n",
    "emb_generator = EmbeddingGenerator()\n",
    "embeddings = emb_generator.generate_embeddings(documents, data_type=\"text\")\n",
    "\n",
    "vec_store = VectorStore()\n",
    "vec_store.store(embeddings, documents, metadata={})\n",
    "hybrid = HybridSearch(vec_store)\n",
    "search_results = hybrid.search(\"Apple founders\", top_k=3)\n",
    "len(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Semantica includes a powerful visualization module. Here we create an interactive network graph from the knowledge graph built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.visualization import KGVisualizer\n",
    "\n",
    "# Create a visualizer instance\n",
    "viz = KGVisualizer(layout=\"force\", color_scheme=\"vibrant\")\n",
    "\n",
    "# Generate an interactive network visualization\n",
    "# This returns a Plotly figure object that renders in the notebook\n",
    "fig = viz.visualize_network(kg, output=\"interactive\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ontology Generation\n",
    "\n",
    "You can also automatically generate an ontology (a formal model of your domain) from the extracted entities and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.ontology import OntologyGenerator\n",
    "\n",
    "generator = OntologyGenerator(base_uri=\"https://example.org/ontology/\")\n",
    "\n",
    "# Generate ontology from the extracted data\n",
    "ontology = generator.generate_ontology({\n",
    "    \"entities\": entities,\n",
    "    \"relationships\": relationships\n",
    "})\n",
    "\n",
    "# View inferred classes\n",
    "[cls[\"name\"] for cls in ontology.get(\"classes\", [])[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Data Splitting and Chunking\n",
    "\n",
    "For RAG applications, splitting documents into smaller chunks is essential. Semantica provides a `split` module for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.split import Splitter\n",
    "\n",
    "splitter = Splitter(chunk_size=100, chunk_overlap=20)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Original documents: {len(documents)}\")\n",
    "print(f\"Generated chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Reasoning and Inference\n",
    "\n",
    "The `reasoning` module allows you to derive new facts from existing knowledge using logic rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.reasoning import InferenceEngine\n",
    "\n",
    "# Simple rule: If X founded Y, then X works_for Y\n",
    "rule = \"\"\"\n",
    "IF (?x founded ?y) THEN (?x works_for ?y)\n",
    "\"\"\"\n",
    "\n",
    "engine = InferenceEngine()\n",
    "engine.add_rule(rule)\n",
    "inferred_facts = engine.infer(kg)\n",
    "\n",
    "print(f\"Inferred {len(inferred_facts)} new facts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Export and Persistence\n",
    "\n",
    "You can save your knowledge graph to disk or export it to standard formats like CSV, JSON, or RDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.export import GraphExporter\n",
    "\n",
    "exporter = GraphExporter()\n",
    "exporter.export(kg, format=\"json\", output_path=\"knowledge_graph.json\")\n",
    "\n",
    "print(\"Graph exported to knowledge_graph.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Specialized Modules\n",
    "\n",
    "Semantica includes many other specialized modules for advanced use cases:\n",
    "\n",
    "- **Context**: Manages agent memory, conversation history, and context graphs for AI agents.\n",
    "- **Conflicts**: Detects and resolves conflicting information from different sources (e.g., different birth dates for the same person).\n",
    "- **Deduplication**: Identifies and merges duplicate entities (e.g., \"Steve Jobs\" vs \"Stephen Jobs\").\n",
    "- **Pipeline**: Orchestrates complex, multi-step workflows with retries and error handling.\n",
    "- **Triplet Store**: Adapters for enterprise RDF stores like BlazeGraph, Jena, or Virtuoso.\n",
    "- **Graph Store**: Adapters for Property Graph databases like Neo4j or FalkorDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Core `Semantica` Class\n",
    "\n",
    "For more complex systems, you can work directly with the `Semantica` core class and a configuration object. This gives you access to lifecycle management, plugin registration, and orchestration helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantica.core import Semantica, ConfigManager\n",
    "\n",
    "config_manager = ConfigManager()\n",
    "config = config_manager.load_from_file(\"config.yaml\")\n",
    "\n",
    "framework = Semantica(config=config)\n",
    "framework.initialize()\n",
    "\n",
    "kb_result = framework.build_knowledge_base(\n",
    "    sources=[str(docs_dir)],\n",
    "    embeddings=True,\n",
    "    graph=True,\n",
    ")\n",
    "\n",
    "framework.shutdown()\n",
    "sorted(kb_result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go Next\n",
    "\n",
    "- Run the notebooks under `cookbook/introduction` for focused module overviews\n",
    "- Explore `cookbook/use_cases` for domain-specific end-to-end workflows\n",
    "- Read the **Core Concepts** documentation for deeper theory and best practices"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
